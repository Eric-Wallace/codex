[
    {
        "extra_left": [],
        "left": "def call_fn(fn, args) -> ",
        "right": ":\n  \"\"\"Calls a transition operator with args, unpacking args if its a sequence.\n\n  Args:\n    fn: A `TransitionOperator`.\n    args: Arguments to `fn`\n\n  Returns:\n    ret: Return value of `fn`.\n  \"\"\"\n\n  if isinstance(args, (list, tuple)) and not mcmc_util.is_namedtuple_like(args):\n    args = args  # type: Tuple[Any]\n    return fn(*args)\n  else:\n    return fn(args)",
        "return_type_from_source": "Any"
    },
    {
        "extra_left": [],
        "left": "def maybe_broadcast_structure(from_structure, to_structure) -> ",
        "right": ":\n  \"\"\"Maybe broadcasts `from_structure` to `to_structure`.\n\n  If `from_structure` is a singleton, it is tiled to match the structure of\n  `to_structure`. Note that the elements in `from_structure` are not copied if\n  this tiling occurs.\n\n  Args:\n    from_structure: A structure.\n    to_structure: A structure.\n\n  Returns:\n    new_from_structure: Same structure as `to_structure`.\n  \"\"\"\n  flat_from = tf.nest.flatten(from_structure)\n  flat_to = tf.nest.flatten(to_structure)\n  if len(flat_from) == 1:\n    flat_from *= len(flat_to)\n  return tf.nest.pack_sequence_as(to_structure, flat_from)",
        "return_type_from_source": "Any"
    },
    {
        "extra_left": [],
        "left": "def sign_adaptation(control,\n                    output,\n                    set_point,\n                    adaptation_rate = 0.01) -> ",
        "right": ":\n  \"\"\"A function to do simple sign-based control of a variable.\n\n  ```\n  control = control * (1. + adaptation_rate) ** sign(output - set_point)\n  ```\n\n  Args:\n    control: The control variable.\n    output: The output variable.\n    set_point: The set point for `output`. This function will adjust `control`\n      so that `output` matches `set_point`.\n    adaptation_rate: Adaptation rate.\n\n  Returns:\n    control: New control.\n  \"\"\"\n\n  def _get_new_control(control, output, set_point):\n    new_control = mcmc_util.choose(output > set_point,\n                                   control * (1. + adaptation_rate),\n                                   control / (1. + adaptation_rate))\n    return new_control\n\n  output = maybe_broadcast_structure(output, control)\n  set_point = maybe_broadcast_structure(set_point, control)\n\n  return tf.nest.map_structure(_get_new_control, control, output, set_point)",
        "return_type_from_source": "FloatNest"
    },
    {
        "extra_left": [],
        "left": "def labels2onehot(labels, classes) -> ",
        "right": ":\n    \"\"\"\n    Convert labels to one-hot vectors for multi-class multi-label classification\n\n    Args:\n        labels: list of samples where each sample is a class or a list of classes which sample belongs with\n        classes: array of classes' names\n\n    Returns:\n        2d array with one-hot representation of given samples\n    \"\"\"\n    n_classes = len(classes)\n    y = []\n    for sample in labels:\n        curr = np.zeros(n_classes)\n        if isinstance(sample, list):\n            for intent in sample:\n                if intent not in classes:\n                    log.warning('Unknown intent {} detected. Assigning no class'.format(intent))\n                else:\n                    curr[np.where(np.array(classes) == intent)[0]] = 1\n        else:\n            curr[np.where(np.array(classes) == sample)[0]] = 1\n        y.append(curr)\n    y = np.asarray(y)\n    return y",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def proba2onehot(proba, confident_threshold, classes) -> ",
        "right": ":\n    \"\"\"\n    Convert vectors of probabilities to one-hot representations using confident threshold\n\n    Args:\n        proba: samples where each sample is a vector of probabilities to belong with given classes\n        confident_threshold: boundary of probability to belong with a class\n        classes: array of classes' names\n\n    Returns:\n        2d array with one-hot representation of given samples\n    \"\"\"\n    return labels2onehot(proba2labels(proba, confident_threshold, classes), classes)",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def load(self) -> ",
        "right": ":\n        \"\"\"Checks existence of the model file, loads the model if the file exists\"\"\"\n\n        # Checks presence of the model files\n        if self.load_path.exists():\n            path = str(self.load_path.resolve())\n            log.info('[loading model from {}]'.format(path))\n            self._net.load(path)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def process_word(word, to_lower = False,\n                 append_case = None) -> ",
        "right": ":\n    \"\"\"Converts word to a tuple of symbols, optionally converts it to lowercase\n    and adds capitalization label.\n\n    Args:\n        word: input word\n        to_lower: whether to lowercase\n        append_case: whether to add case mark\n            ('<FIRST_UPPER>' for first capital and '<ALL_UPPER>' for all caps)\n\n    Returns:\n        a preprocessed word\n    \"\"\"\n    if all(x.isupper() for x in word) and len(word) > 1:\n        uppercase = \"<ALL_UPPER>\"\n    elif word[0].isupper():\n        uppercase = \"<FIRST_UPPER>\"\n    else:\n        uppercase = None\n    if to_lower:\n        word = word.lower()\n    if word.isdigit():\n        answer = [\"<DIGIT>\"]\n    elif word.startswith(\"http://\") or word.startswith(\"www.\"):\n        answer = [\"<HTTP>\"]\n    else:\n        answer = list(word)\n    if to_lower and uppercase is not None:\n        if append_case == \"first\":\n            answer = [uppercase] + answer\n        elif append_case == \"last\":\n            answer = answer + [uppercase]\n    return tuple(answer)",
        "return_type_from_source": "Tuple[str]"
    },
    {
        "extra_left": [],
        "left": "def train_on_batch(self, data, labels) -> ",
        "right": ":\n        \"\"\"Trains model on a single batch\n\n        Args:\n            data: a batch of word sequences\n            labels: a batch of correct tag sequences\n        Returns:\n            the trained model\n        \"\"\"\n        X, Y = self._transform_batch(data, labels)\n        self.model_.train_on_batch(X, Y)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def predict_on_batch(self, data,\n                         return_indexes = False) -> ",
        "right": ":\n        \"\"\"\n        Makes predictions on a single batch\n\n        Args:\n            data: a batch of word sequences together with additional inputs\n            return_indexes: whether to return tag indexes in vocabulary or tags themselves\n\n        Returns:\n            a batch of label sequences\n        \"\"\"\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer",
        "return_type_from_source": "List[List[str]]"
    },
    {
        "extra_left": [],
        "left": "def _make_sent_vector(self, sent, bucket_length =None) -> ",
        "right": ":\n        \"\"\"Transforms a sentence to Numpy array, which will be the network input.\n\n        Args:\n            sent: input sentence\n            bucket_length: the width of the bucket\n\n        Returns:\n            A 3d array, answer[i][j][k] contains the index of k-th letter\n            in j-th word of i-th input sentence.\n        \"\"\"\n        bucket_length = bucket_length or len(sent)\n        answer = np.zeros(shape=(bucket_length, MAX_WORD_LENGTH+2), dtype=np.int32)\n        for i, word in enumerate(sent):\n            answer[i, 0] = self.tags.tok2idx(\"BEGIN\")\n            m = min(len(word), MAX_WORD_LENGTH)\n            for j, x in enumerate(word[-m:]):\n                answer[i, j+1] = self.symbols.tok2idx(x)\n            answer[i, m+1] = self.tags.tok2idx(\"END\")\n            answer[i, m+2:] = self.tags.tok2idx(\"PAD\")\n        return answer",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def _make_tags_vector(self, tags, bucket_length=None) -> ",
        "right": ":\n        \"\"\"Transforms a sentence of tags to Numpy array, which will be the network target.\n\n        Args:\n            tags: input sentence of tags\n            bucket_length: the width of the bucket\n\n        Returns:\n            A 2d array, answer[i][j] contains the index of j-th tag in i-th input sentence.\n        \"\"\"\n        bucket_length = bucket_length or len(tags)\n        answer = np.zeros(shape=(bucket_length,), dtype=np.int32)\n        for i, tag in enumerate(tags):\n            answer[i] = self.tags.tok2idx(tag)\n        return answer",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def bleu_advanced(y_true, y_predicted,\n                  weights=(1,), smoothing_function=SMOOTH.method1,\n                  auto_reweigh=False, penalty=True) -> ",
        "right": ":\n    \"\"\"Calculate BLEU score\n\n    Parameters:\n        y_true: list of reference tokens\n        y_predicted: list of query tokens\n        weights: n-gram weights\n        smoothing_function: SmoothingFunction\n        auto_reweigh: Option to re-normalize the weights uniformly\n        penalty: either enable brevity penalty or not\n\n    Return:\n        BLEU score\n    \"\"\"\n\n    bleu_measure = sentence_bleu([y_true], y_predicted, weights, smoothing_function, auto_reweigh)\n\n    hyp_len = len(y_predicted)\n    hyp_lengths = hyp_len\n    ref_lengths = closest_ref_length([y_true], hyp_len)\n\n    bpenalty = brevity_penalty(ref_lengths, hyp_lengths)\n\n    if penalty is True or bpenalty == 0:\n        return bleu_measure\n\n    return bleu_measure/bpenalty",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def verify_sc_url(url) -> ",
        "right": ":\n    \"\"\"Verify signature certificate URL against Amazon Alexa requirements.\n\n    Each call of Agent passes incoming utterances batch through skills filter,\n    agent skills, skills processor. Batch of dialog IDs can be provided, in\n    other case utterances indexes in incoming batch are used as dialog IDs.\n\n    Args:\n        url: Signature certificate URL from SignatureCertChainUrl HTTP header.\n\n    Returns:\n        result: True if verification was successful, False if not.\n    \"\"\"\n    parsed = urlsplit(url)\n\n    scheme = parsed.scheme\n    netloc = parsed.netloc\n    path = parsed.path\n\n    try:\n        port = parsed.port\n    except ValueError:\n        port = None\n\n    result = (scheme.lower() == 'https' and\n              netloc.lower().split(':')[0] == 's3.amazonaws.com' and\n              path.startswith('/echo.api/') and\n              (port == 443 or port is None))\n\n    return result",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def extract_certs(certs_txt) -> ",
        "right": ":\n    \"\"\"Extracts pycrypto X509 objects from SSL certificates chain string.\n\n    Args:\n        certs_txt: SSL certificates chain string.\n\n    Returns:\n        result: List of pycrypto X509 objects.\n    \"\"\"\n    pattern = r'-----BEGIN CERTIFICATE-----.+?-----END CERTIFICATE-----'\n    certs_txt = re.findall(pattern, certs_txt, flags=re.DOTALL)\n    certs = [crypto.load_certificate(crypto.FILETYPE_PEM, cert_txt) for cert_txt in certs_txt]\n    return certs",
        "return_type_from_source": "List[crypto.X509]"
    },
    {
        "extra_left": [],
        "left": "def verify_signature(amazon_cert, signature, request_body) -> ",
        "right": ":\n    \"\"\"Verifies Alexa request signature.\n\n    Args:\n        amazon_cert: Pycrypto X509 Amazon certificate.\n        signature: Base64 decoded Alexa request signature from Signature HTTP header.\n        request_body: full HTTPS request body\n    Returns:\n        result: True if verification was successful, False if not.\n    \"\"\"\n    signature = base64.b64decode(signature)\n\n    try:\n        crypto.verify(amazon_cert, signature, request_body, 'sha1')\n        result = True\n    except crypto.Error:\n        result = False\n\n    return result",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def verify_cert(signature_chain_url) -> ",
        "right": ":\n    \"\"\"Conducts series of Alexa SSL certificate verifications against Amazon Alexa requirements.\n\n    Args:\n        signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.\n    Returns:\n        result: Amazon certificate if verification was successful, None if not.\n    \"\"\"\n    try:\n        certs_chain_get = requests.get(signature_chain_url)\n    except requests.exceptions.ConnectionError as e:\n        log.error(f'Amazon signature chain get error: {e}')\n        return None\n\n    certs_chain_txt = certs_chain_get.text\n    certs_chain = extract_certs(certs_chain_txt)\n\n    amazon_cert = certs_chain.pop(0)\n\n    # verify signature chain url\n    sc_url_verification = verify_sc_url(signature_chain_url)\n    if not sc_url_verification:\n        log.error(f'Amazon signature url {signature_chain_url} was not verified')\n\n    # verify not expired\n    expired_verification = not amazon_cert.has_expired()\n    if not expired_verification:\n        log.error(f'Amazon certificate ({signature_chain_url}) expired')\n\n    # verify subject alternative names\n    sans_verification = verify_sans(amazon_cert)\n    if not sans_verification:\n        log.error(f'Subject alternative names verification for ({signature_chain_url}) certificate failed')\n\n    # verify certs chain\n    chain_verification = verify_certs_chain(certs_chain, amazon_cert)\n    if not chain_verification:\n        log.error(f'Certificates chain verification for ({signature_chain_url}) certificate failed')\n\n    result = (sc_url_verification and expired_verification and sans_verification and chain_verification)\n\n    return amazon_cert if result else None",
        "return_type_from_source": "Optional[crypto.X509]"
    },
    {
        "extra_left": [],
        "left": "def json(self) -> ",
        "right": ":\n        \"\"\"Returns list of json compatible states of the RichMessage instance\n        nested controls.\n\n        Returns:\n            json_controls: Json representation of RichMessage instance\n                nested controls.\n        \"\"\"\n        json_controls = [control.json() for control in self.controls]\n        return json_controls",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def ms_bot_framework(self) -> ",
        "right": ":\n        \"\"\"Returns list of MS Bot Framework compatible states of the\n        RichMessage instance nested controls.\n\n        Returns:\n            ms_bf_controls: MS Bot Framework representation of RichMessage instance\n                nested controls.\n        \"\"\"\n        ms_bf_controls = [control.ms_bot_framework() for control in self.controls]\n        return ms_bf_controls",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def telegram(self) -> ",
        "right": ":\n        \"\"\"Returns list of Telegram compatible states of the RichMessage\n        instance nested controls.\n\n        Returns:\n            telegram_controls: Telegram representation of RichMessage instance nested\n                controls.\n        \"\"\"\n        telegram_controls = [control.telegram() for control in self.controls]\n        return telegram_controls",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def alexa(self) -> ",
        "right": ":\n        \"\"\"Returns list of Amazon Alexa compatible states of the RichMessage\n        instance nested controls.\n\n        Returns:\n            alexa_controls: Amazon Alexa representation of RichMessage instance nested\n                controls.\n        \"\"\"\n        alexa_controls = [control.alexa() for control in self.controls]\n        return alexa_controls",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def accuracy(y_true, y_predicted) -> ",
        "right": ":\n    \"\"\"\n    Calculate accuracy in terms of absolute coincidence\n\n    Args:\n        y_true: array of true values\n        y_predicted: array of predicted values\n\n    Returns:\n        portion of absolutely coincidental samples\n    \"\"\"\n    examples_len = len(y_true)\n    correct = sum([y1 == y2 for y1, y2 in zip(y_true, y_predicted)])\n    return correct / examples_len if examples_len else 0",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def make_hello_bot_agent() -> ",
        "right": ":\n    \"\"\"Builds agent based on PatternMatchingSkill and HighestConfidenceSelector.\n\n    This is agent building tutorial. You can use this .py file to check how hello-bot agent works.\n\n    Returns:\n        agent: Agent capable of handling several simple greetings.\n    \"\"\"\n    skill_hello = PatternMatchingSkill(['Hello world'], patterns=['hi', 'hello', 'good day'])\n    skill_bye = PatternMatchingSkill(['Goodbye world', 'See you around'], patterns=['bye', 'chao', 'see you'])\n    skill_fallback = PatternMatchingSkill(['I don\\'t understand, sorry', 'I can say \"Hello world\"'])\n\n    agent = DefaultAgent([skill_hello, skill_bye, skill_fallback], skills_processor=HighestConfidenceSelector())\n\n    return agent",
        "return_type_from_source": "DefaultAgent"
    },
    {
        "extra_left": [],
        "left": "def prettify_metrics(metrics, precision = 4) -> ",
        "right": ":\n    \"\"\"Prettifies the dictionary of metrics.\"\"\"\n    prettified_metrics = OrderedDict()\n    for key, value in metrics:\n        value = round(value, precision)\n        prettified_metrics[key] = value\n    return prettified_metrics",
        "return_type_from_source": "OrderedDict"
    },
    {
        "extra_left": [],
        "left": "def populate_settings_dir(force = False) -> ",
        "right": ":\n    \"\"\"\n    Populate settings directory with default settings files\n\n    Args:\n        force: if ``True``, replace existing settings files with default ones\n\n    Returns:\n        ``True`` if any files were copied and ``False`` otherwise\n    \"\"\"\n    res = False\n    if _default_settings_path == _settings_path:\n        return res\n\n    for src in list(_default_settings_path.glob('**/*.json')):\n        dest = _settings_path / src.relative_to(_default_settings_path)\n        if not force and dest.exists():\n            continue\n        res = True\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy(src, dest)\n    return res",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def load(self, exclude_scopes = ('Optimizer',)) -> ",
        "right": ":\n        \"\"\"Load model parameters from self.load_path\"\"\"\n        if not hasattr(self, 'sess'):\n            raise RuntimeError('Your TensorFlow model {} must'\n                               ' have sess attribute!'.format(self.__class__.__name__))\n        path = str(self.load_path.resolve())\n        # Check presence of the model files\n        if tf.train.checkpoint_exists(path):\n            log.info('[loading model from {}]'.format(path))\n            # Exclude optimizer variables from saved variables\n            var_list = self._get_saveable_variables(exclude_scopes)\n            saver = tf.train.Saver(var_list)\n            saver.restore(self.sess, path)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def save(self, exclude_scopes = ('Optimizer',)) -> ",
        "right": ":\n        \"\"\"Save model parameters to self.save_path\"\"\"\n        if not hasattr(self, 'sess'):\n            raise RuntimeError('Your TensorFlow model {} must'\n                               ' have sess attribute!'.format(self.__class__.__name__))\n        path = str(self.save_path.resolve())\n        log.info('[saving model to {}]'.format(path))\n        var_list = self._get_saveable_variables(exclude_scopes)\n        saver = tf.train.Saver(var_list)\n        saver.save(self.sess, path)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _start_timer(self) -> ",
        "right": ":\n        \"\"\"Initiates self-destruct timer.\"\"\"\n        self.timer = Timer(self.config['conversation_lifetime'], self.self_destruct_callback)\n        self.timer.start()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def handle_request(self, request) -> ",
        "right": ":\n        \"\"\"Routes Alexa requests to appropriate handlers.\n\n        Args:\n            request: Alexa request.\n        Returns:\n            response: Response conforming Alexa response specification.\n        \"\"\"\n        request_type = request['request']['type']\n        request_id = request['request']['requestId']\n        log.debug(f'Received request. Type: {request_type}, id: {request_id}')\n\n        if request_type in self.handled_requests.keys():\n            response = self.handled_requests[request_type](request)\n        else:\n            response = self.handled_requests['_unsupported'](request)\n            log.warning(f'Unsupported request type: {request_type}, request id: {request_id}')\n\n        self._rearm_self_destruct()\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _act(self, utterance) -> ",
        "right": ":\n        \"\"\"Infers DeepPavlov agent with raw user input extracted from Alexa request.\n\n        Args:\n            utterance: Raw user input extracted from Alexa request.\n        Returns:\n            response: DeepPavlov agent response.\n        \"\"\"\n        if self.stateful:\n            utterance = [[utterance], [self.key]]\n        else:\n            utterance = [[utterance]]\n\n        agent_response = self.agent(*utterance)\n\n        return agent_response",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def _generate_response(self, response, request) -> ",
        "right": ":\n        \"\"\"Populates generated response with additional data conforming Alexa response specification.\n\n        Args:\n            response: Raw user input extracted from Alexa request.\n            request: Alexa request.\n        Returns:\n            response: Response conforming Alexa response specification.\n        \"\"\"\n        response_template = deepcopy(self.response_template)\n        response_template['sessionAttributes']['sessionId'] = request['session']['sessionId']\n\n        for key, value in response_template.items():\n            if key not in response.keys():\n                response[key] = value\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _handle_intent(self, request) -> ",
        "right": ":\n        \"\"\"Handles IntentRequest Alexa request.\n\n        Args:\n            request: Alexa request.\n        Returns:\n            response: \"response\" part of response dict conforming Alexa specification.\n        \"\"\"\n        intent_name = self.config['intent_name']\n        slot_name = self.config['slot_name']\n\n        request_id = request['request']['requestId']\n        request_intent = request['request']['intent']\n\n        if intent_name != request_intent['name']:\n            log.error(f\"Wrong intent name received: {request_intent['name']} in request {request_id}\")\n            return {'error': 'wrong intent name'}\n\n        if slot_name not in request_intent['slots'].keys():\n            log.error(f'No slot named {slot_name} found in request {request_id}')\n            return {'error': 'no slot found'}\n\n        utterance = request_intent['slots'][slot_name]['value']\n        agent_response = self._act(utterance)\n\n        if not agent_response:\n            log.error(f'Some error during response generation for request {request_id}')\n            return {'error': 'error during response generation'}\n\n        prediction = agent_response[0]\n        prediction = prediction.alexa()\n\n        if not prediction:\n            log.error(f'Some error during response generation for request {request_id}')\n            return {'error': 'error during response generation'}\n\n        response = self._generate_response(prediction[0], request)\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _handle_launch(self, request) -> ",
        "right": ":\n        \"\"\"Handles LaunchRequest Alexa request.\n\n        Args:\n            request: Alexa request.\n        Returns:\n            response: \"response\" part of response dict conforming Alexa specification.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.config['start_message']\n                },\n                'card': {\n                    'type': 'Simple',\n                    'content': self.config['start_message']\n                }\n            }\n        }\n\n        response = self._generate_response(response, request)\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _handle_unsupported(self, request) -> ",
        "right": ":\n        \"\"\"Handles all unsupported types of Alexa requests. Returns standard message.\n\n        Args:\n            request: Alexa request.\n        Returns:\n            response: \"response\" part of response dict conforming Alexa specification.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.config['unsupported_message']\n                },\n                'card': {\n                    'type': 'Simple',\n                    'content': self.config['unsupported_message']\n                }\n            }\n        }\n\n        response = self._generate_response(response, request)\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def elmo_loss2ppl(losses) -> ",
        "right": ":\n    \"\"\" Calculates perplexity by loss\n\n    Args:\n        losses: list of numpy arrays of model losses\n\n    Returns:\n        perplexity : float\n    \"\"\"\n    avg_loss = np.mean(losses)\n    return float(np.exp(avg_loss))",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def build_model(config, mode = 'infer',\n                load_trained = False, download = False,\n                serialized = None) -> ",
        "right": ":\n    \"\"\"Build and return the model described in corresponding configuration file.\"\"\"\n    config = parse_config(config)\n\n    if serialized:\n        serialized = pickle.loads(serialized)\n\n    if download:\n        deep_download(config)\n\n    import_packages(config.get('metadata', {}).get('imports', []))\n\n    model_config = config['chainer']\n\n    model = Chainer(model_config['in'], model_config['out'], model_config.get('in_y'))\n\n    for component_config in model_config['pipe']:\n        if load_trained and ('fit_on' in component_config or 'in_y' in component_config):\n            try:\n                component_config['load_path'] = component_config['save_path']\n            except KeyError:\n                log.warning('No \"save_path\" parameter for the {} component, so \"load_path\" will not be renewed'\n                            .format(component_config.get('class_name', component_config.get('ref', 'UNKNOWN'))))\n\n        if serialized and 'in' in component_config:\n            component_serialized = serialized.pop(0)\n        else:\n            component_serialized = None\n\n        component = from_params(component_config, mode=mode, serialized=component_serialized)\n\n        if 'in' in component_config:\n            c_in = component_config['in']\n            c_out = component_config['out']\n            in_y = component_config.get('in_y', None)\n            main = component_config.get('main', False)\n            model.append(component, c_in, c_out, in_y, main)\n\n    return model",
        "return_type_from_source": "Chainer"
    },
    {
        "extra_left": [],
        "left": "def interact_model(config) -> ",
        "right": ":\n    \"\"\"Start interaction with the model described in corresponding configuration file.\"\"\"\n    model = build_model(config)\n\n    while True:\n        args = []\n        for in_x in model.in_x:\n            args.append((input('{}::'.format(in_x)),))\n            # check for exit command\n            if args[-1][0] in {'exit', 'stop', 'quit', 'q'}:\n                return\n\n        pred = model(*args)\n        if len(model.out_params) > 1:\n            pred = zip(*pred)\n\n        print('>>', *pred)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def predict_on_stream(config, batch_size = 1, file_path = None) -> ",
        "right": ":\n    \"\"\"Make a prediction with the component described in corresponding configuration file.\"\"\"\n    if file_path is None or file_path == '-':\n        if sys.stdin.isatty():\n            raise RuntimeError('To process data from terminal please use interact mode')\n        f = sys.stdin\n    else:\n        f = open(file_path, encoding='utf8')\n\n    model = build_model(config)\n\n    args_count = len(model.in_x)\n    while True:\n        batch = list((l.strip() for l in islice(f, batch_size * args_count)))\n\n        if not batch:\n            break\n\n        args = []\n        for i in range(args_count):\n            args.append(batch[i::args_count])\n\n        res = model(*args)\n        if len(model.out_params) == 1:\n            res = [res]\n        for res in zip(*res):\n            res = json.dumps(res, ensure_ascii=False)\n            print(res, flush=True)\n\n    if f is not sys.stdin:\n        f.close()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def from_str(cls, label) -> ",
        "right": ":\n        \"\"\"\n        Convert given string label of decay type to special index\n\n        Args:\n            label: name of decay type.\n                Set of values: `\"linear\"`, `\"cosine\"`, `\"exponential\"`,\n                 `\"onecycle\"`, `\"trapezoid\"`, `[\"polynomial\", K]`, where K is a polynomial power\n\n        Returns:\n            index of decay type\n        \"\"\"\n        label_norm = label.replace('1', 'one').upper()\n        if label_norm in cls.__members__:\n            return DecayType[label_norm]\n        else:\n            raise NotImplementedError",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def _get_best(values, losses,\n                  max_loss_div = 0.9, min_val_div = 10.0) -> ",
        "right": ":\n        \"\"\"\n        Find the best value according to given losses\n\n        Args:\n            values: list of considered values\n            losses: list of obtained loss values corresponding to `values`\n            max_loss_div: maximal divergence of loss to be considered significant\n            min_val_div: minimum divergence of loss to be considered significant\n\n        Returns:\n            best value divided by `min_val_div`\n        \"\"\"\n        assert len(values) == len(losses), \"lengths of values and losses should be equal\"\n        min_ind = np.argmin(losses)\n        for i in range(min_ind - 1, 0, -1):\n            if (losses[i] * max_loss_div > losses[min_ind]) or\\\n                    (values[i] * min_val_div < values[min_ind]):\n                return values[i + 1]\n        return values[min_ind] / min_val_div",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def sk_log_loss(y_true,\n                y_predicted) -> ",
        "right": ":\n    \"\"\"\n    Calculates log loss.\n\n    Args:\n        y_true: list or array of true values\n        y_predicted: list or array of predicted values\n\n    Returns:\n        Log loss\n    \"\"\"\n    return log_loss(y_true, y_predicted)",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def show_details(item_data) -> ",
        "right": ":\n    \"\"\"Format catalog item output\n\n    Parameters:\n        item_data: item's attributes values\n\n    Returns:\n        [rich_message]: list of formatted rich message\n    \"\"\"\n\n    txt = \"\"\n\n    for key, value in item_data.items():\n        txt += \"**\" + str(key) + \"**\" + ': ' + str(value) + \"  \\n\"\n\n    return txt",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def make_agent() -> ",
        "right": ":\n    \"\"\"Make an agent\n\n    Returns:\n        agent: created Ecommerce agent\n    \"\"\"\n\n    config_path = find_config('tfidf_retrieve')\n    skill = build_model(config_path)\n    agent = EcommerceAgent(skills=[skill])\n    return agent",
        "return_type_from_source": "EcommerceAgent"
    },
    {
        "extra_left": [],
        "left": "def update_dict_recursive(editable_dict, editing_dict) -> ",
        "right": ":\n    \"\"\"Updates dict recursively\n\n    You need to use this function to update dictionary if depth of editing_dict is more then 1\n\n    Args:\n        editable_dict: dictionary, that will be edited\n        editing_dict: dictionary, that contains edits\n    Returns:\n        None\n    \"\"\"\n    for k, v in editing_dict.items():\n        if isinstance(v, collections.Mapping):\n            update_dict_recursive(editable_dict.get(k, {}), v)\n        else:\n            editable_dict[k] = v",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def alexa(self) -> ",
        "right": ":\n        \"\"\"Returns Amazon Alexa compatible state of the PlainText instance.\n\n        Creating Amazon Alexa response blank with populated \"outputSpeech\" and\n        \"card sections.\n\n        Returns:\n            response: Amazon Alexa representation of PlainText state.\n        \"\"\"\n        response = {\n            'response': {\n                'shouldEndSession': False,\n                'outputSpeech': {\n                    'type': 'PlainText',\n                    'text': self.content},\n                'card': {\n                    'type': 'Simple',\n                    'content': self.content\n                }\n            }\n        }\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def json(self) -> ",
        "right": ":\n        \"\"\"Returns json compatible state of the Button instance.\n\n        Returns:\n            control_json: Json representation of Button state.\n        \"\"\"\n        content = {}\n        content['name'] = self.name\n        content['callback'] = self.callback\n        self.control_json['content'] = content\n        return self.control_json",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def ms_bot_framework(self) -> ",
        "right": ":\n        \"\"\"Returns MS Bot Framework compatible state of the Button instance.\n\n        Creates MS Bot Framework CardAction (button) with postBack value return.\n\n        Returns:\n            control_json: MS Bot Framework representation of Button state.\n        \"\"\"\n        card_action = {}\n        card_action['type'] = 'postBack'\n        card_action['title'] = self.name\n        card_action['value'] = self.callback = self.callback\n        return card_action",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def json(self) -> ",
        "right": ":\n        \"\"\"Returns json compatible state of the ButtonsFrame instance.\n\n        Returns json compatible state of the ButtonsFrame instance including\n        all nested buttons.\n\n        Returns:\n            control_json: Json representation of ButtonsFrame state.\n        \"\"\"\n        content = {}\n\n        if self.text:\n            content['text'] = self.text\n\n        content['controls'] = [control.json() for control in self.content]\n\n        self.control_json['content'] = content\n\n        return self.control_json",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def ms_bot_framework(self) -> ",
        "right": ":\n        \"\"\"Returns MS Bot Framework compatible state of the ButtonsFrame instance.\n\n        Creating MS Bot Framework activity blank with RichCard in \"attachments\". RichCard\n        is populated with CardActions corresponding buttons embedded in ButtonsFrame.\n\n        Returns:\n            control_json: MS Bot Framework representation of ButtonsFrame state.\n        \"\"\"\n        rich_card = {}\n\n        buttons = [button.ms_bot_framework() for button in self.content]\n        rich_card['buttons'] = buttons\n\n        if self.text:\n            rich_card['title'] = self.text\n\n        attachments = [\n            {\n                \"contentType\": \"application/vnd.microsoft.card.thumbnail\",\n                \"content\": rich_card\n            }\n        ]\n\n        out_activity = {}\n        out_activity['type'] = 'message'\n        out_activity['attachments'] = attachments\n\n        return out_activity",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def squad_v2_f1(y_true, y_predicted) -> ",
        "right": ":\n    \"\"\" Calculates F-1 score between y_true and y_predicted\n        F-1 score uses the best matching y_true answer\n\n    The same as in SQuAD-v2.0\n\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n\n    Returns:\n        F-1 score : float\n    \"\"\"\n    f1_total = 0.0\n    for ground_truth, prediction in zip(y_true, y_predicted):\n        prediction_tokens = normalize_answer(prediction).split()\n        f1s = []\n        for gt in ground_truth:\n            gt_tokens = normalize_answer(gt).split()\n            if len(gt_tokens) == 0 or len(prediction_tokens) == 0:\n                f1s.append(float(gt_tokens == prediction_tokens))\n                continue\n            common = Counter(prediction_tokens) & Counter(gt_tokens)\n            num_same = sum(common.values())\n            if num_same == 0:\n                f1s.append(0.0)\n                continue\n            precision = 1.0 * num_same / len(prediction_tokens)\n            recall = 1.0 * num_same / len(gt_tokens)\n            f1 = (2 * precision * recall) / (precision + recall)\n            f1s.append(f1)\n        f1_total += max(f1s)\n    return 100 * f1_total / len(y_true) if len(y_true) > 0 else 0",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def _parse_config_property(item, variables) -> ",
        "right": ":\n    \"\"\"Recursively apply config's variables values to its property\"\"\"\n    if isinstance(item, str):\n        return item.format(**variables)\n    elif isinstance(item, list):\n        return [_parse_config_property(item, variables) for item in item]\n    elif isinstance(item, dict):\n        return {k: _parse_config_property(v, variables) for k, v in item.items()}\n    else:\n        return item",
        "return_type_from_source": "_T"
    },
    {
        "extra_left": [],
        "left": "def parse_config(config) -> ",
        "right": ":\n    \"\"\"Read config's variables and apply their values to all its properties\"\"\"\n    if isinstance(config, (str, Path)):\n        config = read_json(find_config(config))\n\n    variables = {\n        'DEEPPAVLOV_PATH': os.getenv(f'DP_DEEPPAVLOV_PATH', Path(__file__).parent.parent.parent)\n    }\n    for name, value in config.get('metadata', {}).get('variables', {}).items():\n        env_name = f'DP_{name}'\n        if env_name in os.environ:\n            value = os.getenv(env_name)\n        variables[name] = value.format(**variables)\n\n    return _parse_config_property(config, variables)",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def expand_path(path) -> ",
        "right": ":\n    \"\"\"Convert relative paths to absolute with resolving user directory.\"\"\"\n    return Path(path).expanduser().resolve()",
        "return_type_from_source": "Path"
    },
    {
        "extra_left": [],
        "left": "def from_params(params, mode = 'infer', serialized = None, **kwargs) -> ",
        "right": ":\n    \"\"\"Builds and returns the Component from corresponding dictionary of parameters.\"\"\"\n    # what is passed in json:\n    config_params = {k: _resolve(v) for k, v in params.items()}\n\n    # get component by reference (if any)\n    if 'ref' in config_params:\n        try:\n            component = _refs[config_params['ref']]\n            if serialized is not None:\n                component.deserialize(serialized)\n            return component\n        except KeyError:\n            e = ConfigError('Component with id \"{id}\" was referenced but not initialized'\n                            .format(id=config_params['ref']))\n            log.exception(e)\n            raise e\n\n    elif 'config_path' in config_params:\n        from deeppavlov.core.commands.infer import build_model\n        refs = _refs.copy()\n        _refs.clear()\n        config = parse_config(expand_path(config_params['config_path']))\n        model = build_model(config, serialized=serialized)\n        _refs.clear()\n        _refs.update(refs)\n        try:\n            _refs[config_params['id']] = model \n        except KeyError:\n            pass\n        return model\n\n    cls_name = config_params.pop('class_name', None)\n    if not cls_name:\n        e = ConfigError('Component config has no `class_name` nor `ref` fields')\n        log.exception(e)\n        raise e\n    cls = get_model(cls_name)\n\n    # find the submodels params recursively\n    config_params = {k: _init_param(v, mode) for k, v in config_params.items()}\n\n    try:\n        spec = inspect.getfullargspec(cls)\n        if 'mode' in spec.args+spec.kwonlyargs or spec.varkw is not None:\n            kwargs['mode'] = mode\n\n        component = cls(**dict(config_params, **kwargs))\n        try:\n            _refs[config_params['id']] = component\n        except KeyError:\n            pass\n    except Exception:\n        log.exception(\"Exception in {}\".format(cls))\n        raise\n\n    if serialized is not None:\n        component.deserialize(serialized)\n    return component",
        "return_type_from_source": "Component"
    },
    {
        "extra_left": [],
        "left": "def run(self) -> ",
        "right": ":\n        \"\"\"Thread run method implementation.\"\"\"\n        while True:\n            request = self.input_queue.get()\n            response = self._handle_request(request)\n            self.output_queue.put(response)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _del_conversation(self, conversation_key) -> ",
        "right": ":\n        \"\"\"Deletes Conversation instance.\n\n        Args:\n            conversation_key: Conversation key.\n        \"\"\"\n        if conversation_key in self.conversations.keys():\n            del self.conversations[conversation_key]\n            log.info(f'Deleted conversation, key: {conversation_key}')",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _refresh_valid_certs(self) -> ",
        "right": ":\n        \"\"\"Conducts cleanup of periodical certificates with expired validation.\"\"\"\n        self.timer = Timer(REFRESH_VALID_CERTS_PERIOD_SECS, self._refresh_valid_certs)\n        self.timer.start()\n\n        expired_certificates = []\n\n        for valid_cert_url, valid_cert in self.valid_certificates.items():\n            valid_cert = valid_cert\n            cert_expiration_time = valid_cert.expiration_timestamp\n            if datetime.utcnow() > cert_expiration_time:\n                expired_certificates.append(valid_cert_url)\n\n        for expired_cert_url in expired_certificates:\n            del self.valid_certificates[expired_cert_url]\n            log.info(f'Validation period of {expired_cert_url} certificate expired')",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _verify_request(self, signature_chain_url, signature, request_body) -> ",
        "right": ":\n        \"\"\"Conducts series of Alexa request verifications against Amazon Alexa requirements.\n\n        Args:\n            signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.\n            signature: Base64 decoded Alexa request signature from Signature HTTP header.\n            request_body: full HTTPS request body\n        Returns:\n            result: True if verification was successful, False if not.\n        \"\"\"\n        if signature_chain_url not in self.valid_certificates.keys():\n            amazon_cert = verify_cert(signature_chain_url)\n            if amazon_cert:\n                amazon_cert_lifetime = self.config['amazon_cert_lifetime']\n                expiration_timestamp = datetime.utcnow() + amazon_cert_lifetime\n                validated_cert = ValidatedCert(cert=amazon_cert, expiration_timestamp=expiration_timestamp)\n                self.valid_certificates[signature_chain_url] = validated_cert\n                log.info(f'Certificate {signature_chain_url} validated')\n            else:\n                log.error(f'Certificate {signature_chain_url} validation failed')\n                return False\n        else:\n            validated_cert = self.valid_certificates[signature_chain_url]\n            amazon_cert = validated_cert.cert\n\n        if verify_signature(amazon_cert, signature, request_body):\n            result = True\n        else:\n            log.error(f'Failed signature verification for request: {request_body.decode(\"utf-8\", \"replace\")}')\n            result = False\n\n        return result",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _handle_request(self, request) -> ",
        "right": ":\n        \"\"\"Processes Alexa requests from skill server and returns responses to Alexa.\n\n        Args:\n            request: Dict with Alexa request payload and metadata.\n        Returns:\n            result: Alexa formatted or error response.\n        \"\"\"\n        request_body = request['request_body']\n        signature_chain_url = request['signature_chain_url']\n        signature = request['signature']\n        alexa_request = request['alexa_request']\n\n        if not self._verify_request(signature_chain_url, signature, request_body):\n            return {'error': 'failed certificate/signature check'}\n\n        timestamp_str = alexa_request['request']['timestamp']\n        timestamp_datetime = datetime.strptime(timestamp_str, '%Y-%m-%dT%H:%M:%SZ')\n        now = datetime.utcnow()\n\n        delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now\n\n        if abs(delta.seconds) > REQUEST_TIMESTAMP_TOLERANCE_SECS:\n            log.error(f'Failed timestamp check for request: {request_body.decode(\"utf-8\", \"replace\")}')\n            return {'error': 'failed request timestamp check'}\n\n        conversation_key = alexa_request['session']['user']['userId']\n\n        if conversation_key not in self.conversations.keys():\n            if self.config['multi_instance']:\n                conv_agent = self._init_agent()\n                log.info('New conversation instance level agent initiated')\n            else:\n                conv_agent = self.agent\n\n            self.conversations[conversation_key] = \\\n                Conversation(config=self.config,\n                             agent=conv_agent,\n                             conversation_key=conversation_key,\n                             self_destruct_callback=lambda: self._del_conversation(conversation_key))\n\n            log.info(f'Created new conversation, key: {conversation_key}')\n\n        conversation = self.conversations[conversation_key]\n        response = conversation.handle_request(alexa_request)\n\n        return response",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def cls_from_str(name) -> ",
        "right": ":\n    \"\"\"Returns a class object with the name given as a string.\"\"\"\n    try:\n        module_name, cls_name = name.split(':')\n    except ValueError:\n        raise ConfigError('Expected class description in a `module.submodules:ClassName` form, but got `{}`'\n                          .format(name))\n\n    return getattr(importlib.import_module(module_name), cls_name)",
        "return_type_from_source": "type"
    },
    {
        "extra_left": [],
        "left": "def get_model(name) -> ",
        "right": ":\n    \"\"\"Returns a registered class object with the name given in the string.\"\"\"\n    if name not in _REGISTRY:\n        if ':' not in name:\n            raise ConfigError(\"Model {} is not registered.\".format(name))\n        return cls_from_str(name)\n    return cls_from_str(_REGISTRY[name])",
        "return_type_from_source": "type"
    },
    {
        "extra_left": [],
        "left": "async def fetch(self) -> ",
        "right": ":\n        \"\"\"Fetch all the information by using aiohttp\"\"\"\n        if self.request_config.get('DELAY', 0) > 0:\n            await asyncio.sleep(self.request_config['DELAY'])\n\n        timeout = self.request_config.get('TIMEOUT', 10)\n        try:\n            async with async_timeout.timeout(timeout):\n                resp = await self._make_request()\n            try:\n                resp_data = await resp.text(encoding=self.encoding)\n            except UnicodeDecodeError:\n                resp_data = await resp.read()\n\n            response = Response(\n                url=self.url,\n                method=self.method,\n                encoding=resp.get_encoding(),\n                html=resp_data,\n                metadata=self.metadata,\n                cookies=resp.cookies,\n                headers=resp.headers,\n                history=resp.history,\n                status=resp.status,\n                aws_json=resp.json,\n                aws_text=resp.text,\n                aws_read=resp.read)\n            # Retry middleware\n            aws_valid_response = self.request_config.get('VALID')\n            if aws_valid_response and iscoroutinefunction(aws_valid_response):\n                response = await aws_valid_response(response)\n            if response.ok:\n                return response\n            else:\n                return await self._retry(error_msg='request url failed!')\n        except asyncio.TimeoutError:\n            return await self._retry(error_msg='timeout')\n        except Exception as e:\n            return await self._retry(error_msg=e)\n        finally:\n            # Close client session\n            await self._close_request_session()",
        "return_type_from_source": "Response"
    },
    {
        "extra_left": [],
        "left": "async def json(self,\n                   *,\n                   encoding: str = None,\n                   loads: JSONDecoder = DEFAULT_JSON_DECODER,\n                   content_type: Optional[str] = 'application/json') -> ",
        "right": ":\n        \"\"\"Read and decodes JSON response.\"\"\"\n        return await self._aws_json(\n            encoding=encoding, loads=loads, content_type=content_type)",
        "return_type_from_source": "Any"
    },
    {
        "extra_left": [],
        "left": "async def text(self,\n                   *,\n                   encoding: Optional[str] = None,\n                   errors: str = 'strict') -> ",
        "right": ":\n        \"\"\"Read response payload and decode.\"\"\"\n        return await self._aws_text(encoding=encoding, errors=errors)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def text(text, speak = None, input_hint = InputHints.accepting_input) -> ",
        "right": ":\n        \"\"\"\n        Returns a simple text message.\n\n        :Example:\n        message = MessageFactory.text('Greetings from example message')\n        await context.send_activity(message)\n\n        :param text:\n        :param speak:\n        :param input_hint:\n        :return:\n        \"\"\"\n        message = Activity(type=ActivityTypes.message, text=text, input_hint=input_hint)\n        if speak:\n            message.speak = speak\n\n        return message",
        "return_type_from_source": "Activity"
    },
    {
        "extra_left": [],
        "left": "def suggested_actions(actions, text = None, speak = None,\n                          input_hint = InputHints.accepting_input) -> ",
        "right": ":\n        \"\"\"\n        Returns a message that includes a set of suggested actions and optional text.\n\n        :Example:\n        message = MessageFactory.suggested_actions([CardAction(title='a', type=ActionTypes.im_back, value='a'),\n                                                    CardAction(title='b', type=ActionTypes.im_back, value='b'),\n                                                    CardAction(title='c', type=ActionTypes.im_back, value='c')], 'Choose a color')\n        await context.send_activity(message)\n\n        :param actions:\n        :param text:\n        :param speak:\n        :param input_hint:\n        :return:\n        \"\"\"\n        actions = SuggestedActions(actions=actions)\n        message = Activity(type=ActivityTypes.message, input_hint=input_hint, suggested_actions=actions)\n        if text:\n            message.text = text\n        if speak:\n            message.speak = speak\n        return message",
        "return_type_from_source": "Activity"
    },
    {
        "extra_left": [],
        "left": "def list(attachments, text = None, speak = None,\n             input_hint = None) -> ",
        "right": ":\n        \"\"\"\n        Returns a message that will display a set of attachments in list form.\n\n        :Example:\n        message = MessageFactory.list([CardFactory.hero_card(HeroCard(title='title1',\n                                                             images=[CardImage(url='imageUrl1')],\n                                                             buttons=[CardAction(title='button1')])),\n                                       CardFactory.hero_card(HeroCard(title='title2',\n                                                             images=[CardImage(url='imageUrl2')],\n                                                             buttons=[CardAction(title='button2')])),\n                                       CardFactory.hero_card(HeroCard(title='title3',\n                                                             images=[CardImage(url='imageUrl3')],\n                                                             buttons=[CardAction(title='button3')]))])\n        await context.send_activity(message)\n\n        :param attachments:\n        :param text:\n        :param speak:\n        :param input_hint:\n        :return:\n        \"\"\"\n        return attachment_activity(AttachmentLayoutTypes.list, attachments, text, speak, input_hint)",
        "return_type_from_source": "Activity"
    },
    {
        "extra_left": [],
        "left": "def create_trace(\n        turn_activity,\n        name,\n        value = None,\n        value_type = None,\n        label = None,\n    ) -> ",
        "right": ":\n        \"\"\"Creates a trace activity based on this activity.\n\n        :param turn_activity:\n        :type turn_activity: Activity\n        :param name: The value to assign to the trace activity's <see cref=\"Activity.name\"/> property.\n        :type name: str\n        :param value: The value to assign to the trace activity's <see cref=\"Activity.value\"/> property., defaults to None\n        :param value: object, optional\n        :param value_type: The value to assign to the trace activity's <see cref=\"Activity.value_type\"/> property, defaults to None\n        :param value_type: str, optional\n        :param label: The value to assign to the trace activity's <see cref=\"Activity.label\"/> property, defaults to None\n        :param label: str, optional\n        :return: The created trace activity.\n        :rtype: Activity\n        \"\"\"\n\n        from_property = (\n            ChannelAccount(\n                id=turn_activity.recipient.id, name=turn_activity.recipient.name\n            )\n            if turn_activity.recipient is not None\n            else ChannelAccount()\n        )\n        if value_type is None and value is not None:\n            value_type = type(value).__name__\n\n        reply = Activity(\n            type=ActivityTypes.trace,\n            timestamp=datetime.utcnow(),\n            from_property=from_property,\n            recipient=ChannelAccount(\n                id=turn_activity.from_property.id, name=turn_activity.from_property.name\n            ),\n            reply_to_id=turn_activity.id,\n            service_url=turn_activity.service_url,\n            channel_id=turn_activity.channel_id,\n            conversation=ConversationAccount(\n                is_group=turn_activity.conversation.is_group,\n                id=turn_activity.conversation.id,\n                name=turn_activity.conversation.name,\n            ),\n            name=name,\n            label=label,\n            value_type=value_type,\n            value=value,\n        )\n        return reply",
        "return_type_from_source": "Activity"
    },
    {
        "extra_left": [],
        "left": "def telemetry_client(self, value) -> ",
        "right": ":\n        \"\"\"\n        Sets the telemetry client for logging events.\n        \"\"\"\n        if value is None:\n            self._telemetry_client = NullTelemetryClient()\n        else:\n            self._telemetry_client = value",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "async def read(self, keys) -> ",
        "right": ":\n        \"\"\"Read storeitems from storage.\n\n        :param keys:\n        :return dict:\n        \"\"\"\n        try:\n            # check if the database and container exists and if not create\n            if not self.__container_exists:\n                self.__create_db_and_container()\n            if len(keys) > 0:\n                # create the parameters object\n                parameters = [\n                    {'name': f'@id{i}', 'value': f'{self.__sanitize_key(key)}'}\n                    for i, key in enumerate(keys)\n                    ]\n                # get the names of the params\n                parameter_sequence = ','.join(param.get('name')\n                                              for param in parameters)\n                # create the query\n                query = {\n                    \"query\":\n                    f\"SELECT c.id, c.realId, c.document, c._etag \\\nFROM c WHERE c.id in ({parameter_sequence})\",\n                    \"parameters\": parameters\n                    }\n                options = {'enableCrossPartitionQuery': True}\n                # run the query and store the results as a list\n                results = list(\n                    self.client.QueryItems(\n                        self.__container_link, query, options)\n                    )\n                # return a dict with a key and a StoreItem\n                return {\n                    r.get('realId'): self.__create_si(r) for r in results\n                    }\n            else:\n                raise Exception('cosmosdb_storage.read(): \\\nprovide at least one key')\n        except TypeError as e:\n            raise e",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def __create_si(self, result) -> ",
        "right": ":\n        \"\"\"Create a StoreItem from a result out of CosmosDB.\n\n        :param result:\n        :return StoreItem:\n        \"\"\"\n        # get the document item from the result and turn into a dict\n        doc = result.get('document')\n        # readd the e_tag from Cosmos\n        doc['e_tag'] = result.get('_etag')\n        # create and return the StoreItem\n        return StoreItem(**doc)",
        "return_type_from_source": "StoreItem"
    },
    {
        "extra_left": [],
        "left": "def __create_dict(self, si) -> ",
        "right": ":\n        \"\"\"Return the dict of a StoreItem.\n\n        This eliminates non_magic attributes and the e_tag.\n\n        :param si:\n        :return dict:\n        \"\"\"\n        # read the content\n        non_magic_attr = ([attr for attr in dir(si)\n                          if not attr.startswith('_') or attr.__eq__('e_tag')])\n        # loop through attributes and write and return a dict\n        return ({attr: getattr(si, attr)\n                for attr in non_magic_attr})",
        "return_type_from_source": "Dict"
    },
    {
        "extra_left": [],
        "left": "def __sanitize_key(self, key) -> ",
        "right": ":\n        \"\"\"Return the sanitized key.\n\n        Replace characters that are not allowed in keys in Cosmos.\n\n        :param key:\n        :return str:\n        \"\"\"\n        # forbidden characters\n        bad_chars = ['\\\\', '?', '/', '#', '\\t', '\\n', '\\r']\n        # replace those with with '*' and the\n        # Unicode code point of the character and return the new string\n        return ''.join(\n            map(\n                lambda x: '*'+str(ord(x)) if x in bad_chars else x, key\n                )\n            )",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def __get_or_create_database(self, doc_client, id) -> ",
        "right": ":\n        \"\"\"Return the database link.\n\n        Check if the database exists or create the db.\n\n        :param doc_client:\n        :param id:\n        :return str:\n        \"\"\"\n        # query CosmosDB for a database with that name/id\n        dbs = list(doc_client.QueryDatabases({\n            \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n            \"parameters\": [\n                {\"name\": \"@id\", \"value\": id}\n            ]\n        }))\n        # if there are results, return the first (db names are unique)\n        if len(dbs) > 0:\n            return dbs[0]['id']\n        else:\n            # create the database if it didn't exist\n            res = doc_client.CreateDatabase({'id': id})\n            return res['id']",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def __get_or_create_container(self, doc_client, container) -> ",
        "right": ":\n        \"\"\"Return the container link.\n\n        Check if the container exists or create the container.\n\n        :param doc_client:\n        :param container:\n        :return str:\n        \"\"\"\n        # query CosmosDB for a container in the database with that name\n        containers = list(doc_client.QueryContainers(\n            self.__database_link,\n            {\n                \"query\": \"SELECT * FROM r WHERE r.id=@id\",\n                \"parameters\": [\n                    {\"name\": \"@id\", \"value\": container}\n                ]\n            }\n        ))\n        # if there are results, return the first (container names are unique)\n        if len(containers) > 0:\n            return containers[0]['id']\n        else:\n            # Create a container if it didn't exist\n            res = doc_client.CreateContainer(\n                self.__database_link, {'id': container})\n            return res['id']",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def fill_qna_event(\n        self,\n        query_results,\n        turn_context,\n        telemetry_properties = None,\n        telemetry_metrics = None\n    ) -> ",
        "right": ":\n        \"\"\"\n        Fills the event properties and metrics for the QnaMessage event for telemetry.\n\n        :return: A tuple of event data properties and metrics that will be sent to the BotTelemetryClient.track_event() method for the QnAMessage event. The properties and metrics returned the standard properties logged with any properties passed from the get_answers() method.\n\n        :rtype: EventData\n        \"\"\"\n\n        properties = dict()\n        metrics = dict()\n\n        properties[QnATelemetryConstants.knowledge_base_id_property] = self._endpoint.knowledge_base_id\n\n        text = turn_context.activity.text\n        userName = turn_context.activity.from_property.name\n\n        # Use the LogPersonalInformation flag to toggle logging PII data; text and username are common examples.\n        if self.log_personal_information:\n            if text:\n                properties[QnATelemetryConstants.question_property] = text\n            \n            if userName:\n                properties[QnATelemetryConstants.username_property] = userName\n\n        # Fill in Qna Results (found or not).\n        if len(query_results) > 0:\n            query_result = query_results[0]\n\n            result_properties = {\n                QnATelemetryConstants.matched_question_property: json.dumps(query_result.questions),\n                QnATelemetryConstants.question_id_property: str(query_result.id),\n                QnATelemetryConstants.answer_property: query_result.answer,\n                QnATelemetryConstants.score_metric: query_result.score,\n                QnATelemetryConstants.article_found_property: 'true'\n            }\n\n            properties.update(result_properties)\n        else:\n            no_match_properties = {\n                QnATelemetryConstants.matched_question_property : 'No Qna Question matched',\n                QnATelemetryConstants.question_id_property : 'No Qna Question Id matched',\n                QnATelemetryConstants.answer_property : 'No Qna Answer matched',\n                QnATelemetryConstants.article_found_property : 'false'\n            }\n            \n            properties.update(no_match_properties)\n\n        # Additional Properties can override \"stock\" properties.\n        if telemetry_properties:\n            properties.update(telemetry_properties)\n\n        # Additional Metrics can override \"stock\" metrics.\n        if telemetry_metrics:\n            metrics.update(telemetry_metrics)\n        \n        return EventData(properties=properties, metrics=metrics)",
        "return_type_from_source": "EventData"
    },
    {
        "extra_left": [],
        "left": "def get_conversation_reference(activity) -> ",
        "right": ":\n        \"\"\"\n        Returns the conversation reference for an activity. This can be saved as a plain old JSON\n        object and then later used to message the user proactively.\n\n        Usage Example:\n        reference = TurnContext.get_conversation_reference(context.request)\n        :param activity:\n        :return:\n        \"\"\"\n        return ConversationReference(activity_id=activity.id,\n                                     user=copy(activity.from_property),\n                                     bot=copy(activity.recipient),\n                                     conversation=copy(activity.conversation),\n                                     channel_id=activity.channel_id,\n                                     service_url=activity.service_url)",
        "return_type_from_source": "ConversationReference"
    },
    {
        "extra_left": [],
        "left": "def get_step_name(self, index) -> ",
        "right": ":\n        \"\"\"\n        Give the waterfall step a unique name\n        \"\"\"\n        step_name = self._steps[index].__qualname__\n\n        if not step_name or \">\" in step_name :\n            step_name = f\"Step{index + 1}of{len(self._steps)}\"\n\n        return step_name",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def supports_suggested_actions(channel_id, button_cnt = 100) -> ",
        "right": ":\n        \"\"\"Determine if a number of Suggested Actions are supported by a Channel.\n\n        Args:\n            channel_id (str): The Channel to check the if Suggested Actions are supported in.\n            button_cnt (int, optional): Defaults to 100. The number of Suggested Actions to check for the Channel.\n\n        Returns:\n            bool: True if the Channel supports the button_cnt total Suggested Actions, False if the Channel does not support that number of Suggested Actions.\n        \"\"\"\n\n        max_actions = {\n            # https://developers.facebook.com/docs/messenger-platform/send-messages/quick-replies\n            Channels.facebook: 10,\n            Channels.skype: 10,\n            # https://developers.line.biz/en/reference/messaging-api/#items-object\n            Channels.line: 13,\n            # https://dev.kik.com/#/docs/messaging#text-response-object\n            Channels.kik: 20,\n            Channels.telegram: 100,\n            Channels.slack: 100,\n            Channels.emulator: 100,\n            Channels.direct_line: 100,\n            Channels.webchat: 100,\n        }\n        return button_cnt <= max_actions[channel_id] if channel_id in max_actions else False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def supports_card_actions(channel_id, button_cnt = 100) -> ",
        "right": ":\n        \"\"\"Determine if a number of Card Actions are supported by a Channel.\n\n        Args:\n            channel_id (str): The Channel to check if the Card Actions are supported in.\n            button_cnt (int, optional): Defaults to 100. The number of Card Actions to check for the Channel.\n\n        Returns:\n            bool: True if the Channel supports the button_cnt total Card Actions, False if the Channel does not support that number of Card Actions.\n        \"\"\"\n\n        max_actions = {\n            Channels.facebook: 3,\n            Channels.skype: 3,\n            Channels.ms_teams: 3,\n            Channels.line: 99,\n            Channels.slack: 100,\n            Channels.emulator: 100,\n            Channels.direct_line: 100,\n            Channels.webchat: 100,\n            Channels.cortana: 100,\n        }\n        return button_cnt <= max_actions[channel_id] if channel_id in max_actions else False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_channel_id(turn_context) -> ",
        "right": ":\n        \"\"\"Get the Channel Id from the current Activity on the Turn Context.\n\n        Args:\n            turn_context (TurnContext): The Turn Context to retrieve the Activity's Channel Id from.\n\n        Returns:\n            str: The Channel Id from the Turn Context's Activity.\n        \"\"\"\n\n        if turn_context.activity.channel_id is None:\n            return \"\"\n        else:\n            return turn_context.activity.channel_id",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def is_token_from_emulator(auth_header) -> ",
        "right": ":\n        \"\"\" Determines if a given Auth header is from the Bot Framework Emulator\n\n        :param auth_header: Bearer Token, in the 'Bearer [Long String]' Format.\n        :type auth_header: str\n\n        :return: True, if the token was issued by the Emulator. Otherwise, false.\n        \"\"\"\n        # The Auth Header generally looks like this:\n        # \"Bearer eyJ0e[...Big Long String...]XAiO\"\n        if not auth_header:\n            # No token. Can't be an emulator token.\n            return False\n\n        parts = auth_header.split(' ')\n        if len(parts) != 2:\n            # Emulator tokens MUST have exactly 2 parts.\n            # If we don't have 2 parts, it's not an emulator token\n            return False\n\n        auth_scheme = parts[0]\n        bearer_token = parts[1]\n\n        # We now have an array that should be:\n        # [0] = \"Bearer\"\n        # [1] = \"[Big Long String]\"\n        if auth_scheme != 'Bearer':\n            # The scheme from the emulator MUST be \"Bearer\"\n            return False\n\n        # Parse the Big Long String into an actual token.\n        token = jwt.decode(bearer_token, verify=False)\n        if not token:\n            return False\n\n        # Is there an Issuer?\n        issuer = token['iss']\n        if not issuer:\n            # No Issuer, means it's not from the Emulator.\n            return False\n\n        # Is the token issues by a source we consider to be the emulator?\n        issuer_list = EmulatorValidation.TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS.issuer\n        if issuer_list and not issuer in issuer_list:\n            # Not a Valid Issuer. This is NOT a Bot Framework Emulator Token.\n            return False\n\n        # The Token is from the Bot Framework Emulator. Success!\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def hero_card(card) -> ",
        "right": ":\n        \"\"\"\n        Returns an attachment for a hero card. Will raise a TypeError if 'card' argument is not a HeroCard.\n\n        Hero cards tend to have one dominant full width image and the cards text & buttons can\n        usually be found below the image.\n        :return:\n        \"\"\"\n        if not isinstance(card, HeroCard):\n            raise TypeError('CardFactory.hero_card(): `card` argument is not an instance of an HeroCard, '\n                            'unable to prepare attachment.')\n\n        return Attachment(content_type=CardFactory.content_types.hero_card,\n                          content=card)",
        "return_type_from_source": "Attachment"
    },
    {
        "extra_left": [],
        "left": "def ch_duration(self, *channels: List[Channel]) -> ",
        "right": ":\n        \"\"\"Return duration of supplied channels.\n\n        Args:\n            *channels: Supplied channels\n        \"\"\"\n        return self.timeslots.ch_duration(*channels)",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def ch_start_time(self, *channels: List[Channel]) -> ",
        "right": ":\n        \"\"\"Return minimum start time for supplied channels.\n\n        Args:\n            *channels: Supplied channels\n        \"\"\"\n        return self.timeslots.ch_start_time(*channels)",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def ch_stop_time(self, *channels: List[Channel]) -> ",
        "right": ":\n        \"\"\"Return maximum start time for supplied channels.\n\n        Args:\n            *channels: Supplied channels\n        \"\"\"\n        return self.timeslots.ch_stop_time(*channels)",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def flatten(schedule, name = None) -> ",
        "right": ":\n    \"\"\"Create a flattened schedule.\n\n    Args:\n        schedule: Schedules to flatten\n        name: Name of the new schedule. Defaults to first element of `schedules`\n    \"\"\"\n    if name is None:\n        name = schedule.name\n\n    return Schedule(*schedule.instructions, name=name)",
        "return_type_from_source": "Schedule"
    },
    {
        "extra_left": [],
        "left": "def shift(schedule, time, name = None) -> ",
        "right": ":\n    \"\"\"Return schedule shifted by `time`.\n\n    Args:\n        schedule: The schedule to shift\n        time: The time to shift by\n        name: Name of shifted schedule. Defaults to name of `schedule`\n    \"\"\"\n    if name is None:\n        name = schedule.name\n    return union((time, schedule), name=name)",
        "return_type_from_source": "Schedule"
    },
    {
        "extra_left": [],
        "left": "def insert(parent, time, child,\n           name = None) -> ",
        "right": ":\n    \"\"\"Return a new schedule with the `child` schedule inserted into the `parent` at `start_time`.\n\n    Args:\n        parent: Schedule to be inserted into\n        time: Time to be inserted defined with respect to `parent`\n        child: Schedule to insert\n        name: Name of the new schedule. Defaults to name of parent\n    \"\"\"\n    return union(parent, (time, child), name=name)",
        "return_type_from_source": "Schedule"
    },
    {
        "extra_left": [],
        "left": "def append(parent, child,\n           name = None) -> ",
        "right": ":\n    r\"\"\"Return a new schedule with by appending `child` to `parent` at\n       the last time of the `parent` schedule's channels\n       over the intersection of the parent and child schedule's channels.\n\n       $t = \\textrm{max}({x.stop\\_time |x \\in parent.channels \\cap child.channels})$\n\n    Args:\n        parent: The schedule to be inserted into\n        child: The schedule to insert\n        name: Name of the new schedule. Defaults to name of parent\n    \"\"\"\n    common_channels = set(parent.channels) & set(child.channels)\n    insertion_time = parent.ch_stop_time(*common_channels)\n    return insert(parent, insertion_time, child, name=name)",
        "return_type_from_source": "Schedule"
    },
    {
        "extra_left": [],
        "left": "def constant(times, amp) -> ",
        "right": ":\n    \"\"\"Continuous constant pulse.\n\n    Args:\n        times: Times to output pulse for.\n        amp: Complex pulse amplitude.\n    \"\"\"\n    return np.full(len(times), amp, dtype=np.complex_)",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def square(times, amp, period, phase = 0) -> ",
        "right": ":\n    \"\"\"Continuous square wave.\n\n    Args:\n        times: Times to output wave for.\n        amp: Pulse amplitude. Wave range is [-amp, amp].\n        period: Pulse period, units of dt.\n        phase: Pulse phase.\n    \"\"\"\n    x = times/period+phase/np.pi\n    return amp*(2*(2*np.floor(x) - np.floor(2*x)) + 1).astype(np.complex_)",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def triangle(times, amp, period, phase = 0) -> ",
        "right": ":\n    \"\"\"Continuous triangle wave.\n\n    Args:\n        times: Times to output wave for.\n        amp: Pulse amplitude. Wave range is [-amp, amp].\n        period: Pulse period, units of dt.\n        phase: Pulse phase.\n    \"\"\"\n    return amp*(-2*np.abs(sawtooth(times, 1, period, (phase-np.pi/2)/2)) + 1).astype(np.complex_)",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def cos(times, amp, freq, phase = 0) -> ",
        "right": ":\n    \"\"\"Continuous cosine wave.\n\n    Args:\n        times: Times to output wave for.\n        amp: Pulse amplitude.\n        freq: Pulse frequency, units of 1/dt.\n        phase: Pulse phase.\n    \"\"\"\n    return amp*np.cos(2*np.pi*freq*times+phase).astype(np.complex_)",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def _fix_gaussian_width(gaussian_samples, amp, center, sigma,\n                        zeroed_width = None, rescale_amp = False,\n                        ret_scale_factor = False) -> ",
        "right": ":\n    r\"\"\"Enforce that the supplied gaussian pulse is zeroed at a specific width.\n\n    This is acheived by subtracting $\\Omega_g(center \\pm zeroed_width/2)$ from all samples.\n\n    amp: Pulse amplitude at `2\\times center+1`.\n    center: Center (mean) of pulse.\n    sigma: Width (standard deviation) of pulse.\n    zeroed_width: Subtract baseline to gaussian pulses to make sure\n             $\\Omega_g(center \\pm zeroed_width/2)=0$ is satisfied. This is used to avoid\n             large discontinuities at the start of a gaussian pulse. If unsupplied,\n             defaults to $2*(center+1)$ such that the samples are zero at $\\Omega_g(-1)$.\n    rescale_amp: If `zeroed_width` is not `None` and `rescale_amp=True` the pulse will\n                 be rescaled so that $\\Omega_g(center)-\\Omega_g(center\\pm zeroed_width/2)=amp$.\n    ret_scale_factor: Return amplitude scale factor.\n    \"\"\"\n    if zeroed_width is None:\n        zeroed_width = 2*(center+1)\n\n    zero_offset = gaussian(np.array([-zeroed_width/2]), amp, center, sigma)\n    gaussian_samples -= zero_offset\n    amp_scale_factor = 1.\n    if rescale_amp:\n        amp_scale_factor = amp/(amp-zero_offset)\n        gaussian_samples *= amp_scale_factor\n\n    if ret_scale_factor:\n        return gaussian_samples, amp_scale_factor\n    return gaussian_samples",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def gaussian_deriv(times, amp, center, sigma,\n                   ret_gaussian = False) -> ",
        "right": ":\n    \"\"\"Continuous unnormalized gaussian derivative pulse.\n\n    Args:\n        times: Times to output pulse for.\n        amp: Pulse amplitude at `center`.\n        center: Center (mean) of pulse.\n        sigma: Width (standard deviation) of pulse.\n        ret_gaussian: Return gaussian with which derivative was taken with.\n    \"\"\"\n    gauss, x = gaussian(times, amp=amp, center=center, sigma=sigma, ret_x=True)\n    gauss_deriv = -x/sigma*gauss\n    if ret_gaussian:\n        return gauss_deriv, gauss\n    return gauss_deriv",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def gaussian_square(times, amp, center, width,\n                    sigma, zeroed_width = None) -> ",
        "right": ":\n    r\"\"\"Continuous gaussian square pulse.\n\n    Args:\n        times: Times to output pulse for.\n        amp: Pulse amplitude.\n        center: Center of the square pulse component.\n        width: Width of the square pulse component.\n        sigma: Width (standard deviation) of gaussian rise/fall portion of the pulse.\n        zeroed_width: Subtract baseline of gaussian square pulse\n                      to enforce $\\OmegaSquare(center \\pm zeroed_width/2)=0$.\n    \"\"\"\n    square_start = center-width/2\n    square_stop = center+width/2\n    if zeroed_width:\n        zeroed_width = min(width, zeroed_width)\n        gauss_zeroed_width = zeroed_width-width\n    else:\n        gauss_zeroed_width = None\n\n    funclist = [functools.partial(gaussian, amp=amp, center=square_start, sigma=sigma,\n                                  zeroed_width=gauss_zeroed_width, rescale_amp=True),\n                functools.partial(gaussian, amp=amp, center=square_stop, sigma=sigma,\n                                  zeroed_width=gauss_zeroed_width, rescale_amp=True),\n                functools.partial(constant, amp=amp)]\n    condlist = [times <= square_start, times >= square_stop]\n    return np.piecewise(times.astype(np.complex_), condlist, funclist)",
        "return_type_from_source": "np.ndarray"
    },
    {
        "extra_left": [],
        "left": "def insert(self, start_time, schedule) -> ",
        "right": ":\n        \"\"\"Return a new schedule with `schedule` inserted within `self` at `start_time`.\n\n        Args:\n            start_time: time to be inserted\n            schedule: schedule to be inserted\n        \"\"\"\n        return ops.insert(self, start_time, schedule)",
        "return_type_from_source": "'ScheduleComponent'"
    },
    {
        "extra_left": [],
        "left": "def has_overlap(self, interval) -> ",
        "right": ":\n        \"\"\"Check if self has overlap with `interval`.\n\n        Args:\n            interval: interval to be examined\n\n        Returns:\n            bool: True if self has overlap with `interval` otherwise False\n        \"\"\"\n        if self.begin < interval.end and interval.begin < self.end:\n            return True\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def shift(self, time) -> ",
        "right": ":\n        \"\"\"Return a new interval shifted by `time` from self\n\n        Args:\n            time: time to be shifted\n\n        Returns:\n            Interval: interval shifted by `time`\n        \"\"\"\n        return Interval(self._begin + time, self._end + time)",
        "return_type_from_source": "'Interval'"
    },
    {
        "extra_left": [],
        "left": "def shift(self, time) -> ",
        "right": ":\n        \"\"\"Return a new Timeslot shifted by `time`.\n\n        Args:\n            time: time to be shifted\n        \"\"\"\n        return Timeslot(self.interval.shift(time), self.channel)",
        "return_type_from_source": "'Timeslot'"
    },
    {
        "extra_left": [],
        "left": "def ch_start_time(self, *channels: List[Channel]) -> ",
        "right": ":\n        \"\"\"Return earliest start time in this collection.\n\n        Args:\n            *channels: Channels over which to obtain start_time.\n        \"\"\"\n        intervals = list(itertools.chain(*(self._table[chan] for chan in channels\n                                           if chan in self._table)))\n        if intervals:\n            return min((interval.begin for interval in intervals))\n        return 0",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def ch_stop_time(self, *channels: List[Channel]) -> ",
        "right": ":\n        \"\"\"Return maximum time of timeslots over all channels.\n\n        Args:\n            *channels: Channels over which to obtain stop time.\n        \"\"\"\n        intervals = list(itertools.chain(*(self._table[chan] for chan in channels\n                                           if chan in self._table)))\n        if intervals:\n            return max((interval.end for interval in intervals))\n        return 0",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def is_mergeable_with(self, timeslots) -> ",
        "right": ":\n        \"\"\"Return if self is mergeable with `timeslots`.\n\n        Args:\n            timeslots: TimeslotCollection to be checked\n        \"\"\"\n        for slot in timeslots.timeslots:\n            for interval in self._table[slot.channel]:\n                if slot.interval.has_overlap(interval):\n                    return False\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def merged(self, timeslots) -> ",
        "right": ":\n        \"\"\"Return a new TimeslotCollection merged with a specified `timeslots`\n\n        Args:\n            timeslots: TimeslotCollection to be merged\n        \"\"\"\n        slots = [Timeslot(slot.interval, slot.channel) for slot in self.timeslots]\n        slots.extend([Timeslot(slot.interval, slot.channel) for slot in timeslots.timeslots])\n        return TimeslotCollection(*slots)",
        "return_type_from_source": "'TimeslotCollection'"
    },
    {
        "extra_left": [],
        "left": "def shift(self, time) -> ",
        "right": ":\n        \"\"\"Return a new TimeslotCollection shifted by `time`.\n\n        Args:\n            time: time to be shifted by\n        \"\"\"\n        slots = [Timeslot(slot.interval.shift(time), slot.channel) for slot in self.timeslots]\n        return TimeslotCollection(*slots)",
        "return_type_from_source": "'TimeslotCollection'"
    },
    {
        "extra_left": [],
        "left": "def includes(self, lo_freq) -> ",
        "right": ":\n        \"\"\"Whether `lo_freq` is within the `LoRange`.\n\n        Args:\n            lo_freq: LO frequency to be checked\n\n        Returns:\n            bool: True if lo_freq is included in this range, otherwise False\n        \"\"\"\n        if self._lb <= lo_freq <= self._ub:\n            return True\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def constant(duration, amp, name = None) -> ",
        "right": ":\n    \"\"\"Generates constant-sampled `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Complex pulse amplitude.\n        name: Name of pulse.\n    \"\"\"\n    return _sampled_constant_pulse(duration, amp, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def zero(duration, name = None) -> ",
        "right": ":\n    \"\"\"Generates zero-sampled `SamplePulse`.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        name: Name of pulse.\n    \"\"\"\n    return _sampled_zero_pulse(duration, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def square(duration, amp, period = None,\n           phase = 0, name = None) -> ",
        "right": ":\n    \"\"\"Generates square wave `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude. Wave range is [-amp, amp].\n        period: Pulse period, units of dt. If `None` defaults to single cycle.\n        phase: Pulse phase.\n        name: Name of pulse.\n    \"\"\"\n    if period is None:\n        period = duration\n\n    return _sampled_square_pulse(duration, amp, period, phase=phase, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def sawtooth(duration, amp, period = None,\n             phase = 0, name = None) -> ",
        "right": ":\n    \"\"\"Generates sawtooth wave `SamplePulse`.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude. Wave range is [-amp, amp].\n        period: Pulse period, units of dt. If `None` defaults to single cycle.\n        phase: Pulse phase.\n        name: Name of pulse.\n    \"\"\"\n    if period is None:\n        period = duration\n\n    return _sampled_sawtooth_pulse(duration, amp, period, phase=phase, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def triangle(duration, amp, period = None,\n             phase = 0, name = None) -> ",
        "right": ":\n    \"\"\"Generates triangle wave `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude. Wave range is [-amp, amp].\n        period: Pulse period, units of dt. If `None` defaults to single cycle.\n        phase: Pulse phase.\n        name: Name of pulse.\n    \"\"\"\n    if period is None:\n        period = duration\n\n    return _sampled_triangle_pulse(duration, amp, period, phase=phase, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def cos(duration, amp, freq = None,\n        phase = 0, name = None) -> ",
        "right": ":\n    \"\"\"Generates cosine wave `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude.\n        freq: Pulse frequency, units of 1/dt. If `None` defaults to single cycle.\n        phase: Pulse phase.\n        name: Name of pulse.\n    \"\"\"\n    if freq is None:\n        freq = 1/duration\n\n    return _sampled_cos_pulse(duration, amp, freq, phase=phase, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def sin(duration, amp, freq = None,\n        phase = 0, name = None) -> ",
        "right": ":\n    \"\"\"Generates sine wave `SamplePulse`.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude.\n        freq: Pulse frequency, units of 1/dt. If `None` defaults to single cycle.\n        phase: Pulse phase.\n        name: Name of pulse.\n    \"\"\"\n    if freq is None:\n        freq = 1/duration\n\n    return _sampled_sin_pulse(duration, amp, freq, phase=phase, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def gaussian(duration, amp, sigma, name = None) -> ",
        "right": ":\n    r\"\"\"Generates unnormalized gaussian `SamplePulse`.\n\n    Centered at `duration/2` and zeroed at `t=-1` to prevent large initial discontinuity.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Integrated area under curve is $\\Omega_g(amp, sigma) = amp \\times np.sqrt(2\\pi \\sigma^2)$\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude at `duration/2`.\n        sigma: Width (standard deviation) of pulse.\n        name: Name of pulse.\n    \"\"\"\n    center = duration/2\n    zeroed_width = duration + 2\n    return _sampled_gaussian_pulse(duration, amp, center, sigma,\n                                   zeroed_width=zeroed_width, rescale_amp=True, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def gaussian_deriv(duration, amp, sigma, name = None) -> ",
        "right": ":\n    r\"\"\"Generates unnormalized gaussian derivative `SamplePulse`.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude at `center`.\n        sigma: Width (standard deviation) of pulse.\n        name: Name of pulse.\n    \"\"\"\n    center = duration/2\n    return _sampled_gaussian_deriv_pulse(duration, amp, center, sigma, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def gaussian_square(duration, amp, sigma,\n                    risefall, name = None) -> ",
        "right": ":\n    \"\"\"Generates gaussian square `SamplePulse`.\n\n    Centered at `duration/2` and zeroed at `t=-1` and `t=duration+1` to prevent\n    large initial/final discontinuities.\n\n    Applies `left` sampling strategy to generate discrete pulse from continuous function.\n\n    Args:\n        duration: Duration of pulse. Must be greater than zero.\n        amp: Pulse amplitude.\n        sigma: Width (standard deviation) of gaussian rise/fall portion of the pulse.\n        risefall: Number of samples over which pulse rise and fall happen. Width of\n            square portion of pulse will be `duration-2*risefall`.\n        name: Name of pulse.\n    \"\"\"\n    center = duration/2\n    width = duration-2*risefall\n    zeroed_width = duration + 2\n    return _sampled_gaussian_square_pulse(duration, amp, center, width, sigma,\n                                          zeroed_width=zeroed_width, name=name)",
        "return_type_from_source": "SamplePulse"
    },
    {
        "extra_left": [],
        "left": "def drive(self) -> ",
        "right": ":\n        \"\"\"Return the primary drive channel of this qubit.\"\"\"\n        if self._drives:\n            return self._drives[0]\n        else:\n            raise PulseError(\"No drive channels in q[%d]\" % self._index)",
        "return_type_from_source": "DriveChannel"
    },
    {
        "extra_left": [],
        "left": "def control(self) -> ",
        "right": ":\n        \"\"\"Return the primary control channel of this qubit.\"\"\"\n        if self._controls:\n            return self._controls[0]\n        else:\n            raise PulseError(\"No control channels in q[%d]\" % self._index)",
        "return_type_from_source": "ControlChannel"
    },
    {
        "extra_left": [],
        "left": "def measure(self) -> ",
        "right": ":\n        \"\"\"Return the primary measure channel of this qubit.\"\"\"\n        if self._measures:\n            return self._measures[0]\n        else:\n            raise PulseError(\"No measurement channels in q[%d]\" % self._index)",
        "return_type_from_source": "MeasureChannel"
    },
    {
        "extra_left": [],
        "left": "def acquire(self) -> ",
        "right": ":\n        \"\"\"Return the primary acquire channel of this qubit.\"\"\"\n        if self._acquires:\n            return self._acquires[0]\n        else:\n            raise PulseError(\"No acquire channels in q[%d]\" % self._index)",
        "return_type_from_source": "AcquireChannel"
    },
    {
        "extra_left": [],
        "left": "def _update_annotations(discretized_pulse) -> ",
        "right": ":\n    \"\"\"Update annotations of discretized continuous pulse function with duration.\n\n    Args:\n        discretized_pulse: Discretized decorated continuous pulse.\n    \"\"\"\n    undecorated_annotations = list(discretized_pulse.__annotations__.items())\n    decorated_annotations = undecorated_annotations[1:]\n    decorated_annotations.insert(0, ('duration', int))\n    discretized_pulse.__annotations__ = dict(decorated_annotations)\n    return discretized_pulse",
        "return_type_from_source": "Callable"
    },
    {
        "extra_left": [],
        "left": "def add_join_conditions(self, conditions) -> ",
        "right": ":\n        \"\"\"Adds an extra condition to an existing JOIN.\n\n        This allows you to for example do:\n\n            INNER JOIN othertable ON (mytable.id = othertable.other_id AND [extra conditions])\n\n        This does not work if nothing else in your query doesn't already generate the\n        initial join in the first place.\n        \"\"\"\n\n        alias = self.get_initial_alias()\n        opts = self.get_meta()\n\n        for name, value in conditions.items():\n            parts = name.split(LOOKUP_SEP)\n            join_info = self.setup_joins(parts, opts, alias, allow_many=True)\n            self.trim_joins(join_info[1], join_info[3], join_info[4])\n\n            target_table = join_info[3][-1]\n            field = join_info[1][-1]\n            join = self.alias_map.get(target_table)\n\n            if not join:\n                raise SuspiciousOperation((\n                    'Cannot add an extra join condition for \"%s\", there\\'s no'\n                    ' existing join to add it to.'\n                ) % target_table)\n\n            # convert the Join object into a ConditionalJoin object, which\n            # allows us to add the extra condition\n            if not isinstance(join, ConditionalJoin):\n                self.alias_map[target_table] = ConditionalJoin.from_join(join)\n                join = self.alias_map[target_table]\n\n            join.add_condition(field, value)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _format_field_name(self, field_name) -> ",
        "right": ":\n        \"\"\"Formats a field's name for usage in SQL.\n\n        Arguments:\n            field_name:\n                The field name to format.\n\n        Returns:\n            The specified field name formatted for\n            usage in SQL.\n        \"\"\"\n\n        field = self._get_model_field(field_name)\n        return self.qn(field.column)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _format_field_value(self, field_name) -> ",
        "right": ":\n        \"\"\"Formats a field's value for usage in SQL.\n\n        Arguments:\n            field_name:\n                The name of the field to format\n                the value of.\n\n        Returns:\n            The field's value formatted for usage\n            in SQL.\n        \"\"\"\n\n        field_name = self._normalize_field_name(field_name)\n        field = self._get_model_field(field_name)\n\n        return SQLInsertCompiler.prepare_value(\n            self,\n            field,\n            # Note: this deliberately doesn't use `pre_save_val` as we don't\n            # want things like auto_now on DateTimeField (etc.) to change the\n            # value. We rely on pre_save having already been done by the\n            # underlying compiler so that things like FileField have already had\n            # the opportunity to save out their data.\n            getattr(self.query.objs[0], field.attname)\n        )",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def add_condition(self, field, value) -> ",
        "right": ":\n        \"\"\"Adds an extra condition to this join.\n\n        Arguments:\n            field:\n                The field that the condition will apply to.\n\n            value:\n                The value to compare.\n        \"\"\"\n\n        self.extra_conditions.append((field, value))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def is_inside_lambda(node) -> ",
        "right": ":\n    \"\"\"Return true if given node is inside lambda\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, astroid.Lambda):\n            return True\n        parent = parent.parent\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_all_elements(\n    node\n) -> ",
        "right": ":\n    \"\"\"Recursively returns all atoms in nested lists and tuples.\"\"\"\n    if isinstance(node, (astroid.Tuple, astroid.List)):\n        for child in node.elts:\n            for e in get_all_elements(child):\n                yield e\n    else:\n        yield node",
        "return_type_from_source": "Iterable[astroid.node_classes.NodeNG]"
    },
    {
        "extra_left": [],
        "left": "def is_super(node) -> ",
        "right": ":\n    \"\"\"return True if the node is referencing the \"super\" builtin function\n    \"\"\"\n    if getattr(node, \"name\", None) == \"super\" and node.root().name == BUILTINS_NAME:\n        return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def is_error(node) -> ",
        "right": ":\n    \"\"\"return true if the function does nothing but raising an exception\"\"\"\n    for child_node in node.get_children():\n        if isinstance(child_node, astroid.Raise):\n            return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def is_default_argument(node) -> ",
        "right": ":\n    \"\"\"return true if the given Name node is used in function or lambda\n    default argument's value\n    \"\"\"\n    parent = node.scope()\n    if isinstance(parent, (astroid.FunctionDef, astroid.Lambda)):\n        for default_node in parent.args.defaults:\n            for default_name_node in default_node.nodes_of_class(astroid.Name):\n                if default_name_node is node:\n                    return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def is_func_decorator(node) -> ",
        "right": ":\n    \"\"\"return true if the name is used in function decorator\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, astroid.Decorators):\n            return True\n        if parent.is_statement or isinstance(\n            parent,\n            (astroid.Lambda, scoped_nodes.ComprehensionScope, scoped_nodes.ListComp),\n        ):\n            break\n        parent = parent.parent\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def is_ancestor_name(\n    frame, node\n) -> ",
        "right": ":\n    \"\"\"return True if `frame` is an astroid.Class node with `node` in the\n    subtree of its bases attribute\n    \"\"\"\n    try:\n        bases = frame.bases\n    except AttributeError:\n        return False\n    for base in bases:\n        if node in base.nodes_of_class(astroid.Name):\n            return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def assign_parent(node) -> ",
        "right": ":\n    \"\"\"return the higher parent which is not an AssignName, Tuple or List node\n    \"\"\"\n    while node and isinstance(node, (astroid.AssignName, astroid.Tuple, astroid.List)):\n        node = node.parent\n    return node",
        "return_type_from_source": "astroid.node_classes.NodeNG"
    },
    {
        "extra_left": [],
        "left": "def check_messages(*messages: str) -> ",
        "right": ":\n    \"\"\"decorator to store messages that are handled by a checker method\"\"\"\n\n    def store_messages(func):\n        func.checks_msgs = messages\n        return func\n\n    return store_messages",
        "return_type_from_source": "Callable"
    },
    {
        "extra_left": [],
        "left": "def collect_string_fields(format_string) -> ",
        "right": ":\n    \"\"\" Given a format string, return an iterator\n    of all the valid format fields. It handles nested fields\n    as well.\n    \"\"\"\n    formatter = string.Formatter()\n    try:\n        parseiterator = formatter.parse(format_string)\n        for result in parseiterator:\n            if all(item is None for item in result[1:]):\n                # not a replacement format\n                continue\n            name = result[1]\n            nested = result[2]\n            yield name\n            if nested:\n                for field in collect_string_fields(nested):\n                    yield field\n    except ValueError as exc:\n        # Probably the format string is invalid.\n        if exc.args[0].startswith(\"cannot switch from manual\"):\n            # On Jython, parsing a string with both manual\n            # and automatic positions will fail with a ValueError,\n            # while on CPython it will simply return the fields,\n            # the validation being done in the interpreter (?).\n            # We're just returning two mixed fields in order\n            # to trigger the format-combined-specification check.\n            yield \"\"\n            yield \"1\"\n            return\n        raise IncompleteFormatString(format_string)",
        "return_type_from_source": "Iterable[Optional[str]]"
    },
    {
        "extra_left": [],
        "left": "def get_argument_from_call(\n    call_node, position = None, keyword = None\n) -> ",
        "right": ":\n    \"\"\"Returns the specified argument from a function call.\n\n    :param astroid.Call call_node: Node representing a function call to check.\n    :param int position: position of the argument.\n    :param str keyword: the keyword of the argument.\n\n    :returns: The node representing the argument, None if the argument is not found.\n    :rtype: astroid.Name\n    :raises ValueError: if both position and keyword are None.\n    :raises NoSuchArgumentError: if no argument at the provided position or with\n    the provided keyword.\n    \"\"\"\n    if position is None and keyword is None:\n        raise ValueError(\"Must specify at least one of: position or keyword.\")\n    if position is not None:\n        try:\n            return call_node.args[position]\n        except IndexError:\n            pass\n    if keyword and call_node.keywords:\n        for arg in call_node.keywords:\n            if arg.arg == keyword:\n                return arg.value\n\n    raise NoSuchArgumentError",
        "return_type_from_source": "astroid.Name"
    },
    {
        "extra_left": [],
        "left": "def inherit_from_std_ex(node) -> ",
        "right": ":\n    \"\"\"\n    Return true if the given class node is subclass of\n    exceptions.Exception.\n    \"\"\"\n    ancestors = node.ancestors() if hasattr(node, \"ancestors\") else []\n    for ancestor in itertools.chain([node], ancestors):\n        if (\n            ancestor.name in (\"Exception\", \"BaseException\")\n            and ancestor.root().name == EXCEPTIONS_MODULE\n        ):\n            return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def error_of_type(handler, error_type) -> ",
        "right": ":\n    \"\"\"\n    Check if the given exception handler catches\n    the given error_type.\n\n    The *handler* parameter is a node, representing an ExceptHandler node.\n    The *error_type* can be an exception, such as AttributeError,\n    the name of an exception, or it can be a tuple of errors.\n    The function will return True if the handler catches any of the\n    given errors.\n    \"\"\"\n\n    def stringify_error(error):\n        if not isinstance(error, str):\n            return error.__name__\n        return error\n\n    if not isinstance(error_type, tuple):\n        error_type = (error_type,)  # type: ignore\n    expected_errors = {stringify_error(error) for error in error_type}  # type: ignore\n    if not handler.type:\n        return True\n    return handler.catch(expected_errors)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def decorated_with_property(node) -> ",
        "right": ":\n    \"\"\" Detect if the given function node is decorated with a property. \"\"\"\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, astroid.Name):\n            continue\n        try:\n            if _is_property_decorator(decorator):\n                return True\n        except astroid.InferenceError:\n            pass\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def decorated_with(func, qnames) -> ",
        "right": ":\n    \"\"\"Determine if the `func` node has a decorator with the qualified name `qname`.\"\"\"\n    decorators = func.decorators.nodes if func.decorators else []\n    for decorator_node in decorators:\n        try:\n            if any(\n                i is not None and i.qname() in qnames for i in decorator_node.infer()\n            ):\n                return True\n        except astroid.InferenceError:\n            continue\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def is_from_fallback_block(node) -> ",
        "right": ":\n    \"\"\"Check if the given node is from a fallback import block.\"\"\"\n    context = find_try_except_wrapper_node(node)\n    if not context:\n        return False\n\n    if isinstance(context, astroid.ExceptHandler):\n        other_body = context.parent.body\n        handlers = context.parent.handlers\n    else:\n        other_body = itertools.chain.from_iterable(\n            handler.body for handler in context.handlers\n        )\n        handlers = context.handlers\n\n    has_fallback_imports = any(\n        isinstance(import_node, (astroid.ImportFrom, astroid.Import))\n        for import_node in other_body\n    )\n    ignores_import_error = _except_handlers_ignores_exception(handlers, ImportError)\n    return ignores_import_error or has_fallback_imports",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_exception_handlers(\n    node, exception=Exception\n) -> ",
        "right": ":\n    \"\"\"Return the collections of handlers handling the exception in arguments.\n\n    Args:\n        node (astroid.NodeNG): A node that is potentially wrapped in a try except.\n        exception (builtin.Exception or str): exception or name of the exception.\n\n    Returns:\n        list: the collection of handlers that are handling the exception or None.\n\n    \"\"\"\n    context = find_try_except_wrapper_node(node)\n    if isinstance(context, astroid.TryExcept):\n        return [\n            handler for handler in context.handlers if error_of_type(handler, exception)\n        ]\n    return None",
        "return_type_from_source": "List[astroid.ExceptHandler]"
    },
    {
        "extra_left": [],
        "left": "def node_ignores_exception(\n    node, exception=Exception\n) -> ",
        "right": ":\n    \"\"\"Check if the node is in a TryExcept which handles the given exception.\n\n    If the exception is not given, the function is going to look for bare\n    excepts.\n    \"\"\"\n    managing_handlers = get_exception_handlers(node, exception)\n    if not managing_handlers:\n        return False\n    return any(managing_handlers)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def class_is_abstract(node) -> ",
        "right": ":\n    \"\"\"return true if the given class node should be considered as an abstract\n    class\n    \"\"\"\n    for method in node.methods():\n        if method.parent.frame() is node:\n            if method.is_abstract(pass_is_abstract=False):\n                return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def safe_infer(\n    node, context=None\n) -> ",
        "right": ":\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except astroid.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except astroid.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value",
        "return_type_from_source": "Optional[astroid.node_classes.NodeNG]"
    },
    {
        "extra_left": [],
        "left": "def node_type(node) -> ",
        "right": ":\n    \"\"\"Return the inferred type for `node`\n\n    If there is more than one possible type, or if inferred type is Uninferable or None,\n    return None\n    \"\"\"\n    # check there is only one possible type for the assign node. Else we\n    # don't handle it for now\n    types = set()\n    try:\n        for var_type in node.infer():\n            if var_type == astroid.Uninferable or is_none(var_type):\n                continue\n            types.add(var_type)\n            if len(types) > 1:\n                return None\n    except astroid.InferenceError:\n        return None\n    return types.pop() if types else None",
        "return_type_from_source": "Optional[type]"
    },
    {
        "extra_left": [],
        "left": "def is_registered_in_singledispatch_function(node) -> ",
        "right": ":\n    \"\"\"Check if the given function node is a singledispatch function.\"\"\"\n\n    singledispatch_qnames = (\n        \"functools.singledispatch\",\n        \"singledispatch.singledispatch\",\n    )\n\n    if not isinstance(node, astroid.FunctionDef):\n        return False\n\n    decorators = node.decorators.nodes if node.decorators else []\n    for decorator in decorators:\n        # func.register are function calls\n        if not isinstance(decorator, astroid.Call):\n            continue\n\n        func = decorator.func\n        if not isinstance(func, astroid.Attribute) or func.attrname != \"register\":\n            continue\n\n        try:\n            func_def = next(func.expr.infer())\n        except astroid.InferenceError:\n            continue\n\n        if isinstance(func_def, astroid.FunctionDef):\n            # pylint: disable=redundant-keyword-arg; some flow inference goes wrong here\n            return decorated_with(func_def, singledispatch_qnames)\n\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def is_postponed_evaluation_enabled(node) -> ",
        "right": ":\n    \"\"\"Check if the postponed evaluation of annotations is enabled\"\"\"\n    name = \"annotations\"\n    module = node.root()\n    stmt = module.locals.get(name)\n    return (\n        stmt\n        and isinstance(stmt[0], astroid.ImportFrom)\n        and stmt[0].modname == \"__future__\"\n    )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def check_consistency(self) -> ",
        "right": ":\n        \"\"\"Check the consistency of msgid.\n\n        msg ids for a checker should be a string of len 4, where the two first\n        characters are the checker id and the two last the msg id in this\n        checker.\n\n        :raises InvalidMessageError: If the checker id in the messages are not\n        always the same. \"\"\"\n        checker_id = None\n        existing_ids = []\n        for message in self.messages:\n            if checker_id is not None and checker_id != message.msgid[1:3]:\n                error_msg = \"Inconsistent checker part in message id \"\n                error_msg += \"'{}' (expected 'x{checker_id}xx' \".format(\n                    message.msgid, checker_id=checker_id\n                )\n                error_msg += \"because we already had {existing_ids}).\".format(\n                    existing_ids=existing_ids\n                )\n                raise InvalidMessageError(error_msg)\n            checker_id = message.msgid[1:3]\n            existing_ids.append(message.msgid)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_message_definitions(self, msgid_or_symbol) -> ",
        "right": ":\n        \"\"\"Returns the Message object for this message.\n\n        :param str msgid_or_symbol: msgid_or_symbol may be either a numeric or symbolic id.\n        :raises UnknownMessageError: if the message id is not defined.\n        :rtype: List of MessageDefinition\n        :return: A message definition corresponding to msgid_or_symbol\n        \"\"\"\n        if msgid_or_symbol[1:].isdigit():\n            msgid_or_symbol = msgid_or_symbol.upper()\n        for source in (self._alternative_names, self._messages_definitions):\n            try:\n                return [source[msgid_or_symbol]]\n            except KeyError:\n                pass\n        error_msg = \"No such message id or symbol '{msgid_or_symbol}'.\".format(\n            msgid_or_symbol=msgid_or_symbol\n        )\n        raise UnknownMessageError(error_msg)",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def _cpu_count() -> ",
        "right": ":\n    \"\"\"Use sched_affinity if available for virtualized or containerized environments.\"\"\"\n    sched_getaffinity = getattr(os, \"sched_getaffinity\", None)\n    # pylint: disable=not-callable,using-constant-test\n    if sched_getaffinity:\n        return len(sched_getaffinity(0))\n    if multiprocessing:\n        return multiprocessing.cpu_count()\n    return 1",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def _is_raising(body) -> ",
        "right": ":\n    \"\"\"Return true if the given statement node raise an exception\"\"\"\n    for node in body:\n        if isinstance(node, astroid.Raise):\n            return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _is_typing_namedtuple(node) -> ",
        "right": ":\n    \"\"\"Check if a class node is a typing.NamedTuple class\"\"\"\n    for base in node.ancestors():\n        if base.qname() == TYPING_NAMEDTUPLE:\n            return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _is_enum_class(node) -> ",
        "right": ":\n    \"\"\"Check if a class definition defines an Enum class.\n\n    :param node: The class node to check.\n    :type node: astroid.ClassDef\n\n    :returns: True if the given node represents an Enum class. False otherwise.\n    :rtype: bool\n    \"\"\"\n    for base in node.bases:\n        try:\n            inferred_bases = base.inferred()\n        except astroid.InferenceError:\n            continue\n\n        for ancestor in inferred_bases:\n            if not isinstance(ancestor, astroid.ClassDef):\n                continue\n\n            if ancestor.name == \"Enum\" and ancestor.root().name == \"enum\":\n                return True\n\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _is_dataclass(node) -> ",
        "right": ":\n    \"\"\"Check if a class definition defines a Python 3.7+ dataclass\n\n    :param node: The class node to check.\n    :type node: astroid.ClassDef\n\n    :returns: True if the given node represents a dataclass class. False otherwise.\n    :rtype: bool\n    \"\"\"\n    if not node.decorators:\n        return False\n\n    root_locals = node.root().locals\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, astroid.Call):\n            decorator = decorator.func\n        if not isinstance(decorator, (astroid.Name, astroid.Attribute)):\n            continue\n        if isinstance(decorator, astroid.Name):\n            name = decorator.name\n        else:\n            name = decorator.attrname\n        if name == DATACLASS_DECORATOR and DATACLASS_DECORATOR in root_locals:\n            return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def from_json(f) -> ",
        "right": ":\n        \"\"\"Return a PrecalculatedTextMeasurer given a JSON stream.\n\n        See precalculate_text.py for details on the required format.\n        \"\"\"\n        o = json.load(f)\n        return PrecalculatedTextMeasurer(o['mean-character-length'],\n                                         o['character-lengths'],\n                                         o['kerning-pairs'])",
        "return_type_from_source": "'PrecalculatedTextMeasurer'"
    },
    {
        "extra_left": [],
        "left": "def default(cls) -> ",
        "right": ":\n        \"\"\"Returns a reasonable default PrecalculatedTextMeasurer.\"\"\"\n        if cls._default_cache is not None:\n            return cls._default_cache\n\n        if pkg_resources.resource_exists(__name__, 'default-widths.json.xz'):\n            import lzma\n            with pkg_resources.resource_stream(__name__,\n                                               'default-widths.json.xz') as f:\n                with lzma.open(f, \"rt\") as g:\n                    cls._default_cache = PrecalculatedTextMeasurer.from_json(\n                        cast(TextIO, g))\n                    return cls._default_cache\n        elif pkg_resources.resource_exists(__name__, 'default-widths.json'):\n            with pkg_resources.resource_stream(__name__,\n                                               'default-widths.json') as f:\n                cls._default_cache = PrecalculatedTextMeasurer.from_json(\n                    io.TextIOWrapper(f, encoding='utf-8'))\n                return cls._default_cache\n        else:\n            raise ValueError('could not load default-widths.json')",
        "return_type_from_source": "'PrecalculatedTextMeasurer'"
    },
    {
        "extra_left": [],
        "left": "def badge(left_text, right_text, left_link = None,\n          right_link = None,\n          whole_link = None, logo = None,\n          left_color = '#555', right_color = '#007ec6',\n          measurer = None,\n          embed_logo = False) -> ",
        "right": ":\n    \"\"\"Creates a github-style badge as an SVG image.\n\n    >>> badge(left_text='coverage', right_text='23%', right_color='red')\n    '<svg...</svg>'\n    >>> badge(left_text='build', right_text='green', right_color='green',\n    ...       whole_link=\"http://www.example.com/\")\n    '<svg...</svg>'\n\n    Args:\n        left_text: The text that should appear on the left-hand-side of the\n            badge e.g. \"coverage\".\n        right_text: The text that should appear on the right-hand-side of the\n            badge e.g. \"23%\".\n        left_link: The URL that should be redirected to when the left-hand text\n            is selected.\n        right_link: The URL that should be redirected to when the right-hand\n            text is selected.\n        whole_link: The link that should be redirected to when the badge is\n            selected. If set then left_link and right_right may not be set.\n        logo: A url representing a logo that will be displayed inside the\n            badge. Can be a data URL e.g. \"data:image/svg+xml;utf8,<svg...\"\n        left_color: The color of the part of the badge containing the left-hand\n            text. Can be an valid CSS color\n            (see https://developer.mozilla.org/en-US/docs/Web/CSS/color) or a\n            color name defined here:\n            https://github.com/badges/shields/blob/master/lib/colorscheme.json\n        right_color: The color of the part of the badge containing the\n            right-hand text. Can be an valid CSS color\n            (see https://developer.mozilla.org/en-US/docs/Web/CSS/color) or a\n            color name defined here:\n            https://github.com/badges/shields/blob/master/lib/colorscheme.json\n        measurer: A text_measurer.TextMeasurer that can be used to measure the\n            width of left_text and right_text.\n        embed_logo: If True then embed the logo image directly in the badge.\n            This can prevent an HTTP request and some browsers will not render\n            external image referenced. When True, `logo` must be a HTTP/HTTPS\n            URI or a filesystem path. Also, the `badge` call may raise an\n            exception if the logo cannot be loaded, is not an image, etc.\n    \"\"\"\n    if measurer is None:\n        measurer = (\n            precalculated_text_measurer.PrecalculatedTextMeasurer\n                .default())\n\n    if (left_link or right_link) and whole_link:\n        raise ValueError(\n            'whole_link may not bet set with left_link or right_link')\n    template = _JINJA2_ENVIRONMENT.get_template('badge-template-full.svg')\n\n    if logo and embed_logo:\n        logo = _embed_image(logo)\n\n    svg = template.render(\n        left_text=left_text,\n        right_text=right_text,\n        left_text_width=measurer.text_width(left_text) / 10.0,\n        right_text_width=measurer.text_width(right_text) / 10.0,\n        left_link=left_link,\n        right_link=right_link,\n        whole_link=whole_link,\n        logo=logo,\n        left_color=_NAME_TO_COLOR.get(left_color, left_color),\n        right_color=_NAME_TO_COLOR.get(right_color, right_color),\n    )\n    xml = minidom.parseString(svg)\n    _remove_blanks(xml)\n    xml.normalize()\n    return xml.documentElement.toxml()",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def generate_supported_characters(deja_vu_sans_path) -> ",
        "right": ":\n    \"\"\"Generate the characters support by the font at the given path.\"\"\"\n    font = ttLib.TTFont(deja_vu_sans_path)\n    for cmap in font['cmap'].tables:\n        if cmap.isUnicode():\n            for code in cmap.cmap:\n                yield chr(code)",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def generate_encodeable_characters(characters,\n                                   encodings) -> ",
        "right": ":\n    \"\"\"Generates the subset of 'characters' that can be encoded by 'encodings'.\n\n    Args:\n        characters: The characters to check for encodeability e.g. 'abcd'.\n        encodings: The encodings to check against e.g. ['cp1252', 'iso-8859-5'].\n\n    Returns:\n        The subset of 'characters' that can be encoded using one of the provided\n        encodings.\n    \"\"\"\n    for c in characters:\n        for encoding in encodings:\n            try:\n                c.encode(encoding)\n                yield c\n            except UnicodeEncodeError:\n                pass",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def write_json(f, deja_vu_sans_path,\n               measurer,\n               encodings) -> ",
        "right": ":\n    \"\"\"Write the data required by PrecalculatedTextMeasurer to a stream.\"\"\"\n    supported_characters = list(\n        generate_supported_characters(deja_vu_sans_path))\n    kerning_characters = ''.join(\n        generate_encodeable_characters(supported_characters, encodings))\n    char_to_length = calculate_character_to_length_mapping(measurer,\n                                                           supported_characters)\n    pair_to_kerning = calculate_pair_to_kern_mapping(measurer, char_to_length,\n                                                     kerning_characters)\n    json.dump(\n        {'mean-character-length': statistics.mean(char_to_length.values()),\n         'character-lengths': char_to_length,\n         'kerning-characters': kerning_characters,\n         'kerning-pairs': pair_to_kerning},\n        f, sort_keys=True, indent=1)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "async def get_albums(self, *, limit: Optional[int] = 20, offset: Optional[int] = 0, include_groups=None, market: Optional[str] = None) -> ",
        "right": ":\n        \"\"\"Get the albums of a Spotify artist.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The maximum number of items to return. Default: 20. Minimum: 1. Maximum: 50.\n        offset : Optiona[int]\n            The offset of which Spotify should start yielding from.\n        include_groups : INCLUDE_GROUPS_TP\n            INCLUDE_GROUPS\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n\n        Returns\n        -------\n        albums : List[Album]\n            The albums of the artist.\n        \"\"\"\n        from .album import Album\n\n        data = await self.__client.http.artist_albums(self.id, limit=limit, offset=offset, include_groups=include_groups, market=market)\n        return list(Album(self.__client, item) for item in data['items'])",
        "return_type_from_source": "List[Album]"
    },
    {
        "extra_left": [],
        "left": "async def get_all_albums(self, *, market='US') -> ",
        "right": ":\n        \"\"\"loads all of the artists albums, depending on how many the artist has this may be a long operation.\n\n        Parameters\n        ----------\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n\n        Returns\n        -------\n        albums : List[Album]\n            The albums of the artist.\n        \"\"\"\n        from .album import Album\n\n        albums = []\n        offset = 0\n        total = await self.total_albums(market=market)\n\n        while len(albums) < total:\n            data = await self.__client.http.artist_albums(self.id, limit=50, offset=offset, market=market)\n\n            offset += 50\n            albums += list(Album(self.__client, item) for item in data['items'])\n\n        return albums",
        "return_type_from_source": "List[Album]"
    },
    {
        "extra_left": [],
        "left": "async def total_albums(self, *, market: str = None) -> ",
        "right": ":\n        \"\"\"get the total amout of tracks in the album.\n\n        Parameters\n        ----------\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code.\n\n        Returns\n        -------\n        total : int\n            The total amount of albums.\n        \"\"\"\n        data = await self.__client.http.artist_albums(self.id, limit=1, offset=0, market=market)\n        return data['total']",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "async def related_artists(self) -> ",
        "right": ":\n        \"\"\"Get Spotify catalog information about artists similar to a given artist.\n\n        Similarity is based on analysis of the Spotify community\u2019s listening history.\n\n        Returns\n        -------\n        artists : List[Artits]\n            The artists deemed similar.\n        \"\"\"\n        related = await self.__client.http.artist_related_artists(self.id)\n        return list(Artist(self.__client, item) for item in related['artists'])",
        "return_type_from_source": "List[Artist]"
    },
    {
        "extra_left": [],
        "left": "async def get_player(self) -> ",
        "right": ":\n        \"\"\"Get information about the users current playback.\n\n        Returns\n        -------\n        player : Player\n            A player object representing the current playback.\n        \"\"\"\n        self._player = player = Player(self.__client, self, await self.http.current_player())\n        return player",
        "return_type_from_source": "Player"
    },
    {
        "extra_left": [],
        "left": "async def get_devices(self) -> ",
        "right": ":\n        \"\"\"Get information about the users avaliable devices.\n\n        Returns\n        -------\n        devices : List[Device]\n            The devices the user has available.\n        \"\"\"\n        data = await self.http.available_devices()\n        return [Device(item) for item in data['devices']]",
        "return_type_from_source": "List[Device]"
    },
    {
        "extra_left": [],
        "left": "async def replace_tracks(self, playlist, *tracks) -> ",
        "right": ":\n        \"\"\"Replace all the tracks in a playlist, overwriting its existing tracks. \n        This powerful request can be useful for replacing tracks, re-ordering existing tracks, or clearing the playlist.\n\n        Parameters\n        ----------\n        playlist : Union[str, PLaylist]\n            The playlist to modify\n        tracks : Sequence[Union[str, Track]]\n            Tracks to place in the playlist\n        \"\"\"\n        tracks = [str(track) for track in tracks]\n        await self.http.replace_playlist_tracks(self.id, str(playlist), tracks=','.join(tracks))",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "async def get_tracks(self, *, limit: Optional[int] = 20, offset: Optional[int] = 0) -> ",
        "right": ":\n        \"\"\"get the albums tracks from spotify.\n\n        Parameters\n        ----------\n        limit : Optional[int]\n            The limit on how many tracks to retrieve for this album (default is 20).\n        offset : Optional[int]\n            The offset from where the api should start from in the tracks.\n        \n        Returns\n        -------\n        tracks : List[Track]\n            The tracks of the artist.\n        \"\"\"\n        data = await self.__client.http.album_tracks(self.id, limit=limit, offset=offset)\n        return list(Track(self.__client, item) for item in data['items'])",
        "return_type_from_source": "List[Track]"
    },
    {
        "extra_left": [],
        "left": "async def get_all_tracks(self, *, market: Optional[str] = 'US') -> ",
        "right": ":\n        \"\"\"loads all of the albums tracks, depending on how many the album has this may be a long operation.\n\n        Parameters\n        ----------\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code. Provide this parameter if you want to apply Track Relinking.\n        \n        Returns\n        -------\n        tracks : List[Track]\n            The tracks of the artist.\n        \"\"\"\n        tracks = []\n        offset = 0\n        total = self.total_tracks or None\n\n        while True:\n            data = await self.__client.http.album_tracks(self.id, limit=50, offset=offset, market=market)\n\n            if total is None:\n                total = data['total']\n\n            offset += 50\n            tracks += list(Track(self.__client, item) for item in data['items'])\n\n            if len(tracks) >= total:\n                break\n\n        return tracks",
        "return_type_from_source": "List[Track]"
    },
    {
        "extra_left": [],
        "left": "def oauth2_url(self, redirect_uri, scope = None, state = None) -> ",
        "right": ":\n        \"\"\"Generate an outh2 url for user authentication.\n\n        Parameters\n        ----------\n        redirect_uri : str\n            Where spotify should redirect the user to after authentication.\n        scope : Optional[str]\n            Space seperated spotify scopes for different levels of access.\n        state : Optional[str]\n            Using a state value can increase your assurance that an incoming connection is the result of an authentication request.\n\n        Returns\n        -------\n        url : str\n            The OAuth2 url.\n        \"\"\"\n        return OAuth2.url_(self.http.client_id, redirect_uri, scope=scope, state=state)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "async def get_album(self, spotify_id, *, market: str = 'US') -> ",
        "right": ":\n        \"\"\"Retrive an album with a spotify ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The ID to search for.\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code\n\n        Returns\n        -------\n        album : Album\n            The album from the ID\n        \"\"\"\n        data = await self.http.album(to_id(spotify_id), market=market)\n        return Album(self, data)",
        "return_type_from_source": "Album"
    },
    {
        "extra_left": [],
        "left": "async def get_artist(self, spotify_id) -> ",
        "right": ":\n        \"\"\"Retrive an artist with a spotify ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The ID to search for.\n\n        Returns\n        -------\n        artist : Artist\n            The artist from the ID\n        \"\"\"\n        data = await self.http.artist(to_id(spotify_id))\n        return Artist(self, data)",
        "return_type_from_source": "Artist"
    },
    {
        "extra_left": [],
        "left": "async def get_track(self, spotify_id) -> ",
        "right": ":\n        \"\"\"Retrive an track with a spotify ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The ID to search for.\n\n        Returns\n        -------\n        track : Track\n            The track from the ID\n        \"\"\"\n        data = await self.http.track(to_id(spotify_id))\n        return Track(self, data)",
        "return_type_from_source": "Track"
    },
    {
        "extra_left": [],
        "left": "async def get_user(self, spotify_id) -> ",
        "right": ":\n        \"\"\"Retrive an user with a spotify ID.\n\n        Parameters\n        ----------\n        spotify_id : str\n            The ID to search for.\n\n        Returns\n        -------\n        user : User\n            The user from the ID\n        \"\"\"\n        data = await self.http.user(to_id(spotify_id))\n        return User(self, data)",
        "return_type_from_source": "User"
    },
    {
        "extra_left": [],
        "left": "async def get_albums(self, *ids: List[str], market: str = 'US') -> ",
        "right": ":\n        \"\"\"Retrive multiple albums with a list of spotify IDs.\n\n        Parameters\n        ----------\n        ids : List[str]\n            the ID to look for\n        market : Optional[str]\n            An ISO 3166-1 alpha-2 country code\n\n        Returns\n        -------\n        albums : List[Album]\n            The albums from the IDs\n        \"\"\"\n        data = await self.http.albums(','.join(to_id(_id) for _id in ids), market=market)\n        return list(Album(self, album) for album in data['albums'])",
        "return_type_from_source": "List[Album]"
    },
    {
        "extra_left": [],
        "left": "async def get_artists(self, *ids: List[str]) -> ",
        "right": ":\n        \"\"\"Retrive multiple artists with a list of spotify IDs.\n\n        Parameters\n        ----------\n        ids : List[str]\n            the IDs to look for\n\n        Returns\n        -------\n        artists : List[Artist]\n            The artists from the IDs\n        \"\"\"\n        data = await self.http.artists(','.join(to_id(_id) for _id in ids))\n        return list(Artist(self, artist) for artist in data['artists'])",
        "return_type_from_source": "List[Artist]"
    },
    {
        "extra_left": [],
        "left": "def to_id(string) -> ",
        "right": ":\n    \"\"\"Get a spotify ID from a URI or open.spotify URL.\n\n    Paramters\n    ---------\n    string : str\n        The string to operate on.\n\n    Returns\n    -------\n    id : str\n        The Spotify ID from the string.\n    \"\"\"\n    string = string.strip()\n\n    match = _URI_RE.match(string)\n\n    if match is None:\n        match = _OPEN_RE.match(string)\n\n        if match is None:\n            return string\n        else:\n            return match.group(2)\n    else:\n        return match.group(1)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def url_(client_id, redirect_uri, *, scope: str = None, state: str = None, secure: bool = True) -> ",
        "right": ":\n        \"\"\"Construct a OAuth2 URL instead of an OAuth2 object.\"\"\"\n        attrs = {\n            'client_id': client_id,\n            'redirect_uri': quote(redirect_uri)\n        }\n\n        if scope is not None:\n            attrs['scope'] = quote(scope)\n\n        if state is not None:\n            attrs['state'] = state\n\n        parameters = '&'.join('{0}={1}'.format(*item) for item in attrs.items())\n\n        return OAuth2._BASE.format(parameters=parameters)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def parameters(self) -> ",
        "right": ":\n        \"\"\"URL parameters used.\"\"\"\n        return '&'.join('{0}={1}'.format(*item) for item in self.attrs.items())",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "async def get_all_tracks(self) -> ",
        "right": ":\n        \"\"\"Get all playlist tracks from the playlist.\n\n        Returns\n        -------\n        tracks : List[PlaylistTrack]\n            The playlists tracks.\n        \"\"\"\n        if isinstance(self._tracks, PartialTracks):\n            return await self._tracks.build()\n\n        _tracks = []\n        offset = 0\n        while len(self.tracks) < self.total_tracks:\n            data = await self.__client.http.get_playlist_tracks(self.owner.id, self.id, limit=50, offset=offset)\n\n            _tracks += [PlaylistTrack(self.__client, item) for item in data['items']]\n            offset += 50\n\n        self.total_tracks = len(self._tracks)\n        return list(self._tracks)",
        "return_type_from_source": "List[PlaylistTrack]"
    },
    {
        "extra_left": [],
        "left": "def decompress_G1(z) -> ",
        "right": ":\n    \"\"\"\n    Recovers x and y coordinates from the compressed point.\n    \"\"\"\n    # b_flag == 1 indicates the infinity point\n    b_flag = (z % POW_2_383) // POW_2_382\n    if b_flag == 1:\n        return Z1\n    x = z % POW_2_381\n\n    # Try solving y coordinate from the equation Y^2 = X^3 + b\n    # using quadratic residue\n    y = pow((x**3 + b.n) % q, (q + 1) // 4, q)\n\n    if pow(y, 2, q) != (x**3 + b.n) % q:\n        raise ValueError(\n            \"The given point is not on G1: y**2 = x**3 + b\"\n        )\n    # Choose the y whose leftmost bit is equal to the a_flag\n    a_flag = (z % POW_2_382) // POW_2_381\n    if (y * 2) // q != a_flag:\n        y = q - y\n    return (FQ(x), FQ(y), FQ(1))",
        "return_type_from_source": "G1Uncompressed"
    },
    {
        "extra_left": [],
        "left": "def prime_field_inv(a, n) -> ",
        "right": ":\n    \"\"\"\n    Extended euclidean algorithm to find modular inverses for integers\n    \"\"\"\n    if a == 0:\n        return 0\n    lm, hm = 1, 0\n    low, high = a % n, n\n    while low > 1:\n        r = high // low\n        nm, new = hm - lm * r, high - low * r\n        lm, low, hm, high = nm, new, lm, low\n    return lm % n",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def items(self) -> ",
        "right": ":\n        \"\"\"Return items as a list of strings.\n\n        Don't include sub-items and the start pattern.\n        \"\"\"\n        items = []  # type: List[str]\n        append = items.append\n        string = self.string\n        match = self._match\n        ms = match.start()\n        for s, e in match.spans('item'):\n            append(string[s - ms:e - ms])\n        return items",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def sublists(\n        self, i = None, pattern = None\n    ) -> ",
        "right": ":\n        \"\"\"Return the Lists inside the item with the given index.\n\n        :param i: The index if the item which its sub-lists are desired.\n            The performance is likely to be better if `i` is None.\n\n        :param pattern: The starting symbol for the desired sub-lists.\n            The `pattern` of the current list will be automatically added\n            as prefix.\n            Although this parameter is optional, but specifying it can improve\n            the performance.\n        \"\"\"\n        patterns = (r'\\#', r'\\*', '[:;]') if pattern is None \\\n            else (pattern,)  # type: Tuple[str, ...]\n        self_pattern = self.pattern\n        lists = self.lists\n        sublists = []  # type: List['WikiList']\n        sublists_append = sublists.append\n        if i is None:\n            # Any sublist is acceptable\n            for pattern in patterns:\n                for lst in lists(self_pattern + pattern):\n                    sublists_append(lst)\n            return sublists\n        # Only return sub-lists that are within the given item\n        match = self._match\n        fullitem_spans = match.spans('fullitem')\n        ss = self._span[0]\n        ms = match.start()\n        s, e = fullitem_spans[i]\n        e -= ms - ss\n        s -= ms - ss\n        for pattern in patterns:\n            for lst in lists(self_pattern + pattern):\n                # noinspection PyProtectedMember\n                ls, le = lst._span\n                if s < ls and le <= e:\n                    sublists_append(lst)\n        return sublists",
        "return_type_from_source": "List['WikiList']"
    },
    {
        "extra_left": [],
        "left": "def convert(self, newstart) -> ",
        "right": ":\n        \"\"\"Convert to another list type by replacing starting pattern.\"\"\"\n        match = self._match\n        ms = match.start()\n        for s, e in reversed(match.spans('pattern')):\n            self[s - ms:e - ms] = newstart\n        self.pattern = escape(newstart)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def arguments(self) -> ",
        "right": ":\n        \"\"\"Parse template content. Create self.name and self.arguments.\"\"\"\n        shadow = self._shadow\n        split_spans = self._args_matcher(shadow).spans('arg')\n        if not split_spans:\n            return []\n        arguments = []\n        arguments_append = arguments.append\n        type_to_spans = self._type_to_spans\n        ss, se = span = self._span\n        type_ = id(span)\n        lststr = self._lststr\n        string = lststr[0]\n        arg_spans = type_to_spans.setdefault(type_, [])\n        span_tuple_to_span_get = {(s[0], s[1]): s for s in arg_spans}.get\n        for arg_self_start, arg_self_end in split_spans:\n            s, e = arg_span = [ss + arg_self_start, ss + arg_self_end]\n            old_span = span_tuple_to_span_get((s, e))\n            if old_span is None:\n                insort(arg_spans, arg_span)\n            else:\n                arg_span = old_span\n            arg = Argument(lststr, type_to_spans, arg_span, type_)\n            arg._shadow_cache = (\n                string[s:e], shadow[arg_self_start:arg_self_end])\n            arguments_append(arg)\n        return arguments",
        "return_type_from_source": "List[Argument]"
    },
    {
        "extra_left": [],
        "left": "def lists(self, pattern = None) -> ",
        "right": ":\n        \"\"\"Return the lists in all arguments.\n\n        For performance reasons it is usually preferred to get a specific\n        Argument and use the `lists` method of that argument instead.\n        \"\"\"\n        return [\n            lst for arg in self.arguments for lst in arg.lists(pattern) if lst]",
        "return_type_from_source": "List[WikiList]"
    },
    {
        "extra_left": [],
        "left": "def _plant_trie(strings) -> ",
        "right": ":\n    \"\"\"Create a Trie out of a list of words and return an atomic regex pattern.\n\n    The corresponding Regex should match much faster than a simple Regex union.\n    \"\"\"\n    # plant the trie\n    trie = {}\n    for string in strings:\n        d = trie\n        for char in string:\n            d[char] = char in d and d[char] or {}\n            d = d[char]\n        d[''] = None  # EOS\n    return trie",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _pattern(trie) -> ",
        "right": ":\n    \"\"\"Convert a trie to a regex pattern.\"\"\"\n    if '' in trie:\n        if len(trie) == 1:\n            return ''\n        optional = True\n        del trie['']\n    else:\n        optional = False\n\n    subpattern_to_chars = _defaultdict(list)\n\n    for char, sub_trie in trie.items():\n        subpattern = _pattern(sub_trie)\n        subpattern_to_chars[subpattern].append(char)\n\n    alts = []\n    for subpattern, chars in subpattern_to_chars.items():\n        if len(chars) == 1:\n            alts.append(chars[0] + subpattern)\n        else:\n            chars.sort(reverse=True)\n            alts.append('[' + ''.join(chars) + ']' + subpattern)\n\n    if len(alts) == 1:\n        result = alts[0]\n        if optional:\n            if len(result) == 1:\n                result += '?+'\n            else:  # more than one character in alts[0]\n                result = '(?:' + result + ')?+'\n    else:\n        alts.sort(reverse=True)\n        result = '(?>' + '|'.join(alts) + ')'\n        if optional:\n            result += '?+'\n    return result",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _check_index(self, key) -> ",
        "right": ":\n        \"\"\"Return adjusted start and stop index as tuple.\n\n        Used in  __setitem__ and __delitem__.\n        \"\"\"\n        ss, se = self._span\n        if isinstance(key, int):\n            if key < 0:\n                key += se - ss\n                if key < 0:\n                    raise IndexError('index out of range')\n            elif key >= se - ss:\n                raise IndexError('index out of range')\n            start = ss + key\n            return start, start + 1\n        # isinstance(key, slice)\n        if key.step is not None:\n            raise NotImplementedError(\n                'step is not implemented for string setter.')\n        start, stop = key.start or 0, key.stop\n        if start < 0:\n            start += se - ss\n            if start < 0:\n                raise IndexError('start index out of range')\n        if stop is None:\n            stop = se - ss\n        elif stop < 0:\n            stop += se - ss\n        if start > stop:\n            raise IndexError(\n                'stop index out of range or start is after the stop')\n        return start + ss, stop + ss",
        "return_type_from_source": "(int, int)"
    },
    {
        "extra_left": [],
        "left": "def insert(self, index, string) -> ",
        "right": ":\n        \"\"\"Insert the given string before the specified index.\n\n        This method has the same effect as ``self[index:index] = string``;\n        it only avoids some condition checks as it rules out the possibility\n        of the key being an slice, or the need to shrink any of the sub-spans.\n\n        If parse is False, don't parse the inserted string.\n        \"\"\"\n        ss, se = self._span\n        lststr = self._lststr\n        lststr0 = lststr[0]\n        if index < 0:\n            index += se - ss\n            if index < 0:\n                index = 0\n        elif index > se - ss:  # Note that it is not >=. Index can be new.\n            index = se - ss\n        index += ss\n        # Update lststr\n        lststr[0] = lststr0[:index] + string + lststr0[index:]\n        string_len = len(string)\n        # Update spans\n        self._insert_update(\n            index=index,\n            length=string_len)\n        # Remember newly added spans by the string.\n        type_to_spans = self._type_to_spans\n        for type_, spans in parse_to_spans(\n            bytearray(string, 'ascii', 'replace')\n        ).items():\n            for s, e in spans:\n                insort(type_to_spans[type_], [index + s, index + e])",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _subspans(self, type_) -> ",
        "right": ":\n        \"\"\"Return all the sub-span including self._span.\"\"\"\n        return self._type_to_spans[type_]",
        "return_type_from_source": "List[List[int]]"
    },
    {
        "extra_left": [],
        "left": "def _shrink_update(self, rmstart, rmstop) -> ",
        "right": ":\n        \"\"\"Update self._type_to_spans according to the removed span.\n\n        Warning: If an operation involves both _shrink_update and\n        _insert_update, you might wanna consider doing the\n        _insert_update before the _shrink_update as this function\n        can cause data loss in self._type_to_spans.\n        \"\"\"\n        # Note: The following algorithm won't work correctly if spans\n        # are not sorted.\n        # Note: No span should be removed from _type_to_spans.\n        for spans in self._type_to_spans.values():\n            i = len(spans) - 1\n            while i >= 0:\n                s, e = span = spans[i]\n                if rmstop <= s:\n                    # rmstart <= rmstop <= s <= e\n                    rmlength = rmstop - rmstart\n                    span[:] = s - rmlength, e - rmlength\n                    i -= 1\n                    continue\n                break\n            else:\n                continue\n            while True:\n                if rmstart <= s:\n                    if rmstop < e:\n                        # rmstart < s <= rmstop < e\n                        span[:] = rmstart, e + rmstart - rmstop\n                        i -= 1\n                        if i < 0:\n                            break\n                        s, e = span = spans[i]\n                        continue\n                    # rmstart <= s <= e < rmstop\n                    spans.pop(i)[:] = -1, -1\n                    i -= 1\n                    if i < 0:\n                        break\n                    s, e = span = spans[i]\n                    continue\n                break\n            while i >= 0:\n                if e <= rmstart:\n                    # s <= e <= rmstart <= rmstop\n                    i -= 1\n                    if i < 0:\n                        break\n                    s, e = span = spans[i]\n                    continue\n                # s <= rmstart <= rmstop <= e\n                span[1] -= rmstop - rmstart\n                i -= 1\n                if i < 0:\n                    break\n                s, e = span = spans[i]\n                continue",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _insert_update(self, index, length) -> ",
        "right": ":\n        \"\"\"Update self._type_to_spans according to the added length.\"\"\"\n        ss, se = self._span\n        for spans in self._type_to_spans.values():\n            for span in spans:\n                if index < span[1] or span[1] == index == se:\n                    span[1] += length\n                    # index is before s, or at s but not on self_span\n                    if index < span[0] or span[0] == index != ss:\n                        span[0] += length",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def nesting_level(self) -> ",
        "right": ":\n        \"\"\"Return the nesting level of self.\n\n        The minimum nesting_level is 0. Being part of any Template or\n        ParserFunction increases the level by one.\n        \"\"\"\n        ss, se = self._span\n        level = 0\n        type_to_spans = self._type_to_spans\n        for type_ in ('Template', 'ParserFunction'):\n            spans = type_to_spans[type_]\n            for s, e in spans[:bisect(spans, [ss + 1])]:\n                if se <= e:\n                    level += 1\n        return level",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def _shadow(self) -> ",
        "right": ":\n        \"\"\"Return a copy of self.string with specific sub-spans replaced.\n\n        Comments blocks are replaced by spaces. Other sub-spans are replaced\n        by underscores.\n\n        The replaced sub-spans are: (\n            'Template', 'WikiLink', 'ParserFunction', 'ExtensionTag',\n            'Comment',\n        )\n\n        This function is called upon extracting tables or extracting the data\n        inside them.\n        \"\"\"\n        ss, se = self._span\n        string = self._lststr[0][ss:se]\n        cached_string, shadow = getattr(\n            self, '_shadow_cache', (None, None))\n        if cached_string == string:\n            return shadow\n        # In the old method the existing spans were used to create the shadow.\n        # But it was slow because there can be thousands of spans and iterating\n        # over them to find the relevant sub-spans could take a significant\n        # amount of time. The new method tries to parse the self.string which\n        # is usually much more faster because there are usually far less\n        # sub-spans for individual objects.\n        shadow = bytearray(string, 'ascii', 'replace')\n        if self._type in SPAN_PARSER_TYPES:\n            head = shadow[:2]\n            tail = shadow[-2:]\n            shadow[:2] = shadow[-2:] = b'__'\n            parse_to_spans(shadow)\n            shadow[:2] = head\n            shadow[-2:] = tail\n        else:\n            parse_to_spans(shadow)\n        self._shadow_cache = string, shadow\n        return shadow",
        "return_type_from_source": "bytearray"
    },
    {
        "extra_left": [],
        "left": "def parameters(self) -> ",
        "right": ":\n        \"\"\"Return a list of parameter objects.\"\"\"\n        _lststr = self._lststr\n        _type_to_spans = self._type_to_spans\n        return [\n            Parameter(_lststr, _type_to_spans, span, 'Parameter')\n            for span in self._subspans('Parameter')]",
        "return_type_from_source": "List['Parameter']"
    },
    {
        "extra_left": [],
        "left": "def parser_functions(self) -> ",
        "right": ":\n        \"\"\"Return a list of parser function objects.\"\"\"\n        _lststr = self._lststr\n        _type_to_spans = self._type_to_spans\n        return [\n            ParserFunction(_lststr, _type_to_spans, span, 'ParserFunction')\n            for span in self._subspans('ParserFunction')]",
        "return_type_from_source": "List['ParserFunction']"
    },
    {
        "extra_left": [],
        "left": "def templates(self) -> ",
        "right": ":\n        \"\"\"Return a list of templates as template objects.\"\"\"\n        _lststr = self._lststr\n        _type_to_spans = self._type_to_spans\n        return [\n            Template(_lststr, _type_to_spans, span, 'Template')\n            for span in self._subspans('Template')]",
        "return_type_from_source": "List['Template']"
    },
    {
        "extra_left": [],
        "left": "def wikilinks(self) -> ",
        "right": ":\n        \"\"\"Return a list of wikilink objects.\"\"\"\n        _lststr = self._lststr\n        _type_to_spans = self._type_to_spans\n        return [\n            WikiLink(_lststr, _type_to_spans, span, 'WikiLink')\n            for span in self._subspans('WikiLink')]",
        "return_type_from_source": "List['WikiLink']"
    },
    {
        "extra_left": [],
        "left": "def comments(self) -> ",
        "right": ":\n        \"\"\"Return a list of comment objects.\"\"\"\n        _lststr = self._lststr\n        _type_to_spans = self._type_to_spans\n        return [\n            Comment(_lststr, _type_to_spans, span, 'Comment')\n            for span in self._subspans('Comment')]",
        "return_type_from_source": "List['Comment']"
    },
    {
        "extra_left": [],
        "left": "def external_links(self) -> ",
        "right": ":\n        \"\"\"Return a list of found external link objects.\n\n        Note:\n            Templates adjacent to external links are considered part of the\n            link. In reality, this depends on the contents of the template:\n\n            >>> WikiText(\n            ...    'http://example.com{{dead link}}'\n            ...).external_links[0].url\n            'http://example.com{{dead link}}'\n\n            >>> WikiText(\n            ...    '[http://example.com{{space template}} text]'\n            ...).external_links[0].url\n            'http://example.com{{space template}}'\n        \"\"\"\n        external_links = []  # type: List['ExternalLink']\n        external_links_append = external_links.append\n        type_to_spans = self._type_to_spans\n        lststr = self._lststr\n        ss, se = self._span\n        spans = type_to_spans.setdefault('ExternalLink', [])\n        if not spans:\n            # All the added spans will be new.\n            spans_append = spans.append\n            for m in EXTERNAL_LINK_FINDITER(self._ext_link_shadow):\n                s, e = m.span()\n                span = [ss + s, ss + e]\n                spans_append(span)\n                external_links_append(\n                    ExternalLink(lststr, type_to_spans, span, 'ExternalLink'))\n            return external_links\n        # There are already some ExternalLink spans. Use the already existing\n        # ones when the detected span is one of those.\n        span_tuple_to_span_get = {(s[0], s[1]): s for s in spans}.get\n        for m in EXTERNAL_LINK_FINDITER(self._ext_link_shadow):\n            s, e = m.span()\n            span = s, e = [s + ss, e + ss]\n            old_span = span_tuple_to_span_get((s, e))\n            if old_span is None:\n                insort(spans, span)\n            else:\n                span = old_span\n            external_links_append(\n                ExternalLink(lststr, type_to_spans, span, 'ExternalLink'))\n        return external_links",
        "return_type_from_source": "List['ExternalLink']"
    },
    {
        "extra_left": [],
        "left": "def sections(self) -> ",
        "right": ":\n        \"\"\"Return a list of section in current wikitext.\n\n        The first section will always be the lead section, even if it is an\n        empty string.\n        \"\"\"\n        sections = []  # type: List['Section']\n        sections_append = sections.append\n        type_to_spans = self._type_to_spans\n        lststr = self._lststr\n        ss, se = _span = self._span\n        type_spans = type_to_spans.setdefault('Section', [])\n        full_match = SECTIONS_FULLMATCH(self._shadow)\n        section_spans = full_match.spans('section')\n        levels = [len(eq) for eq in full_match.captures('equals')]\n        if not type_spans:\n            # All spans are new\n            spans_append = type_spans.append\n            for current_index, (current_level, (s, e)) in enumerate(\n                zip(levels, section_spans), 1\n            ):\n                # Add text of the current_section to any parent section.\n                # Note that section 0 is not a parent for any subsection.\n                for section_index, section_level in enumerate(\n                    levels[current_index:], current_index\n                ):\n                    if current_level and section_level > current_level:\n                        e = section_spans[section_index][1]\n                    else:\n                        break\n                span = [ss + s, ss + e]\n                spans_append(span)\n                sections_append(\n                    Section(lststr, type_to_spans, span, 'Section'))\n            return sections\n        # There are already some spans. Instead of appending new spans\n        # use them when the detected span already exists.\n        span_tuple_to_span = {(s[0], s[1]): s for s in type_spans}.get\n        for current_index, (current_level, (s, e)) in enumerate(\n            zip(levels, section_spans), 1\n        ):\n            # Add text of the current_section to any parent section.\n            # Note that section 0 is not a parent for any subsection.\n            for section_index, section_level in enumerate(\n                levels[current_index:], current_index\n            ):\n                if current_level and section_level > current_level:\n                    e = section_spans[section_index][1]\n                else:\n                    break\n            s, e = ss + s, ss + e\n            old_span = span_tuple_to_span((s, e))\n            if old_span is None:\n                span = [s, e]\n                insort(type_spans, span)\n            else:\n                span = old_span\n            sections_append(Section(lststr, type_to_spans, span, 'Section'))\n        return sections",
        "return_type_from_source": "List['Section']"
    },
    {
        "extra_left": [],
        "left": "def tables(self) -> ",
        "right": ":\n        \"\"\"Return a list of found table objects.\"\"\"\n        tables = []  # type: List['Table']\n        tables_append = tables.append\n        type_to_spans = self._type_to_spans\n        lststr = self._lststr\n        shadow = self._shadow[:]\n        ss, se = self._span\n        spans = type_to_spans.setdefault('Table', [])\n        if not spans:\n            # All the added spans will be new.\n            m = True  # type: Any\n            while m:\n                m = False\n                for m in TABLE_FINDITER(shadow):\n                    ms, me = m.span()\n                    # Ignore leading whitespace using len(m[1]).\n                    span = [ss + ms + len(m[1]), ss + me]\n                    spans.append(span)\n                    tables_append(Table(lststr, type_to_spans, span, 'Table'))\n                    shadow[ms:me] = b'_' * (me - ms)\n            return tables\n        # There are already exists some spans. Try to use the already existing\n        # before appending new spans.\n        span_tuple_to_span_get = {(s[0], s[1]): s for s in spans}.get\n        m = True\n        while m:\n            m = False\n            for m in TABLE_FINDITER(shadow):\n                ms, me = m.span()\n                # Ignore leading whitespace using len(m[1]).\n                s, e = ss + ms + len(m[1]), ss + me\n                old_span = span_tuple_to_span_get((s, e))\n                if old_span is None:\n                    span = [s, e]\n                    insort(spans, span)\n                else:\n                    span = old_span\n                tables_append(Table(lststr, type_to_spans, span, 'Table'))\n                shadow[ms:me] = b'_' * (me - ms)\n        return tables",
        "return_type_from_source": "List['Table']"
    },
    {
        "extra_left": [],
        "left": "def lists(self, pattern = None) -> ",
        "right": ":\n        r\"\"\"Return a list of WikiList objects.\n\n        :param pattern: The starting pattern for list items.\n            Return all types of lists (ol, ul, and dl) if pattern is None.\n            If pattern is not None, it will be passed to the regex engine,\n            remember to escape the `*` character. Examples:\n\n                - `\\#` means top-level ordered lists\n                - `\\#\\*` means unordred lists inside an ordered one\n                - Currently definition lists are not well supported, but you\n                    can use `[:;]` as their pattern.\n\n            Tips and tricks:\n\n                Be careful when using the following patterns as they will\n                probably cause malfunction in the `sublists` method of the\n                resultant List. (However don't worry about them if you are\n                not going to use the `sublists` method.)\n\n                - Use `\\*+` as a pattern and nested unordered lists will be\n                    treated as flat.\n                - Use `\\*\\s*` as pattern to rtstrip `items` of the list.\n\n                Although the pattern parameter is optional, but specifying it\n                can improve the performance.\n        \"\"\"\n        lists = []\n        lists_append = lists.append\n        lststr = self._lststr\n        type_to_spans = self._type_to_spans\n        spans = type_to_spans.setdefault('WikiList', [])\n        span_tuple_to_span_get = {(s[0], s[1]): s for s in spans}.get\n        shadow, ss = self._lists_shadow_ss\n        for pattern in \\\n                (r'\\#', r'\\*', '[:;]') if pattern is None else (pattern,):\n            for m in finditer(\n                LIST_PATTERN_FORMAT.replace(b'{pattern}', pattern.encode()),\n                shadow, MULTILINE\n            ):\n                ms, me = m.span()\n                s, e = ss + ms, ss + me\n                old_span = span_tuple_to_span_get((s, e))\n                if old_span is None:\n                    span = [s, e]\n                    insort(spans, span)\n                else:\n                    span = old_span\n                lists_append(WikiList(\n                    lststr, pattern, m, type_to_spans, span, 'WikiList'))\n        return lists",
        "return_type_from_source": "List['WikiList']"
    },
    {
        "extra_left": [],
        "left": "def tags(self, name=None) -> ",
        "right": ":\n        \"\"\"Return all tags with the given name.\"\"\"\n        lststr = self._lststr\n        type_to_spans = self._type_to_spans\n        if name:\n            if name in _tag_extensions:\n                string = lststr[0]\n                return [\n                    Tag(lststr, type_to_spans, span, 'ExtensionTag')\n                    for span in type_to_spans['ExtensionTag']\n                    if string.startswith('<' + name, span[0])]\n            tags = []  # type: List['Tag']\n        else:\n            # There is no name, add all extension tags. Before using shadow.\n            tags = [\n                Tag(lststr, type_to_spans, span, 'ExtensionTag')\n                for span in type_to_spans['ExtensionTag']]\n        tags_append = tags.append\n        # Get the left-most start tag, match it to right-most end tag\n        # and so on.\n        ss = self._span[0]\n        shadow = self._shadow\n        if name:\n            # There is a name but it is not in TAG_EXTENSIONS.\n            reversed_start_matches = reversed([m for m in regex_compile(\n                START_TAG_PATTERN.replace(\n                    rb'{name}', rb'(?P<name>' + name.encode() + rb')')\n            ).finditer(shadow)])\n            end_search = regex_compile(END_TAG_PATTERN .replace(\n                b'{name}', name.encode())).search\n        else:\n            reversed_start_matches = reversed(\n                [m for m in START_TAG_FINDITER(shadow)])\n        shadow_copy = shadow[:]\n        spans = type_to_spans.setdefault('Tag', [])\n        span_tuple_to_span_get = {(s[0], s[1]): s for s in spans}.get\n        spans_append = spans.append\n        for start_match in reversed_start_matches:\n            if start_match['self_closing']:\n                # Don't look for the end tag\n                s, e = start_match.span()\n                span = [ss + s, ss + e]\n            else:\n                # look for the end-tag\n                if name:\n                    # the end_search is already available\n                    # noinspection PyUnboundLocalVariable\n                    end_match = end_search(shadow_copy, start_match.end())\n                else:\n                    # build end_search according to start tag name\n                    end_match = search(\n                        END_TAG_PATTERN.replace(\n                            b'{name}', start_match['name']),\n                        shadow_copy)\n                if end_match:\n                    s, e = end_match.span()\n                    shadow_copy[s:e] = b'_' * (e - s)\n                    span = [ss + start_match.start(), ss + e]\n                else:\n                    # Assume start-only tag.\n                    s, e = start_match.span()\n                    span = [ss + s, ss + e]\n            old_span = span_tuple_to_span_get((span[0], span[1]))\n            if old_span is None:\n                spans_append(span)\n            else:\n                span = old_span\n            tags_append(Tag(lststr, type_to_spans, span, 'Tag'))\n        return sorted(tags, key=attrgetter('_span'))",
        "return_type_from_source": "List['Tag']"
    },
    {
        "extra_left": [],
        "left": "def ancestors(self, type_ = None) -> ",
        "right": ":\n        \"\"\"Return the ancestors of the current node.\n\n        :param type_: the type of the desired ancestors as a string.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means all the ancestors of any type above.\n        \"\"\"\n        if type_ is None:\n            types = SPAN_PARSER_TYPES\n        else:\n            types = type_,\n        lststr = self._lststr\n        type_to_spans = self._type_to_spans\n        ss, se = self._span\n        ancestors = []\n        ancestors_append = ancestors.append\n        for type_ in types:\n            cls = globals()[type_]\n            spans = type_to_spans[type_]\n            for span in spans[:bisect(spans, [ss])]:\n                if se < span[1]:\n                    ancestors_append(cls(lststr, type_to_spans, span, type_))\n        return sorted(ancestors, key=lambda i: ss - i._span[0])",
        "return_type_from_source": "List['WikiText']"
    },
    {
        "extra_left": [],
        "left": "def parent(self, type_ = None) -> ",
        "right": ":\n        \"\"\"Return the parent node of the current object.\n\n        :param type_: the type of the desired parent object.\n            Currently the following types are supported: {Template,\n            ParserFunction, WikiLink, Comment, Parameter, ExtensionTag}.\n            The default is None and means the first parent, of any type above.\n        :return: parent WikiText object or None if no parent with the desired\n            `type_` is found.\n        \"\"\"\n        ancestors = self.ancestors(type_)\n        if ancestors:\n            return ancestors[0]\n        return None",
        "return_type_from_source": "Optional['WikiText']"
    },
    {
        "extra_left": [],
        "left": "def mode(list_) -> ",
        "right": ":\n    \"\"\"Return the most common item in the list.\n\n    Return the first one if there are more than one most common items.\n\n    Example:\n\n    >>> mode([1,1,2,2,])\n    1\n    >>> mode([1,2,2])\n    2\n    >>> mode([])\n    ...\n    ValueError: max() arg is an empty sequence\n    \"\"\"\n    return max(set(list_), key=list_.count)",
        "return_type_from_source": "T"
    },
    {
        "extra_left": [],
        "left": "def get_arg(name, args) -> ",
        "right": ":\n    \"\"\"Return the first argument in the args that has the given name.\n\n    Return None if no such argument is found.\n\n    As the computation of self.arguments is a little costly, this\n    function was created so that other methods that have already computed\n    the arguments use it instead of calling self.get_arg directly.\n    \"\"\"\n    for arg in args:\n        if arg.name.strip(WS) == name.strip(WS):\n            return arg\n    return None",
        "return_type_from_source": "Optional[Argument]"
    },
    {
        "extra_left": [],
        "left": "def normal_name(\n        self,\n        rm_namespaces=('Template',),\n        capital_links=False,\n        _code = None,\n        *,\n        code: str = None,\n        capitalize=False\n    ) -> ",
        "right": ":\n        \"\"\"Return normal form of self.name.\n\n        - Remove comments.\n        - Remove language code.\n        - Remove namespace (\"template:\" or any of `localized_namespaces`.\n        - Use space instead of underscore.\n        - Remove consecutive spaces.\n        - Use uppercase for the first letter if `capitalize`.\n        - Remove #anchor.\n\n        :param rm_namespaces: is used to provide additional localized\n            namespaces for the template namespace. They will be removed from\n            the result. Default is ('Template',).\n        :param capitalize: If True, convert the first letter of the\n            template's name to a capital letter. See\n            [[mw:Manual:$wgCapitalLinks]] for more info.\n        :param code: is the language code.\n        :param capital_links: deprecated.\n        :param _code: deprecated.\n\n        Example:\n            >>> Template(\n            ...     '{{ eN : tEmPlAtE : <!-- c - -> t_1 # b | a }}'\n            ... ).normal_name(code='en')\n            'T 1'\n        \"\"\"\n        if capital_links:\n            warn('`capital_links` argument is deprecated,'\n                 ' use `capitalize` instead', DeprecationWarning)\n            capitalize = capital_links\n        if _code:\n            warn('`positional_code` argument is deprecated,'\n                 ' use `code` instead', DeprecationWarning)\n            code = _code\n        # Remove comments\n        name = COMMENT_SUB('', self.name).strip(WS)\n        # Remove code\n        if code:\n            head, sep, tail = name.partition(':')\n            if not head and sep:\n                name = tail.strip(' ')\n                head, sep, tail = name.partition(':')\n            if code.lower() == head.strip(' ').lower():\n                name = tail.strip(' ')\n        # Remove namespace\n        head, sep, tail = name.partition(':')\n        if not head and sep:\n            name = tail.strip(' ')\n            head, sep, tail = name.partition(':')\n        if head:\n            ns = head.strip(' ').lower()\n            for namespace in rm_namespaces:\n                if namespace.lower() == ns:\n                    name = tail.strip(' ')\n                    break\n        # Use space instead of underscore\n        name = name.replace('_', ' ')\n        if capitalize:\n            # Use uppercase for the first letter\n            n0 = name[0]\n            if n0.islower():\n                name = n0.upper() + name[1:]\n        # Remove #anchor\n        name, sep, tail = name.partition('#')\n        return ' '.join(name.split())",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def rm_first_of_dup_args(self) -> ",
        "right": ":\n        \"\"\"Eliminate duplicate arguments by removing the first occurrences.\n\n        Remove the first occurrences of duplicate arguments, regardless of\n        their value. Result of the rendered wikitext should remain the same.\n        Warning: Some meaningful data may be removed from wikitext.\n\n        Also see `rm_dup_args_safe` function.\n        \"\"\"\n        names = set()  # type: set\n        for a in reversed(self.arguments):\n            name = a.name.strip(WS)\n            if name in names:\n                del a[:len(a.string)]\n            else:\n                names.add(name)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def rm_dup_args_safe(self, tag = None) -> ",
        "right": ":\n        \"\"\"Remove duplicate arguments in a safe manner.\n\n        Remove the duplicate arguments only in the following situations:\n            1. Both arguments have the same name AND value. (Remove one of\n                them.)\n            2. Arguments have the same name and one of them is empty. (Remove\n                the empty one.)\n\n        Warning: Although this is considered to be safe and no meaningful data\n            is removed from wikitext, but the result of the rendered wikitext\n            may actually change if the second arg is empty and removed but\n            the first had had a value.\n\n        If `tag` is defined, it should be a string that will be appended to\n        the value of the remaining duplicate arguments.\n\n        Also see `rm_first_of_dup_args` function.\n        \"\"\"\n        name_to_lastarg_vals = {} \\\n            # type: Dict[str, Tuple[Argument, List[str]]]\n        # Removing positional args affects their name. By reversing the list\n        # we avoid encountering those kind of args.\n        for arg in reversed(self.arguments):\n            name = arg.name.strip(WS)\n            if arg.positional:\n                # Value of keyword arguments is automatically stripped by MW.\n                val = arg.value\n            else:\n                # But it's not OK to strip whitespace in positional arguments.\n                val = arg.value.strip(WS)\n            if name in name_to_lastarg_vals:\n                # This is a duplicate argument.\n                if not val:\n                    # This duplicate argument is empty. It's safe to remove it.\n                    del arg[0:len(arg.string)]\n                else:\n                    # Try to remove any of the detected duplicates of this\n                    # that are empty or their value equals to this one.\n                    lastarg, dup_vals = name_to_lastarg_vals[name]\n                    if val in dup_vals:\n                        del arg[0:len(arg.string)]\n                    elif '' in dup_vals:\n                        # This happens only if the last occurrence of name has\n                        # been an empty string; other empty values will\n                        # be removed as they are seen.\n                        # In other words index of the empty argument in\n                        # dup_vals is always 0.\n                        del lastarg[0:len(lastarg.string)]\n                        dup_vals.pop(0)\n                    else:\n                        # It was not possible to remove any of the duplicates.\n                        dup_vals.append(val)\n                        if tag:\n                            arg.value += tag\n            else:\n                name_to_lastarg_vals[name] = (arg, [val])",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def set_arg(\n        self, name,\n        value,\n        positional = None,\n        before = None,\n        after = None,\n        preserve_spacing = True\n    ) -> ",
        "right": ":\n        \"\"\"Set the value for `name` argument. Add it if it doesn't exist.\n\n        - Use `positional`, `before` and `after` keyword arguments only when\n          adding a new argument.\n        - If `before` is given, ignore `after`.\n        - If neither `before` nor `after` are given and it's needed to add a\n          new argument, then append the new argument to the end.\n        - If `positional` is True, try to add the given value as a positional\n          argument. Ignore `preserve_spacing` if positional is True.\n          If it's None, do what seems more appropriate.\n        \"\"\"\n        args = list(reversed(self.arguments))\n        arg = get_arg(name, args)\n        # Updating an existing argument.\n        if arg:\n            if positional:\n                arg.positional = positional\n            if preserve_spacing:\n                val = arg.value\n                arg.value = val.replace(val.strip(WS), value)\n            else:\n                arg.value = value\n            return\n        # Adding a new argument\n        if not name and positional is None:\n            positional = True\n        # Calculate the whitespace needed before arg-name and after arg-value.\n        if not positional and preserve_spacing and args:\n            before_names = []\n            name_lengths = []\n            before_values = []\n            after_values = []\n            for arg in args:\n                aname = arg.name\n                name_len = len(aname)\n                name_lengths.append(name_len)\n                before_names.append(STARTING_WS_MATCH(aname)[0])\n                arg_value = arg.value\n                before_values.append(STARTING_WS_MATCH(arg_value)[0])\n                after_values.append(ENDING_WS_MATCH(arg_value)[0])\n            pre_name_ws_mode = mode(before_names)\n            name_length_mode = mode(name_lengths)\n            post_value_ws_mode = mode(\n                [SPACE_AFTER_SEARCH(self.string)[0]] + after_values[1:]\n            )\n            pre_value_ws_mode = mode(before_values)\n        else:\n            preserve_spacing = False\n        # Calculate the string that needs to be added to the Template.\n        if positional:\n            # Ignore preserve_spacing for positional args.\n            addstring = '|' + value\n        else:\n            if preserve_spacing:\n                # noinspection PyUnboundLocalVariable\n                addstring = (\n                    '|' + (pre_name_ws_mode + name.strip(WS)).\n                    ljust(name_length_mode) +\n                    '=' + pre_value_ws_mode + value + post_value_ws_mode\n                )\n            else:\n                addstring = '|' + name + '=' + value\n        # Place the addstring in the right position.\n        if before:\n            arg = get_arg(before, args)\n            arg.insert(0, addstring)\n        elif after:\n            arg = get_arg(after, args)\n            arg.insert(len(arg.string), addstring)\n        else:\n            if args and not positional:\n                arg = args[0]\n                arg_string = arg.string\n                if preserve_spacing:\n                    # Insert after the last argument.\n                    # The addstring needs to be recalculated because we don't\n                    # want to change the the whitespace before final braces.\n                    # noinspection PyUnboundLocalVariable\n                    arg[0:len(arg_string)] = (\n                        arg.string.rstrip(WS) + post_value_ws_mode +\n                        addstring.rstrip(WS) + after_values[0]\n                    )\n                else:\n                    arg.insert(len(arg_string), addstring)\n            else:\n                # The template has no arguments or the new arg is\n                # positional AND is to be added at the end of the template.\n                self.insert(-2, addstring)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_arg(self, name) -> ",
        "right": ":\n        \"\"\"Return the last argument with the given name.\n\n        Return None if no argument with that name is found.\n        \"\"\"\n        return get_arg(name, reversed(self.arguments))",
        "return_type_from_source": "Optional[Argument]"
    },
    {
        "extra_left": [],
        "left": "def has_arg(self, name, value = None) -> ",
        "right": ":\n        \"\"\"Return true if the is an arg named `name`.\n\n        Also check equality of values if `value` is provided.\n\n        Note: If you just need to get an argument and you want to LBYL, it's\n            better to get_arg directly and then check if the returned value\n            is None.\n        \"\"\"\n        for arg in reversed(self.arguments):\n            if arg.name.strip(WS) == name.strip(WS):\n                if value:\n                    if arg.positional:\n                        if arg.value == value:\n                            return True\n                        return False\n                    if arg.value.strip(WS) == value.strip(WS):\n                        return True\n                    return False\n                return True\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def del_arg(self, name) -> ",
        "right": ":\n        \"\"\"Delete all arguments with the given then.\"\"\"\n        for arg in reversed(self.arguments):\n            if arg.name.strip(WS) == name.strip(WS):\n                del arg[:]",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def parse_geo_tiff_keys_from_vlrs(vlr_list) -> ",
        "right": ":\n    \"\"\" Gets the 3 GeoTiff vlrs from the vlr_list and parse them into\n    a nicer structure\n\n    Parameters\n    ----------\n    vlr_list: pylas.vrls.vlrslist.VLRList list of vlrs from a las file\n\n    Raises\n    ------\n        IndexError if any of the needed GeoTiffVLR is not found in the list\n\n    Returns\n    -------\n    List of GeoTiff keys parsed from the VLRs\n\n    \"\"\"\n    geo_key_dir = vlr_list.get_by_id(\n        GeoKeyDirectoryVlr.official_user_id(), GeoKeyDirectoryVlr.official_record_ids()\n    )[0]\n    geo_doubles = vlr_list.get_by_id(\n        GeoDoubleParamsVlr.official_user_id(), GeoDoubleParamsVlr.official_record_ids()\n    )[0]\n    geo_ascii = vlr_list.get_by_id(\n        GeoAsciiParamsVlr.official_user_id(), GeoAsciiParamsVlr.official_record_ids()\n    )[0]\n    return parse_geo_tiff(geo_key_dir, geo_doubles, geo_ascii)",
        "return_type_from_source": "List[GeoTiffKey]"
    },
    {
        "extra_left": [],
        "left": "def parse_geo_tiff(\n    key_dir_vlr,\n    double_vlr,\n    ascii_vlr,\n) -> ",
        "right": ":\n    \"\"\" Parses the GeoTiff VLRs information into nicer structs\n    \"\"\"\n    geotiff_keys = []\n\n    for k in key_dir_vlr.geo_keys:\n        if k.tiff_tag_location == 0:\n            value = k.value_offset\n        elif k.tiff_tag_location == 34736:\n            value = double_vlr.doubles[k.value_offset]\n        elif k.tiff_tag_location == 34737:\n            try:\n                value = ascii_vlr.strings[k.value_offset][k.count :]\n            except IndexError:\n                # Maybe I'm just misunderstanding the specification :thinking:\n                value = ascii_vlr.strings[0][k.value_offset : k.value_offset + k.count]\n        else:\n            logger.warning(\n                \"GeoTiffKey with unknown tiff tag location ({})\".format(\n                    k.tiff_tag_location\n                )\n            )\n            continue\n\n        geotiff_keys.append(GeoTiffKey(k.id, value))\n    return geotiff_keys",
        "return_type_from_source": "List[GeoTiffKey]"
    },
    {
        "extra_left": [],
        "left": "def on_epoch_end(self) -> ",
        "right": ":\n        'Updates indexes after each epoch for shuffling'\n        self.indexes = np.arange(self.nrows)\n        if self.shuffle:\n            np.random.shuffle(self.indexes)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def textacy_cleaner(text) -> ",
        "right": ":\n    \"\"\"\n    Defines the default function for cleaning text.\n\n    This function operates over a list.\n    \"\"\"\n    return preprocess_text(text,\n                           fix_unicode=True,\n                           lowercase=True,\n                           transliterate=True,\n                           no_urls=True,\n                           no_emails=True,\n                           no_phone_numbers=True,\n                           no_numbers=True,\n                           no_currency_symbols=True,\n                           no_punct=True,\n                           no_contractions=False,\n                           no_accents=True)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def apply_parallel(func,\n                   data,\n                   cpu_cores = None) -> ",
        "right": ":\n    \"\"\"\n    Apply function to list of elements.\n\n    Automatically determines the chunk size.\n    \"\"\"\n    if not cpu_cores:\n        cpu_cores = cpu_count()\n\n    try:\n        chunk_size = ceil(len(data) / cpu_cores)\n        pool = Pool(cpu_cores)\n        transformed_data = pool.map(func, chunked(data, chunk_size), chunksize=1)\n    finally:\n        pool.close()\n        pool.join()\n        return transformed_data",
        "return_type_from_source": "List[Any]"
    },
    {
        "extra_left": [],
        "left": "def process_text(self, text) -> ",
        "right": ":\n        \"\"\"Combine the cleaner and tokenizer.\"\"\"\n        process_text = process_text_constructor(cleaner=self.cleaner,\n                                                tokenizer=self.tokenizer,\n                                                append_indicators=self.append_indicators,\n                                                start_tok=self.start_tok,\n                                                end_tok=self.end_tok)\n        return process_text(text)",
        "return_type_from_source": "List[List[str]]"
    },
    {
        "extra_left": [],
        "left": "def parallel_process_text(self, data) -> ",
        "right": ":\n        \"\"\"Apply cleaner -> tokenizer.\"\"\"\n        process_text = process_text_constructor(cleaner=self.cleaner,\n                                                tokenizer=self.tokenizer,\n                                                append_indicators=self.append_indicators,\n                                                start_tok=self.start_tok,\n                                                end_tok=self.end_tok)\n        n_cores = self.num_cores\n        return flattenlist(apply_parallel(process_text, data, n_cores))",
        "return_type_from_source": "List[List[str]]"
    },
    {
        "extra_left": [],
        "left": "def add_missing_row(\n    df,\n    id_cols,\n    reference_col,\n    complete_index = None,\n    method = None,\n    cols_to_keep = None\n) -> ",
        "right": ":\n    \"\"\"\n    Add missing row to a df base on a reference column\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - `id_cols` (*list of str*): names of the columns used to create each group\n    - `reference_col` (*str*): name of the column used to identify missing rows\n\n    *optional :*\n    - `complete_index` (*list* or *dict*): [A, B, C] a list of values used to add missing rows.\n      It can also be a dict to declare a date range.\n      By default, use all values of reference_col.\n    - `method` (*str*): by default all missing rows are added. The possible values are :\n        - `\"between\"` : add missing rows having their value between min and max values for each group,\n        - `\"between_and_after\"` : add missing rows having their value bigger than min value for each group.\n        - `\"between_and_before\"` : add missing rows having their value smaller than max values for each group.\n    - `cols_to_keep` (*list of str*): name of other columns to keep, linked to the reference_col.\n\n    ---\n\n    ### Example\n\n    **Input**\n\n    YEAR | MONTH | NAME\n    :---:|:---:|:--:\n    2017|1|A\n    2017|2|A\n    2017|3|A\n    2017|1|B\n    2017|3|B\n\n    ```cson\n    add_missing_row:\n      id_cols: ['NAME']\n      reference_col: 'MONTH'\n    ```\n\n    **Output**\n\n    YEAR | MONTH | NAME\n    :---:|:---:|:--:\n    2017|1|A\n    2017|2|A\n    2017|3|A\n    2017|1|B\n    2017|2|B\n    2017|3|B\n\n    \"\"\"\n    if cols_to_keep is None:\n        cols_for_index = [reference_col]\n    else:\n        cols_for_index = [reference_col] + cols_to_keep\n    check_params_columns_duplicate(id_cols + cols_for_index)\n\n    if method == 'between' or method == 'between_and_after':\n        df['start'] = df.groupby(id_cols)[reference_col].transform(min)\n        id_cols += ['start']\n    if method == 'between' or method == 'between_and_before':\n        df['end'] = df.groupby(id_cols)[reference_col].transform(max)\n        id_cols += ['end']\n\n    names = id_cols + cols_for_index\n    new_df = df.set_index(names)\n    index_values = df.groupby(id_cols).sum().index.values\n\n    if complete_index is None:\n        complete_index = df.groupby(cols_for_index).sum().index.values\n    elif isinstance(complete_index, dict):\n        if complete_index['type'] == 'date':\n            freq = complete_index['freq']\n            date_format = complete_index['format']\n            start = complete_index['start']\n            end = complete_index['end']\n            if isinstance(freq, dict):\n                freq = pd.DateOffset(**{k: int(v) for k, v in freq.items()})\n            complete_index = pd.date_range(start=start, end=end, freq=freq)\n            complete_index = complete_index.strftime(date_format)\n        else:\n            raise ParamsValueError(f'Unknown complete index type: '\n                                   f'{complete_index[\"type\"]}')\n\n    if not isinstance(index_values[0], tuple):\n        index_values = [(x,) for x in index_values]\n    if not isinstance(complete_index[0], tuple):\n        complete_index = [(x,) for x in complete_index]\n    new_tuples_index = [x + y for x in index_values for y in complete_index]\n\n    new_index = pd.MultiIndex.from_tuples(new_tuples_index, names=names)\n    new_df = new_df.reindex(new_index).reset_index()\n\n    if method == 'between' or method == 'between_and_after':\n        new_df = new_df[new_df[reference_col] >= new_df['start']]\n        del new_df['start']\n    if method == 'between' or method == 'between_and_before':\n        new_df = new_df[new_df[reference_col] <= new_df['end']]\n        del new_df['end']\n\n    return new_df",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def clean_cachedir_old_entries(cachedir, func_name, limit) -> ",
        "right": ":\n    \"\"\"Remove old entries from the cache\"\"\"\n    if limit < 1:\n        raise ValueError(\"'limit' must be greater or equal to 1\")\n\n    cache_entries = get_cachedir_entries(cachedir, func_name)\n    cache_entries = sorted(cache_entries, key=lambda e: e.last_access, reverse=True)\n    cache_entries_to_remove = cache_entries[limit:]\n    for entry in cache_entries_to_remove:\n        shutil.rmtree(entry.path, ignore_errors=True)\n\n    return len(cache_entries_to_remove)",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def parse_date(datestr, date_fmt) -> ",
        "right": ":\n    \"\"\"parse `datestr` and return corresponding date object.\n\n    `datestr` should be a string matching `date_fmt` and parseable by `strptime`\n    but some offset can also be added using `(datestr) + OFFSET` or `(datestr) -\n    OFFSET` syntax. When using this syntax, `OFFSET` should be understable by\n    `pandas.Timedelta` (cf.\n    http://pandas.pydata.org/pandas-docs/stable/timedeltas.html) and `w`, `week`\n    `month` and `year` offset keywords are also accepted. `datestr` MUST be wrapped\n    with parenthesis.\n\n    Additionally, the following symbolic names are supported: `TODAY`,\n    `YESTERDAY`, `TOMORROW`.\n\n    Example usage:\n\n    >>> parse_date('2018-01-01', '%Y-%m-%d') datetime.date(2018, 1, 1)\n    parse_date('(2018-01-01) + 1day', '%Y-%m-%d') datetime.date(2018, 1, 2)\n    parse_date('(2018-01-01) + 2weeks', '%Y-%m-%d') datetime.date(2018, 1, 15)\n\n    Parameters: `datestr`: the date to parse, formatted as `date_fmt`\n        `date_fmt`: expected date format\n\n    Returns: The `date` object. If date could not be parsed, a ValueError will\n        be raised.\n    \"\"\"\n    rgx = re.compile(r'\\((?P<date>.*)\\)(\\s*(?P<sign>[+-])(?P<offset>.*))?$')\n    datestr = datestr.strip()\n    match = rgx.match(datestr)\n    # if regexp doesn't match, date must match the expected format\n    if match is None:\n        return _norm_date(datestr, date_fmt)\n    datestr = match.group('date').strip()\n    dateobj = _norm_date(datestr, date_fmt)\n    offset = match.group('offset')\n    if offset:\n        return add_offset(dateobj, offset, match.group('sign'))\n    return dateobj",
        "return_type_from_source": "date"
    },
    {
        "extra_left": [],
        "left": "def sll(sig, howMany) -> ",
        "right": ":\n    \"Logical shift left\"\n    width = sig._dtype.bit_length()\n    return sig[(width - howMany):]._concat(vec(0, howMany))",
        "return_type_from_source": "RtlSignalBase"
    },
    {
        "extra_left": [],
        "left": "def isPow2(num) -> ",
        "right": ":\n    \"\"\"\n    Check if number or constant is power of two\n    \"\"\"\n    if not isinstance(num, int):\n        num = int(num)\n    return num != 0 and ((num & (num - 1)) == 0)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def name_for_process_and_mark_outputs(statements)\\ -> ",
        "right": ":\n    \"\"\"\n    Resolve name for process and mark outputs of statemens as not hidden\n    \"\"\"\n    out_names = []\n    for stm in statements:\n        for sig in stm._outputs:\n            if not sig.hasGenericName:\n                out_names.append(sig.name)\n\n    if out_names:\n        return min(out_names)\n    else:\n        return \"\"",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _discover_sensitivity_seq(self,\n                                  signals,\n                                  seen, ctx)\\ -> ",
        "right": ":\n        \"\"\"\n        Discover sensitivity for list of signals\n\n        \"\"\"\n        casualSensitivity = set()\n        for s in signals:\n            s._walk_sensitivity(casualSensitivity, seen, ctx)\n            if ctx.contains_ev_dependency:\n                break\n\n        # if event dependent sensitivity found do not add other sensitivity\n        if not ctx.contains_ev_dependency:\n            ctx.extend(casualSensitivity)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _on_reduce(self, self_reduced, io_changed,\n                   result_statements) -> ",
        "right": ":\n        \"\"\"\n        Update signal IO after reuce atempt\n\n        :param self_reduced: if True this object was reduced\n        :param io_changed: if True IO of this object may changed\n            and has to be updated\n        :param result_statements: list of statements which are result\n            of reduce operation on this statement\n        \"\"\"\n\n        parentStm = self.parentStm\n        if self_reduced:\n            was_top = parentStm is None\n            # update signal drivers/endpoints\n            if was_top:\n                # disconnect self from signals\n                ctx = self._get_rtl_context()\n                ctx.statements.remove(self)\n                ctx.statements.update(result_statements)\n\n                for i in self._inputs:\n                    i.endpoints.discard(self)\n                for o in self._outputs:\n                    o.drivers.remove(self)\n\n            for stm in result_statements:\n                stm.parentStm = parentStm\n                if parentStm is None:\n                    # conect signals to child statements\n                    for inp in stm._inputs:\n                        inp.endpoints.append(stm)\n                    for outp in stm._outputs:\n                        outp.drivers.append(stm)\n        else:\n            # parent has to update it's inputs/outputs\n            if io_changed:\n                self._inputs = UniqList()\n                self._outputs = UniqList()\n                self._collect_io()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def HWProcess(cls, proc, ctx) -> ",
        "right": ":\n        \"\"\"\n        Gues resource usage by HWProcess\n        \"\"\"\n        seen = ctx.seen\n        for stm in proc.statements:\n            encl = stm._enclosed_for\n            full_ev_dep = stm._is_completly_event_dependent\n            now_ev_dep = stm._now_is_event_dependent\n            ev_dep = full_ev_dep or now_ev_dep\n\n            out_mux_dim = count_mux_inputs_for_outputs(stm)\n            for o in stm._outputs:\n                if o in seen:\n                    continue\n\n                i = out_mux_dim[o]\n                if isinstance(o._dtype, HArray):\n                    assert i == 1, (o, i, \" only one ram port per HWProcess\")\n                    for a in walk_assignments(stm, o):\n                        assert len(a.indexes) == 1, \"one address per RAM port\"\n                        addr = a.indexes[0]\n                    ctx.registerRAM_write_port(o, addr, ev_dep)\n                elif ev_dep:\n                    ctx.registerFF(o)\n                    if i > 1:\n                        ctx.registerMUX(stm, o, i)\n                elif o not in encl:\n                    ctx.registerLatch(o)\n                    if i > 1:\n                        ctx.registerMUX(stm, o, i)\n                elif i > 1:\n                    ctx.registerMUX(stm, o, i)\n                else:\n                    # just a connection\n                    continue\n\n            if isinstance(stm, SwitchContainer):\n                caseEqs = set([stm.switchOn._eq(c[0]) for c in stm.cases])\n                inputs = chain(\n                    [sig for sig in stm._inputs if sig not in caseEqs], [stm.switchOn])\n            else:\n                inputs = stm._inputs\n\n            for i in inputs:\n                # discover only internal signals in this statements for\n                # operators\n                if not i.hidden or i in seen:\n                    continue\n\n                cls.HWProcess_operators(i, ctx, ev_dep)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _loadFromArray(self, dtype, bitAddr) -> ",
        "right": ":\n        \"\"\"\n        Parse HArray type to this transaction template instance\n\n        :return: address of it's end\n        \"\"\"\n        self.itemCnt = evalParam(dtype.size).val\n        self.children = TransTmpl(\n            dtype.elmType, 0, parent=self, origin=self.origin)\n        return bitAddr + self.itemCnt * self.children.bitAddrEnd",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def _loadFromHType(self, dtype, bitAddr) -> ",
        "right": ":\n        \"\"\"\n        Parse any HDL type to this transaction template instance\n        \"\"\"\n        self.bitAddr = bitAddr\n        childrenAreChoice = False\n        if isinstance(dtype, Bits):\n            ld = self._loadFromBits\n        elif isinstance(dtype, HStruct):\n            ld = self._loadFromHStruct\n        elif isinstance(dtype, HArray):\n            ld = self._loadFromArray\n        elif isinstance(dtype, HStream):\n            ld = self._loadFromHStream\n        elif isinstance(dtype, HUnion):\n            ld = self._loadFromUnion\n            childrenAreChoice = True\n        else:\n            raise TypeError(\"expected instance of HdlType\", dtype)\n\n        self.bitAddrEnd = ld(dtype, bitAddr)\n        self.childrenAreChoice = childrenAreChoice",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def getItemWidth(self) -> ",
        "right": ":\n        \"\"\"\n        Only for transactions derived from HArray\n\n        :return: width of item in original array\n        \"\"\"\n        if not isinstance(self.dtype, HArray):\n            raise TypeError()\n        return (self.bitAddrEnd - self.bitAddr) // self.itemCnt",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def _merge_with_other_stm(self, other) -> ",
        "right": ":\n        \"\"\"\n        Merge other statement to this statement\n        \"\"\"\n        merge = self._merge_statement_lists\n        newCases = []\n        for (c, caseA), (_, caseB) in zip(self.cases, other.cases):\n            newCases.append((c, merge(caseA, caseB)))\n\n        self.cases = newCases\n\n        if self.default is not None:\n            self.default = merge(self.default, other.default)\n\n        self._on_merge(other)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _registerParameter(self, pName, parameter) -> ",
        "right": ":\n        \"\"\"\n        Register Param object on interface level object\n        \"\"\"\n        nameAvailabilityCheck(self, pName, parameter)\n        # resolve name in this scope\n        try:\n            hasName = parameter._name is not None\n        except AttributeError:\n            hasName = False\n        if not hasName:\n            parameter._name = pName\n        # add name in this scope\n        parameter._registerScope(pName, self)\n\n        if parameter.hasGenericName:\n            parameter.name = pName\n\n        if parameter._parent is None:\n            parameter._parent = self\n\n        self._params.append(parameter)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _updateParamsFrom(self, otherObj, updater, exclude, prefix) -> ",
        "right": ":\n        \"\"\"\n        Update all parameters which are defined on self from otherObj\n\n        :param otherObj: other object which Param instances should be updated\n        :param updater: updater function(self, myParameter, onOtherParameterName, otherParameter)\n        :param exclude: iterable of parameter on otherObj object which should be excluded\n        :param prefix: prefix which should be added to name of paramters of this object before matching\n            parameter name on parent\n        \"\"\"\n        excluded = set()\n        if exclude is not None:\n            exclude = set(exclude)\n\n        for myP in self._params:\n            pPName = prefix + myP._scopes[self][1]\n            try:\n                otherP = getattr(otherObj, pPName)\n                if not isinstance(otherP, Param):\n                    continue\n            except AttributeError:\n                continue\n\n            if exclude and otherP in exclude:\n                excluded.add(otherP)\n                continue\n            updater(self, myP, otherP)\n        \n        if exclude is not None:\n            # assert that what should be excluded really exists\n            assert excluded == exclude",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def isEvDependentOn(sig, process) -> ",
        "right": ":\n    \"\"\"\n    Check if hdl process has event depenency on signal\n    \"\"\"\n    if sig is None:\n        return False\n\n    return process in sig.simFallingSensProcs\\\n        or process in sig.simRisingSensProcs",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _add_process(self, proc, priority) -> ",
        "right": ":\n        \"\"\"\n        Schedule process on actual time with specified priority\n        \"\"\"\n        self._events.push(self.now, priority, proc)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _addHdlProcToRun(self, trigger, proc) -> ",
        "right": ":\n        \"\"\"\n        Add hdl process to execution queue\n\n        :param trigger: instance of SimSignal\n        :param proc: python generator function representing HDL process\n        \"\"\"\n        # first process in time has to plan executing of apply values on the\n        # end of this time\n        if not self._applyValPlaned:\n            # (apply on end of this time to minimalize process reevaluation)\n            self._scheduleApplyValues()\n\n        if isEvDependentOn(trigger, proc):\n            if self.now == 0:\n                return  # pass event dependent on startup\n            self._seqProcsToRun.append(proc)\n        else:\n            self._combProcsToRun.append(proc)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _scheduleCombUpdateDoneEv(self) -> ",
        "right": ":\n        \"\"\"\n        Schedule combUpdateDoneEv event to let agents know that current\n        delta step is ending and values from combinational logic are stable\n        \"\"\"\n        assert not self._combUpdateDonePlaned, self.now\n        cud = Event(self)\n        cud.process_to_wake.append(self.__deleteCombUpdateDoneEv())\n        self._add_process(cud, PRIORITY_AGENTS_UPDATE_DONE)\n        self._combUpdateDonePlaned = True\n        self.combUpdateDoneEv = cud\n        return cud",
        "return_type_from_source": "Event"
    },
    {
        "extra_left": [],
        "left": "def _scheduleApplyValues(self) -> ",
        "right": ":\n        \"\"\"\n        Apply stashed values to signals\n        \"\"\"\n        assert not self._applyValPlaned, self.now\n        self._add_process(self._applyValues(), PRIORITY_APPLY_COMB)\n        self._applyValPlaned = True\n\n        if self._runSeqProcessesPlaned:\n            # if runSeqProcesses is already scheduled\n            return\n\n        assert not self._seqProcsToRun and not self._runSeqProcessesPlaned, self.now\n        self._add_process(self._runSeqProcesses(), PRIORITY_APPLY_SEQ)\n        self._runSeqProcessesPlaned = True",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _runCombProcesses(self) -> ",
        "right": ":\n        \"\"\"\n        Delta step for combinational processes\n        \"\"\"\n        for proc in self._combProcsToRun:\n            cont = self._outputContainers[proc]\n            proc(self, cont)\n            for sigName, sig in cont._all_signals:\n                newVal = getattr(cont, sigName)\n                if newVal is not None:\n                    res = self._conflictResolveStrategy(newVal)\n                    # prepare update\n                    updater, isEvDependent = res\n                    self._valuesToApply.append(\n                        (sig, updater, isEvDependent, proc))\n                    setattr(cont, sigName, None)\n                    # else value is latched\n\n        self._combProcsToRun = UniqList()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def read(self, sig) -> ",
        "right": ":\n        \"\"\"\n        Read value from signal or interface\n        \"\"\"\n        try:\n            v = sig._val\n        except AttributeError:\n            v = sig._sigInside._val\n\n        return v.clone()",
        "return_type_from_source": "Value"
    },
    {
        "extra_left": [],
        "left": "def write(self, val, sig) -> ",
        "right": ":\n        \"\"\"\n        Write value to signal or interface.\n        \"\"\"\n        # get target RtlSignal\n        try:\n            simSensProcs = sig.simSensProcs\n        except AttributeError:\n            sig = sig._sigInside\n            simSensProcs = sig.simSensProcs\n\n        # type cast of input value\n        t = sig._dtype\n\n        if isinstance(val, Value):\n            v = val.clone()\n            v = v._auto_cast(t)\n        else:\n            v = t.fromPy(val)\n\n        # can not update value in signal directly due singnal proxies\n        sig.simUpdateVal(self, lambda curentV: (\n            valueHasChanged(curentV, v), v))\n\n        if not self._applyValPlaned:\n            if not (simSensProcs or\n                    sig.simRisingSensProcs or\n                    sig.simFallingSensProcs):\n                # signal value was changed but there are no sensitive processes\n                # to it because of this _applyValues is never planed\n                # and should be\n                self._scheduleApplyValues()\n            elif (sig._writeCallbacks or\n                  sig._writeCallbacksToEn):\n                # signal write did not caused any change on any other signal\n                # but there are still simulation agets waiting on\n                # updateComplete event\n                self._scheduleApplyValues()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def add_process(self, proc) -> ",
        "right": ":\n        \"\"\"\n        Add process to events with default priority on current time\n        \"\"\"\n        self._events.push(self.now, PRIORITY_NORMAL, proc)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _mkOp(fn):\n    \"\"\"\n    Function to create variadic operator function\n\n    :param fn: function to perform binary operation\n    \"\"\"\n    def op(*operands, key=None) -> ",
        "right": ":\n        \"\"\"\n        :param operands: variadic parameter of input uperands\n        :param key: optional function applied on every operand\n            before processing\n        \"\"\"\n        assert operands, operands\n        top = None\n        if key is not None:\n            operands = map(key, operands)\n\n        for s in operands:\n            if top is None:\n                top = s\n            else:\n                top = fn(top, s)\n        return top\n\n    return op",
        "return_type_from_source": "RtlSignalBase"
    },
    {
        "extra_left": [],
        "left": "def load_config_from_cli(config, argv) -> ",
        "right": ":\n    \"\"\"Loads config, checking CLI arguments for a config file\"\"\"\n\n    # Monkey patch Django's command parser\n    from django.core.management.base import BaseCommand\n    original_parser = BaseCommand.create_parser\n\n    def patched_parser(self, prog_name, subcommand):\n        parser = original_parser(self, prog_name, subcommand)\n        argparser_add_argument(parser, config)\n        return parser\n\n    BaseCommand.create_parser = patched_parser\n\n    try:\n        parser = argparse.ArgumentParser(add_help=False)\n        argparser_add_argument(parser, config)\n\n        config_arg, default_args = parser.parse_known_args(argv)\n        config.load(config_arg.config)\n        yield default_args\n    finally:\n        # Put that create_parser back where it came from or so help me!\n        BaseCommand.create_parser = original_parser",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def parse_file(self, file_path, currency) -> ",
        "right": ":\n        \"\"\" Load and parse a .csv file \"\"\"\n        # load file\n                # read csv into memory?\n        contents = self.load_file(file_path)\n        prices = []\n\n        # parse price elements\n        for line in contents:\n            price = self.parse_line(line)\n            assert isinstance(price, PriceModel)\n            price.currency = currency\n            prices.append(price)\n\n        return prices",
        "return_type_from_source": "List[PriceModel]"
    },
    {
        "extra_left": [],
        "left": "def load_file(self, file_path) -> ",
        "right": ":\n        \"\"\" Loads the content of the text file \"\"\"\n        content = []\n        content = read_lines_from_file(file_path)\n        return content",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def parse_line(self, line) -> ",
        "right": ":\n        \"\"\" Parse a CSV line into a price element \"\"\"\n        line = line.rstrip()\n        parts = line.split(',')\n\n        result = PriceModel()\n\n        # symbol\n        result.symbol = self.translate_symbol(parts[0])\n\n        # value\n        result.value = Decimal(parts[1])\n\n        # date\n        date_str = parts[2]\n        date_str = date_str.replace('\"', '')\n        date_parts = date_str.split('/')\n\n        year_str = date_parts[2]\n        month_str = date_parts[1]\n        day_str = date_parts[0]\n\n        logging.debug(f\"parsing {date_parts} into date\")\n        result.datetime = datetime(int(year_str), int(month_str), int(day_str))\n\n        return result",
        "return_type_from_source": "PriceModel"
    },
    {
        "extra_left": [],
        "left": "def translate_symbol(self, in_symbol) -> ",
        "right": ":\n        \"\"\" translate the incoming symbol into locally-used \"\"\"\n        # read all mappings from the db\n        if not self.symbol_maps:\n            self.__load_symbol_maps()\n        # translate the incoming symbol\n        result = self.symbol_maps[in_symbol] if in_symbol in self.symbol_maps else in_symbol\n\n        return result",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_by_id(self, symbol) -> ",
        "right": ":\n        \"\"\" Finds the map by in-symbol \"\"\"\n        return self.query.filter(SymbolMap.in_symbol == symbol).first()",
        "return_type_from_source": "SymbolMap"
    },
    {
        "extra_left": [],
        "left": "def read_lines_from_file(file_path) -> ",
        "right": ":\n    \"\"\" Read text lines from a file \"\"\"\n    # check if the file exists?\n    with open(file_path) as csv_file:\n        content = csv_file.readlines()\n    return content",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def map_entity(self, entity) -> ",
        "right": ":\n        \"\"\" Map the price entity \"\"\"\n        if not entity:\n            return None\n\n        result = PriceModel()\n        result.currency = entity.currency\n\n        # date/time\n        dt_string = entity.date\n        format_string = \"%Y-%m-%d\"\n        if entity.time:\n            dt_string += f\"T{entity.time}\"\n            format_string += \"T%H:%M:%S\"\n        price_datetime = datetime.strptime(dt_string, format_string)\n        result.datum = Datum()\n        result.datum.from_datetime(price_datetime)\n        assert isinstance(result.datum, Datum)\n\n        #result.namespace = entity.namespace\n        #result.symbol = entity.symbol\n        result.symbol = SecuritySymbol(entity.namespace, entity.symbol)\n\n        # Value\n        value = Decimal(entity.value) / Decimal(entity.denom)\n        result.value = Decimal(value)\n\n        return result",
        "return_type_from_source": "PriceModel"
    },
    {
        "extra_left": [],
        "left": "def map_model(self, model) -> ",
        "right": ":\n        \"\"\" Parse into the Price entity, ready for saving \"\"\"\n        # assert isinstance(model, PriceModel)\n        assert isinstance(model.symbol, SecuritySymbol)\n        assert isinstance(model.datum, Datum)\n\n        entity = Price()\n\n        # Format date as ISO string\n        date_iso = f\"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}\"\n        entity.date = date_iso\n\n        entity.time = f\"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}\"\n\n        # Symbol\n        # properly mapped symbols have a namespace, except for the US markets\n        # TODO check this with .csv import\n        if model.symbol.namespace:\n            entity.namespace = model.symbol.namespace.upper()\n        entity.symbol = model.symbol.mnemonic.upper()\n\n        assert isinstance(model.value, Decimal)\n        # Find number of decimal places\n        dec_places = abs(model.value.as_tuple().exponent)\n        entity.denom = 10 ** dec_places\n        # Price value\n        entity.value = int(model.value * entity.denom)\n\n        # Currency\n        entity.currency = model.currency.upper()\n\n        # self.logger.debug(f\"{entity}\")\n        return entity",
        "return_type_from_source": "Price"
    },
    {
        "extra_left": [],
        "left": "def __get_config_template_path(self) -> ",
        "right": ":\n        \"\"\" gets the default config path from resources \"\"\"\n        filename = resource_filename(\n            Requirement.parse(package_name),\n            template_path + config_filename)\n        return filename",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_config_path(self) -> ",
        "right": ":\n        \"\"\"\n        Returns the path where the active config file is expected.\n        This is the user's profile folder.\n        \"\"\"\n        dst_dir = self.__get_user_path()\n        dst = dst_dir + \"/\" + config_filename\n        return dst",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_contents(self) -> ",
        "right": ":\n        \"\"\" Reads the contents of the config file \"\"\"\n        content = None\n        # with open(file_path) as cfg_file:\n        #     contents = cfg_file.read()\n\n        # Dump the current contents into an in-memory file.\n        in_memory = io.StringIO(\"\")\n        self.config.write(in_memory)\n        in_memory.seek(0)\n        content = in_memory.read()\n        #     log(DEBUG, \"config content: %s\", content)\n        in_memory.close()\n        return content",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def parse(self, symbol) -> ",
        "right": ":\n        \"\"\" Splits the symbol into namespace, symbol tuple \"\"\"\n        symbol_parts = symbol.split(\":\")\n        namespace = None\n        mnemonic = symbol\n\n        if len(symbol_parts) > 1:\n            namespace = symbol_parts[0]\n            mnemonic = symbol_parts[1]\n\n        self.namespace = namespace\n        self.mnemonic = mnemonic\n\n        return namespace, mnemonic",
        "return_type_from_source": "(str, str)"
    },
    {
        "extra_left": [],
        "left": "def download_price(self, symbol, currency, agent) -> ",
        "right": ":\n        \"\"\" Download and save price online \"\"\"\n        price = self.__download_price(symbol, currency, agent)\n        self.save()\n        return price",
        "return_type_from_source": "PriceModel"
    },
    {
        "extra_left": [],
        "left": "def get_prices(self, date, currency) -> ",
        "right": ":\n        \"\"\" Fetches all the prices for the given arguments \"\"\"\n        from .repositories import PriceRepository\n\n        session = self.session\n        repo = PriceRepository(session)\n        query = repo.query\n        if date:\n            query = query.filter(dal.Price.date == date)\n        if currency:\n            query = query.filter(dal.Price.currency == currency)\n        # Sort by symbol.\n        query = query.order_by(dal.Price.namespace, dal.Price.symbol)\n        price_entities = query.all()\n\n        mapper = mappers.PriceMapper()\n        result = []\n        for entity in price_entities:\n            model = mapper.map_entity(entity)\n            result.append(model)\n        return result",
        "return_type_from_source": "List[PriceModel]"
    },
    {
        "extra_left": [],
        "left": "def prune_all(self) -> ",
        "right": ":\n        \"\"\"\n        Prune historical prices for all symbols, leaving only the latest.\n        Returns the number of items removed.\n        \"\"\"\n        from .repositories import PriceRepository\n\n        # get all symbols that have prices\n        repo = PriceRepository()\n        items = repo.query.distinct(dal.Price.namespace, dal.Price.symbol).all()\n        # self.logger.debug(items)\n        count = 0\n\n        for item in items:\n            symbol = SecuritySymbol(item.namespace, item.symbol)\n            deleted = self.prune(symbol)\n            if deleted:\n                count += 1\n\n        return count",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def __get_securities(self, currency, agent, symbol,\n                         namespace) -> ",
        "right": ":\n        \"\"\" Fetches the securities that match the given filters \"\"\"\n        repo = self.get_security_repository()\n        query = repo.query\n\n        if currency is not None:\n            query = query.filter(dal.Security.currency == currency)\n\n        if agent is not None:\n            query = query.filter(dal.Security.updater == agent)\n\n        if symbol is not None:\n            query = query.filter(dal.Security.symbol == symbol)\n\n        if namespace is not None:\n            query = query.filter(dal.Security.namespace == namespace)\n\n        # Sorting\n        query = query.order_by(dal.Security.namespace, dal.Security.symbol)\n\n        securities = query.all()\n        return securities",
        "return_type_from_source": "List[dal.Security]"
    },
    {
        "extra_left": [],
        "left": "def archive_info(database, archive_case) -> ",
        "right": ":\n    \"\"\"Get information about a case from archive.\"\"\"\n    data = {\n        'collaborators': archive_case['collaborators'],\n        'synopsis': archive_case.get('synopsis'),\n        'assignees': [],\n        'suspects': [],\n        'causatives': [],\n        'phenotype_terms': [],\n        'phenotype_groups': [],\n    }\n    if archive_case.get('assignee'):\n        archive_user = database.user.find_one({'_id': archive_case['assignee']})\n        data['assignee'].append(archive_user['email'])\n\n    for key in ['suspects', 'causatives']:\n        for variant_id in archive_case.get(key, []):\n            archive_variant = database.variant.find_one({'_id': variant_id})\n            data[key].append({\n                'chromosome': archive_variant['chromosome'],\n                'position': archive_variant['position'],\n                'reference': archive_variant['reference'],\n                'alternative': archive_variant['alternative'],\n                'variant_type': archive_variant['variant_type'],\n            })\n\n    for key in ['phenotype_terms', 'phenotype_groups']:\n        for archive_term in archive_case.get(key, []):\n            data[key].append({\n                'phenotype_id': archive_term['phenotype_id'],\n                'feature': archive_term['feature'],\n            })\n\n    return data",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def get_header_path() -> ",
        "right": ":\n    \"\"\"Return local folder path of header files.\"\"\"\n    import os\n    return os.path.abspath(os.path.dirname(os.path.realpath(__file__))) + '/headers/'",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def run_command(self,\n                    args,\n                    max_num_processes=None,\n                    max_stack_size=None,\n                    max_virtual_memory=None,\n                    as_root=False,\n                    stdin=None,\n                    timeout=None,\n                    check=False,\n                    truncate_stdout=None,\n                    truncate_stderr=None) -> ",
        "right": ":\n        \"\"\"\n        Runs a command inside the sandbox and returns the results.\n\n        :param args: A list of strings that specify which command should\n            be run inside the sandbox.\n\n        :param max_num_processes: The maximum number of processes the\n            command is allowed to spawn.\n\n        :param max_stack_size: The maximum stack size, in bytes, allowed\n            for the command.\n\n        :param max_virtual_memory: The maximum amount of memory, in\n            bytes, allowed for the command.\n\n        :param as_root: Whether to run the command as a root user.\n\n        :param stdin: A file object to be redirected as input to the\n            command's stdin. If this is None, /dev/null is sent to the\n            command's stdin.\n\n        :param timeout: The time limit for the command.\n\n        :param check: Causes CalledProcessError to be raised if the\n            command exits nonzero or times out.\n\n        :param truncate_stdout: When not None, stdout from the command\n            will be truncated after this many bytes.\n\n        :param truncate_stderr: When not None, stderr from the command\n            will be truncated after this many bytes.\n        \"\"\"\n        cmd = ['docker', 'exec', '-i', self.name, 'cmd_runner.py']\n\n        if stdin is None:\n            cmd.append('--stdin_devnull')\n\n        if max_num_processes is not None:\n            cmd += ['--max_num_processes', str(max_num_processes)]\n\n        if max_stack_size is not None:\n            cmd += ['--max_stack_size', str(max_stack_size)]\n\n        if max_virtual_memory is not None:\n            cmd += ['--max_virtual_memory', str(max_virtual_memory)]\n\n        if timeout is not None:\n            cmd += ['--timeout', str(timeout)]\n\n        if truncate_stdout is not None:\n            cmd += ['--truncate_stdout', str(truncate_stdout)]\n\n        if truncate_stderr is not None:\n            cmd += ['--truncate_stderr', str(truncate_stderr)]\n\n        if not as_root:\n            cmd += ['--linux_user_id', str(self._linux_uid)]\n\n        cmd += args\n\n        if self.debug:\n            print('running: {}'.format(cmd), flush=True)\n\n        with tempfile.TemporaryFile() as f:\n            try:\n                subprocess.run(cmd, stdin=stdin, stdout=f, stderr=subprocess.PIPE, check=True)\n                f.seek(0)\n                json_len = int(f.readline().decode().rstrip())\n                results_json = json.loads(f.read(json_len).decode())\n\n                stdout_len = int(f.readline().decode().rstrip())\n                stdout = tempfile.NamedTemporaryFile()\n                stdout.write(f.read(stdout_len))\n                stdout.seek(0)\n\n                stderr_len = int(f.readline().decode().rstrip())\n                stderr = tempfile.NamedTemporaryFile()\n                stderr.write(f.read(stderr_len))\n                stderr.seek(0)\n\n                result = CompletedCommand(return_code=results_json['return_code'],\n                                          timed_out=results_json['timed_out'],\n                                          stdout=stdout,\n                                          stderr=stderr,\n                                          stdout_truncated=results_json['stdout_truncated'],\n                                          stderr_truncated=results_json['stderr_truncated'])\n\n                if (result.return_code != 0 or results_json['timed_out']) and check:\n                    raise subprocess.CalledProcessError(\n                        result.return_code, cmd,\n                        output=result.stdout, stderr=result.stderr)\n\n                return result\n            except subprocess.CalledProcessError as e:\n                f.seek(0)\n                print(f.read())\n                print(e.stderr)\n                raise",
        "return_type_from_source": "'CompletedCommand'"
    },
    {
        "extra_left": [],
        "left": "def add_and_rename_file(self, filename, new_filename) -> ",
        "right": ":\n        \"\"\"\n        Copies the specified file into the working directory of this\n        sandbox and renames it to new_filename.\n        \"\"\"\n        dest = os.path.join(\n            self.name + ':' + SANDBOX_WORKING_DIR_NAME,\n            new_filename)\n        subprocess.check_call(['docker', 'cp', filename, dest])\n        self._chown_files([new_filename])",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_metric(\n            self, name,\n            labels = None) -> ",
        "right": ":\n        \"\"\"Return a metric, optionally configured with labels.\"\"\"\n        metric = self._metrics[name]\n        if labels:\n            return metric.labels(**labels)\n\n        return metric",
        "return_type_from_source": "Metric"
    },
    {
        "extra_left": [],
        "left": "async def _handle_home(self, request) -> ",
        "right": ":\n        \"\"\"Home page request handler.\"\"\"\n        if self.description:\n            title = f'{self.name} - {self.description}'\n        else:\n            title = self.name\n\n        text = dedent(\n            f'''<!DOCTYPE html>\n            <html>\n              <head>\n                <title>{title}</title>\n              </head>\n              <body>\n                <h1>{title}</h1>\n                <p>\n                  Metric are exported at the\n                  <a href=\"/metrics\">/metrics</a> endpoint.\n                </p>\n              </body>\n            </html>\n            ''')\n        return Response(content_type='text/html', text=text)",
        "return_type_from_source": "Response"
    },
    {
        "extra_left": [],
        "left": "async def _handle_metrics(self, request) -> ",
        "right": ":\n        \"\"\"Handler for metrics.\"\"\"\n        if self._update_handler:\n            await self._update_handler(self.registry.get_metrics())\n        response = Response(body=self.registry.generate_metrics())\n        response.content_type = CONTENT_TYPE_LATEST\n        return response",
        "return_type_from_source": "Response"
    },
    {
        "extra_left": [],
        "left": "def vector(members, meta = None) -> ",
        "right": ":\n    \"\"\"Creates a new vector.\"\"\"\n    return Vector(pvector(members), meta=meta)",
        "return_type_from_source": "Vector[T]"
    },
    {
        "extra_left": [],
        "left": "def v(*members: T, meta: Optional[IPersistentMap] = None) -> ",
        "right": ":\n    \"\"\"Creates a new vector from members.\"\"\"\n    return Vector(pvector(members), meta=meta)",
        "return_type_from_source": "Vector[T]"
    },
    {
        "extra_left": [],
        "left": "def bootstrap_repl(which_ns) -> ",
        "right": ":\n    \"\"\"Bootstrap the REPL with a few useful vars and returned the\n    bootstrapped module so it's functions can be used by the REPL\n    command.\"\"\"\n    repl_ns = runtime.Namespace.get_or_create(sym.symbol(\"basilisp.repl\"))\n    ns = runtime.Namespace.get_or_create(sym.symbol(which_ns))\n    repl_module = importlib.import_module(\"basilisp.repl\")\n    ns.add_alias(sym.symbol(\"basilisp.repl\"), repl_ns)\n    ns.refer_all(repl_ns)\n    return repl_module",
        "return_type_from_source": "types.ModuleType"
    },
    {
        "extra_left": [],
        "left": "def multifn(dispatch, default=None) -> ",
        "right": ":\n    \"\"\"Decorator function which can be used to make Python multi functions.\"\"\"\n    name = sym.symbol(dispatch.__qualname__, ns=dispatch.__module__)\n    return MultiFunction(name, dispatch, default)",
        "return_type_from_source": "MultiFunction[T]"
    },
    {
        "extra_left": [],
        "left": "def __add_method(m, key, method) -> ",
        "right": ":\n        \"\"\"Swap the methods atom to include method with key.\"\"\"\n        return m.assoc(key, method)",
        "return_type_from_source": "lmap.Map"
    },
    {
        "extra_left": [],
        "left": "def add_method(self, key, method) -> ",
        "right": ":\n        \"\"\"Add a new method to this function which will respond for\n        key returned from the dispatch function.\"\"\"\n        self._methods.swap(MultiFunction.__add_method, key, method)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_method(self, key) -> ",
        "right": ":\n        \"\"\"Return the method which would handle this dispatch key or\n        None if no method defined for this key and no default.\"\"\"\n        method_cache = self.methods\n        # The 'type: ignore' comment below silences a spurious MyPy error\n        # about having a return statement in a method which does not return.\n        return Maybe(method_cache.entry(key, None)).or_else(\n            lambda: method_cache.entry(self._default, None)  # type: ignore\n        )",
        "return_type_from_source": "Optional[Method]"
    },
    {
        "extra_left": [],
        "left": "def __remove_method(m, key) -> ",
        "right": ":\n        \"\"\"Swap the methods atom to remove method with key.\"\"\"\n        return m.dissoc(key)",
        "return_type_from_source": "lmap.Map"
    },
    {
        "extra_left": [],
        "left": "def remove_method(self, key) -> ",
        "right": ":\n        \"\"\"Remove the method defined for this key and return it.\"\"\"\n        method = self.methods.entry(key, None)\n        if method:\n            self._methods.swap(MultiFunction.__remove_method, key)\n        return method",
        "return_type_from_source": "Optional[Method]"
    },
    {
        "extra_left": [],
        "left": "def _is_macro(v) -> ",
        "right": ":\n    \"\"\"Return True if the Var holds a macro function.\"\"\"\n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.entry(SYM_MACRO_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _with_loc(f):\n    \"\"\"Attach any available location information from the input form to\n    the node environment returned from the parsing function.\"\"\"\n\n    @wraps(f)\n    def _parse_form(ctx, form) -> ",
        "right": ":\n        form_loc = _loc(form)\n        if form_loc is None:\n            return f(ctx, form)\n        else:\n            return f(ctx, form).fix_missing_locations(form_loc)\n\n    return _parse_form",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def _assert_no_recur(node) -> ",
        "right": ":\n    \"\"\"Assert that `recur` forms do not appear in any position of this or\n    child AST nodes.\"\"\"\n    if node.op == NodeOp.RECUR:\n        raise ParserException(\n            \"recur must appear in tail position\", form=node.form, lisp_ast=node\n        )\n    elif node.op in {NodeOp.FN, NodeOp.LOOP}:\n        pass\n    else:\n        node.visit(_assert_no_recur)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _assert_recur_is_tail(node) -> ",
        "right": ":  # pylint: disable=too-many-branches\n    \"\"\"Assert that `recur` forms only appear in the tail position of this\n    or child AST nodes.\n\n    `recur` forms may only appear in `do` nodes (both literal and synthetic\n    `do` nodes) and in either the :then or :else expression of an `if` node.\"\"\"\n    if node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        for child in node.statements:\n            _assert_no_recur(child)\n        _assert_recur_is_tail(node.ret)\n    elif node.op in {NodeOp.FN, NodeOp.FN_METHOD, NodeOp.METHOD}:\n        assert isinstance(node, (Fn, FnMethod, Method))\n        node.visit(_assert_recur_is_tail)\n    elif node.op == NodeOp.IF:\n        assert isinstance(node, If)\n        _assert_no_recur(node.test)\n        _assert_recur_is_tail(node.then)\n        _assert_recur_is_tail(node.else_)\n    elif node.op in {NodeOp.LET, NodeOp.LETFN}:\n        assert isinstance(node, (Let, LetFn))\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n        _assert_recur_is_tail(node.body)\n    elif node.op == NodeOp.LOOP:\n        assert isinstance(node, Loop)\n        for binding in node.bindings:\n            assert binding.init is not None\n            _assert_no_recur(binding.init)\n    elif node.op == NodeOp.RECUR:\n        pass\n    elif node.op == NodeOp.TRY:\n        assert isinstance(node, Try)\n        _assert_recur_is_tail(node.body)\n        for catch in node.catches:\n            _assert_recur_is_tail(catch)\n        if node.finally_:\n            _assert_no_recur(node.finally_)\n    else:\n        node.visit(_assert_no_recur)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def parse_ast(ctx, form) -> ",
        "right": ":\n    \"\"\"Take a Lisp form as an argument and produce a Basilisp syntax\n    tree matching the clojure.tools.analyzer AST spec.\"\"\"\n    return _parse_ast(ctx, form).assoc(top_level=True)",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def warn_on_shadowed_var(self) -> ",
        "right": ":\n        \"\"\"If True, warn when a def'ed Var name is shadowed in an inner scope.\n\n        Implied by warn_on_shadowed_name. The value of warn_on_shadowed_name\n        supersedes the value of this flag.\"\"\"\n        return self.warn_on_shadowed_name or self._opts.entry(\n            WARN_ON_SHADOWED_VAR, False\n        )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def map_lrepr(\n    entries,\n    start,\n    end,\n    meta=None,\n    **kwargs,\n) -> ",
        "right": ":\n    \"\"\"Produce a Lisp representation of an associative collection, bookended\n    with the start and end string supplied. The entries argument must be a\n    callable which will produce tuples of key-value pairs.\n\n    The keyword arguments will be passed along to lrepr for the sequence\n    elements.\"\"\"\n    print_level = kwargs[\"print_level\"]\n    if isinstance(print_level, int) and print_level < 1:\n        return SURPASSED_PRINT_LEVEL\n\n    kwargs = _process_kwargs(**kwargs)\n\n    def entry_reprs():\n        for k, v in entries():\n            yield \"{k} {v}\".format(k=lrepr(k, **kwargs), v=lrepr(v, **kwargs))\n\n    trailer = []\n    print_dup = kwargs[\"print_dup\"]\n    print_length = kwargs[\"print_length\"]\n    if not print_dup and isinstance(print_length, int):\n        items = seq(entry_reprs()).take(print_length + 1).to_list()\n        if len(items) > print_length:\n            items.pop()\n            trailer.append(SURPASSED_PRINT_LENGTH)\n    else:\n        items = list(entry_reprs())\n\n    seq_lrepr = PRINT_SEPARATOR.join(items + trailer)\n\n    print_meta = kwargs[\"print_meta\"]\n    if print_meta and meta:\n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"\n\n    return f\"{start}{seq_lrepr}{end}\"",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def seq_lrepr(\n    iterable, start, end, meta=None, **kwargs\n) -> ",
        "right": ":\n    \"\"\"Produce a Lisp representation of a sequential collection, bookended\n    with the start and end string supplied. The keyword arguments will be\n    passed along to lrepr for the sequence elements.\"\"\"\n    print_level = kwargs[\"print_level\"]\n    if isinstance(print_level, int) and print_level < 1:\n        return SURPASSED_PRINT_LEVEL\n\n    kwargs = _process_kwargs(**kwargs)\n\n    trailer = []\n    print_dup = kwargs[\"print_dup\"]\n    print_length = kwargs[\"print_length\"]\n    if not print_dup and isinstance(print_length, int):\n        items = seq(iterable).take(print_length + 1).to_list()\n        if len(items) > print_length:\n            items.pop()\n            trailer.append(SURPASSED_PRINT_LENGTH)\n    else:\n        items = iterable\n\n    items = list(map(lambda o: lrepr(o, **kwargs), items))\n    seq_lrepr = PRINT_SEPARATOR.join(items + trailer)\n\n    print_meta = kwargs[\"print_meta\"]\n    if print_meta and meta:\n        return f\"^{lrepr(meta, **kwargs)} {start}{seq_lrepr}{end}\"\n\n    return f\"{start}{seq_lrepr}{end}\"",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def lrepr(  # pylint: disable=too-many-arguments\n    o,\n    human_readable = False,\n    print_dup = PRINT_DUP,\n    print_length = PRINT_LENGTH,\n    print_level = PRINT_LEVEL,\n    print_meta = PRINT_META,\n    print_readably = PRINT_READABLY,\n) -> ",
        "right": ":\n    \"\"\"Return a string representation of a Lisp object.\n\n    Permissible keyword arguments are:\n    - human_readable: if logical True, print strings without quotations or\n                      escape sequences (default: false)\n    - print_dup: if logical true, print objects in a way that preserves their\n                 types (default: false)\n    - print_length: the number of items in a collection which will be printed,\n                    or no limit if bound to a logical falsey value (default: 50)\n    - print_level: the depth of the object graph to print, starting with 0, or\n                   no limit if bound to a logical falsey value (default: nil)\n    - print_meta: if logical true, print objects meta in a way that can be\n                  read back by the reader (default: false)\n    - print_readably: if logical false, print strings and characters with\n                      non-alphanumeric characters converted to escape sequences\n                      (default: true)\n\n    Note that this function is not capable of capturing the values bound at\n    runtime to the basilisp.core dynamic variables which correspond to each\n    of the keyword arguments to this function. To use a version of lrepr\n    which does capture those values, call basilisp.lang.runtime.lrepr directly.\"\"\"\n    if isinstance(o, LispObject):\n        return o._lrepr(\n            human_readable=human_readable,\n            print_dup=print_dup,\n            print_length=print_length,\n            print_level=print_level,\n            print_meta=print_meta,\n            print_readably=print_readably,\n        )\n    else:  # pragma: no cover\n        return _lrepr_fallback(\n            o,\n            human_readable=human_readable,\n            print_dup=print_dup,\n            print_length=print_length,\n            print_level=print_level,\n            print_meta=print_meta,\n            print_readably=print_readably,\n        )",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _lrepr_fallback(  # pylint: disable=too-many-arguments\n    o,\n    human_readable = False,\n    print_dup = PRINT_DUP,\n    print_length = PRINT_LENGTH,\n    print_level = PRINT_LEVEL,\n    print_meta = PRINT_META,\n    print_readably = PRINT_READABLY,\n) -> ",
        "right": ":  # pragma: no cover\n    \"\"\"Fallback function for lrepr for subclasses of standard types.\n\n    The singledispatch used for standard lrepr dispatches using an exact\n    type match on the first argument, so we will only hit this function\n    for subclasses of common Python types like strings or lists.\"\"\"\n    kwargs = {\n        \"human_readable\": human_readable,\n        \"print_dup\": print_dup,\n        \"print_length\": print_length,\n        \"print_level\": print_level,\n        \"print_meta\": print_meta,\n        \"print_readably\": print_readably,\n    }\n    if isinstance(o, bool):\n        return _lrepr_bool(o)\n    elif o is None:\n        return _lrepr_nil(o)\n    elif isinstance(o, str):\n        return _lrepr_str(\n            o, human_readable=human_readable, print_readably=print_readably\n        )\n    elif isinstance(o, dict):\n        return _lrepr_py_dict(o, **kwargs)\n    elif isinstance(o, list):\n        return _lrepr_py_list(o, **kwargs)\n    elif isinstance(o, set):\n        return _lrepr_py_set(o, **kwargs)\n    elif isinstance(o, tuple):\n        return _lrepr_py_tuple(o, **kwargs)\n    elif isinstance(o, complex):\n        return _lrepr_complex(o)\n    elif isinstance(o, datetime.datetime):\n        return _lrepr_datetime(o)\n    elif isinstance(o, Decimal):\n        return _lrepr_decimal(o, print_dup=print_dup)\n    elif isinstance(o, Fraction):\n        return _lrepr_fraction(o)\n    elif isinstance(o, Pattern):\n        return _lrepr_pattern(o)\n    elif isinstance(o, uuid.UUID):\n        return _lrepr_uuid(o)\n    else:\n        return repr(o)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def compile_and_exec_form(  # pylint: disable= too-many-arguments\n    form,\n    ctx,\n    module,\n    wrapped_fn_name = _DEFAULT_FN,\n    collect_bytecode = None,\n) -> ",
        "right": ":\n    \"\"\"Compile and execute the given form. This function will be most useful\n    for the REPL and testing purposes. Returns the result of the executed expression.\n\n    Callers may override the wrapped function name, which is used by the\n    REPL to evaluate the result of an expression and print it back out.\"\"\"\n    if form is None:\n        return None\n\n    if not module.__basilisp_bootstrapped__:  # type: ignore\n        _bootstrap_module(ctx.generator_context, ctx.py_ast_optimizer, module)\n\n    final_wrapped_name = genname(wrapped_fn_name)\n\n    lisp_ast = parse_ast(ctx.parser_context, form)\n    py_ast = gen_py_ast(ctx.generator_context, lisp_ast)\n    form_ast = list(\n        map(\n            _statementize,\n            itertools.chain(\n                py_ast.dependencies,\n                [_expressionize(GeneratedPyAST(node=py_ast.node), final_wrapped_name)],\n            ),\n        )\n    )\n\n    ast_module = ast.Module(body=form_ast)\n    ast_module = ctx.py_ast_optimizer.visit(ast_module)\n    ast.fix_missing_locations(ast_module)\n\n    _emit_ast_string(ast_module)\n\n    bytecode = compile(ast_module, ctx.filename, \"exec\")\n    if collect_bytecode:\n        collect_bytecode(bytecode)\n    exec(bytecode, module.__dict__)\n    return getattr(module, final_wrapped_name)()",
        "return_type_from_source": "Any"
    },
    {
        "extra_left": [],
        "left": "def _incremental_compile_module(\n    optimizer,\n    py_ast,\n    mod,\n    source_filename,\n    collect_bytecode = None,\n) -> ",
        "right": ":\n    \"\"\"Incrementally compile a stream of AST nodes in module mod.\n\n    The source_filename will be passed to Python's native compile.\n\n    Incremental compilation is an integral part of generating a Python module\n    during the same process as macro-expansion.\"\"\"\n    module_body = list(\n        map(_statementize, itertools.chain(py_ast.dependencies, [py_ast.node]))\n    )\n\n    module = ast.Module(body=list(module_body))\n    module = optimizer.visit(module)\n    ast.fix_missing_locations(module)\n\n    _emit_ast_string(module)\n\n    bytecode = compile(module, source_filename, \"exec\")\n    if collect_bytecode:\n        collect_bytecode(bytecode)\n    exec(bytecode, mod.__dict__)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def compile_module(\n    forms,\n    ctx,\n    module,\n    collect_bytecode = None,\n) -> ",
        "right": ":\n    \"\"\"Compile an entire Basilisp module into Python bytecode which can be\n    executed as a Python module.\n\n    This function is designed to generate bytecode which can be used for the\n    Basilisp import machinery, to allow callers to import Basilisp modules from\n    Python code.\n    \"\"\"\n    _bootstrap_module(ctx.generator_context, ctx.py_ast_optimizer, module)\n\n    for form in forms:\n        nodes = gen_py_ast(ctx.generator_context, parse_ast(ctx.parser_context, form))\n        _incremental_compile_module(\n            ctx.py_ast_optimizer,\n            nodes,\n            module,\n            source_filename=ctx.filename,\n            collect_bytecode=collect_bytecode,\n        )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def compile_bytecode(\n    code,\n    gctx,\n    optimizer,\n    module,\n) -> ",
        "right": ":\n    \"\"\"Compile cached bytecode into the given module.\n\n    The Basilisp import hook attempts to cache bytecode while compiling Basilisp\n    namespaces. When the cached bytecode is reloaded from disk, it needs to be\n    compiled within a bootstrapped module. This function bootstraps the module\n    and then proceeds to compile a collection of bytecodes into the module.\"\"\"\n    _bootstrap_module(gctx, optimizer, module)\n    for bytecode in code:\n        exec(bytecode, module.__dict__)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def sequence(s) -> ",
        "right": ":\n    \"\"\"Create a Sequence from Iterable s.\"\"\"\n    try:\n        i = iter(s)\n        return _Sequence(i, next(i))\n    except StopIteration:\n        return EMPTY",
        "return_type_from_source": "ISeq[Any]"
    },
    {
        "extra_left": [],
        "left": "def munge(s, allow_builtins = False) -> ",
        "right": ":\n    \"\"\"Replace characters which are not valid in Python symbols\n    with valid replacement strings.\"\"\"\n    new_str = []\n    for c in s:\n        new_str.append(_MUNGE_REPLACEMENTS.get(c, c))\n\n    new_s = \"\".join(new_str)\n\n    if keyword.iskeyword(new_s):\n        return f\"{new_s}_\"\n\n    if not allow_builtins and new_s in builtins.__dict__:\n        return f\"{new_s}_\"\n\n    return new_s",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def fraction(numerator, denominator) -> ",
        "right": ":\n    \"\"\"Create a Fraction from a numerator and denominator.\"\"\"\n    return Fraction(numerator=numerator, denominator=denominator)",
        "return_type_from_source": "Fraction"
    },
    {
        "extra_left": [],
        "left": "def get_handler(level, fmt) -> ",
        "right": ":\n    \"\"\"Get the default logging handler for Basilisp.\"\"\"\n    handler = logging.NullHandler()\n    if os.getenv(\"BASILISP_USE_DEV_LOGGER\") == \"true\":\n        handler = logging.StreamHandler()\n\n    handler.setFormatter(logging.Formatter(fmt))\n    handler.setLevel(level)\n    return handler",
        "return_type_from_source": "logging.Handler"
    },
    {
        "extra_left": [],
        "left": "def _with_loc(f) -> ",
        "right": ":\n    \"\"\"Wrap a reader function in a decorator to supply line and column\n    information along with relevant forms.\"\"\"\n\n    @functools.wraps(f)\n    def with_lineno_and_col(ctx):\n        meta = lmap.map(\n            {READER_LINE_KW: ctx.reader.line, READER_COL_KW: ctx.reader.col}\n        )\n        v = f(ctx)\n        try:\n            return v.with_meta(meta)  # type: ignore\n        except AttributeError:\n            return v\n\n    return cast(W, with_lineno_and_col)",
        "return_type_from_source": "W"
    },
    {
        "extra_left": [],
        "left": "def _read_list(ctx) -> ",
        "right": ":\n    \"\"\"Read a list element from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"(\"\n    return _read_coll(ctx, llist.list, \")\", \"list\")",
        "return_type_from_source": "llist.List"
    },
    {
        "extra_left": [],
        "left": "def _read_vector(ctx) -> ",
        "right": ":\n    \"\"\"Read a vector element from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"[\"\n    return _read_coll(ctx, vector.vector, \"]\", \"vector\")",
        "return_type_from_source": "vector.Vector"
    },
    {
        "extra_left": [],
        "left": "def _read_map(ctx) -> ",
        "right": ":\n    \"\"\"Return a map from the input stream.\"\"\"\n    reader = ctx.reader\n    start = reader.advance()\n    assert start == \"{\"\n    d = {}\n    while True:\n        if reader.peek() == \"}\":\n            reader.next_token()\n            break\n        k = _read_next(ctx)\n        if k is COMMENT:\n            continue\n        while True:\n            if reader.peek() == \"}\":\n                raise SyntaxError(\"Unexpected token '}'; expected map value\")\n            v = _read_next(ctx)\n            if v is COMMENT:\n                continue\n            if k in d:\n                raise SyntaxError(f\"Duplicate key '{k}' in map literal\")\n            break\n        d[k] = v\n\n    return lmap.map(d)",
        "return_type_from_source": "lmap.Map"
    },
    {
        "extra_left": [],
        "left": "def _read_str(ctx, allow_arbitrary_escapes = False) -> ",
        "right": ":\n    \"\"\"Return a string from the input stream.\n\n    If allow_arbitrary_escapes is True, do not throw a SyntaxError if an\n    unknown escape sequence is encountered.\"\"\"\n    s = []\n    reader = ctx.reader\n    while True:\n        token = reader.next_token()\n        if token == \"\":\n            raise SyntaxError(\"Unexpected EOF in string\")\n        if token == \"\\\\\":\n            token = reader.next_token()\n            escape_char = _STR_ESCAPE_CHARS.get(token, None)\n            if escape_char:\n                s.append(escape_char)\n                continue\n            if allow_arbitrary_escapes:\n                s.append(\"\\\\\")\n            else:\n                raise SyntaxError(\"Unknown escape sequence: \\\\{token}\")\n        if token == '\"':\n            reader.next_token()\n            return \"\".join(s)\n        s.append(token)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _read_sym(ctx) -> ",
        "right": ":\n    \"\"\"Return a symbol from the input stream.\n\n    If a symbol appears in a syntax quoted form, the reader will attempt\n    to resolve the symbol using the resolver in the ReaderContext `ctx`.\n    The resolver will look into the current namespace for an alias or\n    namespace matching the symbol's namespace.\"\"\"\n    ns, name = _read_namespaced(ctx, allowed_suffix=\"#\")\n    if not ctx.is_syntax_quoted and name.endswith(\"#\"):\n        raise SyntaxError(\"Gensym may not appear outside syntax quote\")\n    if ns is not None:\n        if any(map(lambda s: len(s) == 0, ns.split(\".\"))):\n            raise SyntaxError(\n                \"All '.' separated segments of a namespace \"\n                \"must contain at least one character.\"\n            )\n    if name.startswith(\".\") and ns is not None:\n        raise SyntaxError(\"Symbols starting with '.' may not have a namespace\")\n    if ns is None:\n        if name == \"nil\":\n            return None\n        elif name == \"true\":\n            return True\n        elif name == \"false\":\n            return False\n    if ctx.is_syntax_quoted and not name.endswith(\"#\"):\n        return ctx.resolve(symbol.symbol(name, ns))\n    return symbol.symbol(name, ns=ns)",
        "return_type_from_source": "MaybeSymbol"
    },
    {
        "extra_left": [],
        "left": "def _read_kw(ctx) -> ",
        "right": ":\n    \"\"\"Return a keyword from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \":\"\n    ns, name = _read_namespaced(ctx)\n    if \".\" in name:\n        raise SyntaxError(\"Found '.' in keyword name\")\n    return keyword.keyword(name, ns=ns)",
        "return_type_from_source": "keyword.Keyword"
    },
    {
        "extra_left": [],
        "left": "def _read_meta(ctx) -> ",
        "right": ":\n    \"\"\"Read metadata and apply that to the next object in the\n    input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"^\"\n    meta = _read_next_consuming_comment(ctx)\n\n    meta_map = None\n    if isinstance(meta, symbol.Symbol):\n        meta_map = lmap.map({keyword.keyword(\"tag\"): meta})\n    elif isinstance(meta, keyword.Keyword):\n        meta_map = lmap.map({meta: True})\n    elif isinstance(meta, lmap.Map):\n        meta_map = meta\n    else:\n        raise SyntaxError(\n            f\"Expected symbol, keyword, or map for metadata, not {type(meta)}\"\n        )\n\n    obj_with_meta = _read_next_consuming_comment(ctx)\n    try:\n        return obj_with_meta.with_meta(meta_map)  # type: ignore\n    except AttributeError:\n        raise SyntaxError(\n            f\"Can not attach metadata to object of type {type(obj_with_meta)}\"\n        )",
        "return_type_from_source": "IMeta"
    },
    {
        "extra_left": [],
        "left": "def _read_function(ctx) -> ",
        "right": ":\n    \"\"\"Read a function reader macro from the input stream.\"\"\"\n    if ctx.is_in_anon_fn:\n        raise SyntaxError(f\"Nested #() definitions not allowed\")\n\n    with ctx.in_anon_fn():\n        form = _read_list(ctx)\n    arg_set = set()\n\n    def arg_suffix(arg_num):\n        if arg_num is None:\n            return \"1\"\n        elif arg_num == \"&\":\n            return \"rest\"\n        else:\n            return arg_num\n\n    def sym_replacement(arg_num):\n        suffix = arg_suffix(arg_num)\n        return symbol.symbol(f\"arg-{suffix}\")\n\n    def identify_and_replace(f):\n        if isinstance(f, symbol.Symbol):\n            if f.ns is None:\n                match = fn_macro_args.match(f.name)\n                if match is not None:\n                    arg_num = match.group(2)\n                    suffix = arg_suffix(arg_num)\n                    arg_set.add(suffix)\n                    return sym_replacement(arg_num)\n        return f\n\n    body = walk.postwalk(identify_and_replace, form) if len(form) > 0 else None\n\n    arg_list = []\n    numbered_args = sorted(map(int, filter(lambda k: k != \"rest\", arg_set)))\n    if len(numbered_args) > 0:\n        max_arg = max(numbered_args)\n        arg_list = [sym_replacement(str(i)) for i in range(1, max_arg + 1)]\n        if \"rest\" in arg_set:\n            arg_list.append(_AMPERSAND)\n            arg_list.append(sym_replacement(\"rest\"))\n\n    return llist.l(_FN, vector.vector(arg_list), body)",
        "return_type_from_source": "llist.List"
    },
    {
        "extra_left": [],
        "left": "def _read_quoted(ctx) -> ",
        "right": ":\n    \"\"\"Read a quoted form from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"'\"\n    next_form = _read_next_consuming_comment(ctx)\n    return llist.l(_QUOTE, next_form)",
        "return_type_from_source": "llist.List"
    },
    {
        "extra_left": [],
        "left": "def _expand_syntax_quote(\n    ctx, form\n) -> ",
        "right": ":\n    \"\"\"Expand syntax quoted forms to handle unquoting and unquote-splicing.\n\n    The unquoted form (unquote x) becomes:\n        (list x)\n\n    The unquote-spliced form (unquote-splicing x) becomes\n        x\n\n    All other forms are recursively processed as by _process_syntax_quoted_form\n    and are returned as:\n        (list form)\"\"\"\n    expanded = []\n\n    for elem in form:\n        if _is_unquote(elem):\n            expanded.append(llist.l(_LIST, elem[1]))\n        elif _is_unquote_splicing(elem):\n            expanded.append(elem[1])\n        else:\n            expanded.append(llist.l(_LIST, _process_syntax_quoted_form(ctx, elem)))\n\n    return expanded",
        "return_type_from_source": "Iterable[LispForm]"
    },
    {
        "extra_left": [],
        "left": "def _process_syntax_quoted_form(ctx, form) -> ",
        "right": ":\n    \"\"\"Post-process syntax quoted forms to generate forms that can be assembled\n    into the correct types at runtime.\n\n    Lists are turned into:\n        (basilisp.core/seq\n         (basilisp.core/concat [& rest]))\n\n    Vectors are turned into:\n        (basilisp.core/apply\n         basilisp.core/vector\n         (basilisp.core/concat [& rest]))\n\n    Sets are turned into:\n        (basilisp.core/apply\n         basilisp.core/hash-set\n         (basilisp.core/concat [& rest]))\n\n    Maps are turned into:\n        (basilisp.core/apply\n         basilisp.core/hash-map\n         (basilisp.core/concat [& rest]))\n\n    The child forms (called rest above) are processed by _expand_syntax_quote.\n\n    All other forms are passed through without modification.\"\"\"\n    lconcat = lambda v: llist.list(v).cons(_CONCAT)\n    if _is_unquote(form):\n        return form[1]  # type: ignore\n    elif _is_unquote_splicing(form):\n        raise SyntaxError(\"Cannot splice outside collection\")\n    elif isinstance(form, llist.List):\n        return llist.l(_SEQ, lconcat(_expand_syntax_quote(ctx, form)))\n    elif isinstance(form, vector.Vector):\n        return llist.l(_APPLY, _VECTOR, lconcat(_expand_syntax_quote(ctx, form)))\n    elif isinstance(form, lset.Set):\n        return llist.l(_APPLY, _HASH_SET, lconcat(_expand_syntax_quote(ctx, form)))\n    elif isinstance(form, lmap.Map):\n        flat_kvs = seq(form.items()).flatten().to_list()\n        return llist.l(_APPLY, _HASH_MAP, lconcat(_expand_syntax_quote(ctx, flat_kvs)))\n    elif isinstance(form, symbol.Symbol):\n        if form.ns is None and form.name.endswith(\"#\"):\n            try:\n                return llist.l(_QUOTE, ctx.gensym_env[form.name])\n            except KeyError:\n                genned = symbol.symbol(langutil.genname(form.name[:-1])).with_meta(\n                    form.meta\n                )\n                ctx.gensym_env[form.name] = genned\n                return llist.l(_QUOTE, genned)\n        return llist.l(_QUOTE, form)\n    else:\n        return form",
        "return_type_from_source": "ReaderForm"
    },
    {
        "extra_left": [],
        "left": "def _read_syntax_quoted(ctx) -> ",
        "right": ":\n    \"\"\"Read a syntax-quote and set the syntax-quoting state in the reader.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"`\"\n\n    with ctx.syntax_quoted():\n        return _process_syntax_quoted_form(ctx, _read_next_consuming_comment(ctx))",
        "return_type_from_source": "ReaderForm"
    },
    {
        "extra_left": [],
        "left": "def _read_unquote(ctx) -> ",
        "right": ":\n    \"\"\"Read an unquoted form and handle any special logic of unquoting.\n\n    Unquoted forms can take two, well... forms:\n\n      `~form` is read as `(unquote form)` and any nested forms are read\n      literally and passed along to the compiler untouched.\n\n      `~@form` is read as `(unquote-splicing form)` which tells the compiler\n      to splice in the contents of a sequential form such as a list or\n      vector into the final compiled form. This helps macro writers create\n      longer forms such as function calls, function bodies, or data structures\n      with the contents of another collection they have.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"~\"\n\n    with ctx.unquoted():\n        next_char = ctx.reader.peek()\n        if next_char == \"@\":\n            ctx.reader.advance()\n            next_form = _read_next_consuming_comment(ctx)\n            return llist.l(_UNQUOTE_SPLICING, next_form)\n        else:\n            next_form = _read_next_consuming_comment(ctx)\n            return llist.l(_UNQUOTE, next_form)",
        "return_type_from_source": "LispForm"
    },
    {
        "extra_left": [],
        "left": "def _read_deref(ctx) -> ",
        "right": ":\n    \"\"\"Read a derefed form from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"@\"\n    next_form = _read_next_consuming_comment(ctx)\n    return llist.l(_DEREF, next_form)",
        "return_type_from_source": "LispForm"
    },
    {
        "extra_left": [],
        "left": "def _read_character(ctx) -> ",
        "right": ":\n    \"\"\"Read a character literal from the input stream.\n\n    Character literals may appear as:\n      - \\\\a \\\\b \\\\c etc will yield 'a', 'b', and 'c' respectively\n\n      - \\\\newline, \\\\space, \\\\tab, \\\\formfeed, \\\\backspace, \\\\return yield\n        the named characters\n\n      - \\\\uXXXX yield the unicode digit corresponding to the code\n        point named by the hex digits XXXX\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"\\\\\"\n\n    s = []\n    reader = ctx.reader\n    token = reader.peek()\n    while True:\n        if token == \"\" or whitespace_chars.match(token):\n            break\n        if not alphanumeric_chars.match(token):\n            break\n        s.append(token)\n        token = reader.next_token()\n\n    char = \"\".join(s)\n    special = _SPECIAL_CHARS.get(char, None)\n    if special is not None:\n        return special\n\n    match = unicode_char.match(char)\n    if match is not None:\n        try:\n            return chr(int(f\"0x{match.group(1)}\", 16))\n        except (ValueError, OverflowError):\n            raise SyntaxError(f\"Unsupported character \\\\u{char}\") from None\n\n    if len(char) > 1:\n        raise SyntaxError(f\"Unsupported character \\\\{char}\")\n\n    return char",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _read_regex(ctx) -> ",
        "right": ":\n    \"\"\"Read a regex reader macro from the input stream.\"\"\"\n    s = _read_str(ctx, allow_arbitrary_escapes=True)\n    try:\n        return langutil.regex_from_str(s)\n    except re.error:\n        raise SyntaxError(f\"Unrecognized regex pattern syntax: {s}\")",
        "return_type_from_source": "Pattern"
    },
    {
        "extra_left": [],
        "left": "def _read_reader_macro(ctx) -> ",
        "right": ":\n    \"\"\"Return a data structure evaluated as a reader\n    macro from the input stream.\"\"\"\n    start = ctx.reader.advance()\n    assert start == \"#\"\n    token = ctx.reader.peek()\n    if token == \"{\":\n        return _read_set(ctx)\n    elif token == \"(\":\n        return _read_function(ctx)\n    elif token == \"'\":\n        ctx.reader.advance()\n        s = _read_sym(ctx)\n        return llist.l(_VAR, s)\n    elif token == '\"':\n        return _read_regex(ctx)\n    elif token == \"_\":\n        ctx.reader.advance()\n        _read_next(ctx)  # Ignore the entire next form\n        return COMMENT\n    elif ns_name_chars.match(token):\n        s = _read_sym(ctx)\n        assert isinstance(s, symbol.Symbol)\n        v = _read_next_consuming_comment(ctx)\n        if s in ctx.data_readers:\n            f = ctx.data_readers[s]\n            return f(v)\n        else:\n            raise SyntaxError(f\"No data reader found for tag #{s}\")\n\n    raise SyntaxError(f\"Unexpected token '{token}' in reader macro\")",
        "return_type_from_source": "LispReaderForm"
    },
    {
        "extra_left": [],
        "left": "def _read_next_consuming_comment(ctx) -> ",
        "right": ":\n    \"\"\"Read the next full form from the input stream, consuming any\n    reader comments completely.\"\"\"\n    while True:\n        v = _read_next(ctx)\n        if v is ctx.eof:\n            return ctx.eof\n        if v is COMMENT or isinstance(v, Comment):\n            continue\n        return v",
        "return_type_from_source": "ReaderForm"
    },
    {
        "extra_left": [],
        "left": "def _read_next(ctx) -> ",
        "right": ":  # noqa: C901\n    \"\"\"Read the next full form from the input stream.\"\"\"\n    reader = ctx.reader\n    token = reader.peek()\n    if token == \"(\":\n        return _read_list(ctx)\n    elif token == \"[\":\n        return _read_vector(ctx)\n    elif token == \"{\":\n        return _read_map(ctx)\n    elif begin_num_chars.match(token):\n        return _read_num(ctx)\n    elif whitespace_chars.match(token):\n        reader.next_token()\n        return _read_next(ctx)\n    elif token == \":\":\n        return _read_kw(ctx)\n    elif token == '\"':\n        return _read_str(ctx)\n    elif token == \"'\":\n        return _read_quoted(ctx)\n    elif token == \"\\\\\":\n        return _read_character(ctx)\n    elif ns_name_chars.match(token):\n        return _read_sym(ctx)\n    elif token == \"#\":\n        return _read_reader_macro(ctx)\n    elif token == \"^\":\n        return _read_meta(ctx)  # type: ignore\n    elif token == \";\":\n        return _read_comment(ctx)\n    elif token == \"`\":\n        return _read_syntax_quoted(ctx)\n    elif token == \"~\":\n        return _read_unquote(ctx)\n    elif token == \"@\":\n        return _read_deref(ctx)\n    elif token == \"\":\n        return ctx.eof\n    else:\n        raise SyntaxError(\"Unexpected token '{token}'\".format(token=token))",
        "return_type_from_source": "LispReaderForm"
    },
    {
        "extra_left": [],
        "left": "def read(\n    stream,\n    resolver = None,\n    data_readers = None,\n    eof = EOF,\n    is_eof_error = False,\n) -> ",
        "right": ":\n    \"\"\"Read the contents of a stream as a Lisp expression.\n\n    Callers may optionally specify a namespace resolver, which will be used\n    to adjudicate the fully-qualified name of symbols appearing inside of\n    a syntax quote.\n\n    Callers may optionally specify a map of custom data readers that will\n    be used to resolve values in reader macros. Data reader tags specified\n    by callers must be namespaced symbols; non-namespaced symbols are\n    reserved by the reader. Data reader functions must be functions taking\n    one argument and returning a value.\n\n    The caller is responsible for closing the input stream.\"\"\"\n    reader = StreamReader(stream)\n    ctx = ReaderContext(reader, resolver=resolver, data_readers=data_readers, eof=eof)\n    while True:\n        expr = _read_next(ctx)\n        if expr is ctx.eof:\n            if is_eof_error:\n                raise EOFError\n            return\n        if expr is COMMENT or isinstance(expr, Comment):\n            continue\n        yield expr",
        "return_type_from_source": "Iterable[ReaderForm]"
    },
    {
        "extra_left": [],
        "left": "def read_str(\n    s,\n    resolver = None,\n    data_readers = None,\n    eof = None,\n    is_eof_error = False,\n) -> ",
        "right": ":\n    \"\"\"Read the contents of a string as a Lisp expression.\n\n    Keyword arguments to this function have the same meanings as those of\n    basilisp.lang.reader.read.\"\"\"\n    with io.StringIO(s) as buf:\n        yield from read(\n            buf,\n            resolver=resolver,\n            data_readers=data_readers,\n            eof=eof,\n            is_eof_error=is_eof_error,\n        )",
        "return_type_from_source": "Iterable[ReaderForm]"
    },
    {
        "extra_left": [],
        "left": "def read_file(\n    filename,\n    resolver = None,\n    data_readers = None,\n    eof = None,\n    is_eof_error = False,\n) -> ",
        "right": ":\n    \"\"\"Read the contents of a file as a Lisp expression.\n\n    Keyword arguments to this function have the same meanings as those of\n    basilisp.lang.reader.read.\"\"\"\n    with open(filename) as f:\n        yield from read(\n            f,\n            resolver=resolver,\n            data_readers=data_readers,\n            eof=eof,\n            is_eof_error=is_eof_error,\n        )",
        "return_type_from_source": "Iterable[ReaderForm]"
    },
    {
        "extra_left": [],
        "left": "def pushback(self) -> ",
        "right": ":\n        \"\"\"Push one character back onto the stream, allowing it to be\n        read again.\"\"\"\n        if abs(self._idx - 1) > self._pushback_depth:\n            raise IndexError(\"Exceeded pushback depth\")\n        self._idx -= 1",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def next_token(self) -> ",
        "right": ":\n        \"\"\"Advance the stream forward by one character and return the\n        next token in the stream.\"\"\"\n        if self._idx < StreamReader.DEFAULT_INDEX:\n            self._idx += 1\n        else:\n            c = self._stream.read(1)\n            self._update_loc(c)\n            self._buffer.append(c)\n        return self.peek()",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _basilisp_bytecode(\n    mtime, source_size, code\n) -> ",
        "right": ":\n    \"\"\"Return the bytes for a Basilisp bytecode cache file.\"\"\"\n    data = bytearray(MAGIC_NUMBER)\n    data.extend(_w_long(mtime))\n    data.extend(_w_long(source_size))\n    data.extend(marshal.dumps(code))  # type: ignore\n    return data",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def _get_basilisp_bytecode(\n    fullname, mtime, source_size, cache_data\n) -> ",
        "right": ":\n    \"\"\"Unmarshal the bytes from a Basilisp bytecode cache file, validating the\n    file header prior to returning. If the file header does not match, throw\n    an exception.\"\"\"\n    exc_details = {\"name\": fullname}\n    magic = cache_data[:4]\n    raw_timestamp = cache_data[4:8]\n    raw_size = cache_data[8:12]\n    if magic != MAGIC_NUMBER:\n        message = (\n            f\"Incorrect magic number ({magic}) in {fullname}; expected {MAGIC_NUMBER}\"\n        )\n        logger.debug(message)\n        raise ImportError(message, **exc_details)  # type: ignore\n    elif len(raw_timestamp) != 4:\n        message = f\"Reached EOF while reading timestamp in {fullname}\"\n        logger.debug(message)\n        raise EOFError(message)\n    elif _r_long(raw_timestamp) != mtime:\n        message = f\"Non-matching timestamp ({_r_long(raw_timestamp)}) in {fullname} bytecode cache; expected {mtime}\"\n        logger.debug(message)\n        raise ImportError(message, **exc_details)  # type: ignore\n    elif len(raw_size) != 4:\n        message = f\"Reached EOF while reading size of source in {fullname}\"\n        logger.debug(message)\n        raise EOFError(message)\n    elif _r_long(raw_size) != source_size:\n        message = f\"Non-matching filesize ({_r_long(raw_size)}) in {fullname} bytecode cache; expected {source_size}\"\n        logger.debug(message)\n        raise ImportError(message, **exc_details)  # type: ignore\n\n    return marshal.loads(cache_data[12:])",
        "return_type_from_source": "List[types.CodeType]"
    },
    {
        "extra_left": [],
        "left": "def _cache_from_source(path) -> ",
        "right": ":\n    \"\"\"Return the path to the cached file for the given path. The original path\n    does not have to exist.\"\"\"\n    cache_path, cache_file = os.path.split(importlib.util.cache_from_source(path))\n    filename, _ = os.path.splitext(cache_file)\n    return os.path.join(cache_path, filename + \".lpyc\")",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def find_spec(\n        self,\n        fullname,\n        path,  # Optional[List[str]] # MyPy complains this is incompatible with supertype\n        target = None,\n    ) -> ",
        "right": ":\n        \"\"\"Find the ModuleSpec for the specified Basilisp module.\n\n        Returns None if the module is not a Basilisp module to allow import processing to continue.\"\"\"\n        package_components = fullname.split(\".\")\n        if path is None:\n            path = sys.path\n            module_name = package_components\n        else:\n            module_name = [package_components[-1]]\n\n        for entry in path:\n            filenames = [\n                f\"{os.path.join(entry, *module_name, '__init__')}.lpy\",\n                f\"{os.path.join(entry, *module_name)}.lpy\",\n            ]\n            for filename in filenames:\n                if os.path.exists(filename):\n                    state = {\n                        \"fullname\": fullname,\n                        \"filename\": filename,\n                        \"path\": entry,\n                        \"target\": target,\n                        \"cache_filename\": _cache_from_source(filename),\n                    }\n                    logger.debug(\n                        f\"Found potential Basilisp module '{fullname}' in file '{filename}'\"\n                    )\n                    return importlib.machinery.ModuleSpec(\n                        fullname, self, origin=filename, loader_state=state\n                    )\n        return None",
        "return_type_from_source": "Optional[importlib.machinery.ModuleSpec]"
    },
    {
        "extra_left": [],
        "left": "def symbol(name, ns = None, meta=None) -> ",
        "right": ":\n    \"\"\"Create a new symbol.\"\"\"\n    return Symbol(name, ns=ns, meta=meta)",
        "return_type_from_source": "Symbol"
    },
    {
        "extra_left": [],
        "left": "def complete(\n    text, kw_cache = __INTERN\n) -> ",
        "right": ":\n    \"\"\"Return an iterable of possible completions for the given text.\"\"\"\n    assert text.startswith(\":\")\n    interns = kw_cache.deref()\n    text = text[1:]\n\n    if \"/\" in text:\n        prefix, suffix = text.split(\"/\", maxsplit=1)\n        results = filter(\n            lambda kw: (kw.ns is not None and kw.ns == prefix)\n            and kw.name.startswith(suffix),\n            interns.itervalues(),\n        )\n    else:\n        results = filter(\n            lambda kw: kw.name.startswith(text)\n            or (kw.ns is not None and kw.ns.startswith(text)),\n            interns.itervalues(),\n        )\n\n    return map(str, results)",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def __get_or_create(\n    kw_cache, h, name, ns\n) -> ",
        "right": ":\n    \"\"\"Private swap function used to either get the interned keyword\n    instance from the input string.\"\"\"\n    if h in kw_cache:\n        return kw_cache\n    kw = Keyword(name, ns=ns)\n    return kw_cache.set(h, kw)",
        "return_type_from_source": "PMap"
    },
    {
        "extra_left": [],
        "left": "def keyword(\n    name,\n    ns = None,\n    kw_cache = __INTERN,\n) -> ",
        "right": ":\n    \"\"\"Create a new keyword.\"\"\"\n    h = hash((name, ns))\n    return kw_cache.swap(__get_or_create, h, name, ns)[h]",
        "return_type_from_source": "Keyword"
    },
    {
        "extra_left": [],
        "left": "def _load_attr(name, ctx = ast.Load()) -> ",
        "right": ":\n    \"\"\"Generate recursive Python Attribute AST nodes for resolving nested\n    names.\"\"\"\n    attrs = name.split(\".\")\n\n    def attr_node(node, idx):\n        if idx >= len(attrs):\n            node.ctx = ctx\n            return node\n        return attr_node(\n            ast.Attribute(value=node, attr=attrs[idx], ctx=ast.Load()), idx + 1\n        )\n\n    return attr_node(ast.Name(id=attrs[0], ctx=ast.Load()), 1)",
        "return_type_from_source": "ast.Attribute"
    },
    {
        "extra_left": [],
        "left": "def _simple_ast_generator(gen_ast):\n    \"\"\"Wrap simpler AST generators to return a GeneratedPyAST.\"\"\"\n\n    @wraps(gen_ast)\n    def wrapped_ast_generator(ctx, form) -> ",
        "right": ":\n        return GeneratedPyAST(node=gen_ast(ctx, form))\n\n    return wrapped_ast_generator",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _ast_with_loc(\n    py_ast, env, include_dependencies = False\n) -> ",
        "right": ":\n    \"\"\"Hydrate Generated Python AST nodes with line numbers and column offsets\n    if they exist in the node environment.\"\"\"\n    if env.line is not None:\n        py_ast.node.lineno = env.line\n\n        if include_dependencies:\n            for dep in py_ast.dependencies:\n                dep.lineno = env.line\n\n    if env.col is not None:\n        py_ast.node.col_offset = env.col\n\n        if include_dependencies:\n            for dep in py_ast.dependencies:\n                dep.col_offset = env.col\n\n    return py_ast",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _with_ast_loc(f):\n    \"\"\"Wrap a generator function in a decorator to supply line and column\n    information to the returned Python AST node. Dependency nodes will not\n    be hydrated, functions whose returns need dependency nodes to be\n    hydrated should use `_with_ast_loc_deps` below.\"\"\"\n\n    @wraps(f)\n    def with_lineno_and_col(\n        ctx, node, *args, **kwargs\n    ) -> ",
        "right": ":\n        py_ast = f(ctx, node, *args, **kwargs)\n        return _ast_with_loc(py_ast, node.env)\n\n    return with_lineno_and_col",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _with_ast_loc_deps(f):\n    \"\"\"Wrap a generator function in a decorator to supply line and column\n    information to the returned Python AST node and dependency nodes.\n\n    Dependency nodes should likely only be included if they are new nodes\n    created in the same function wrapped by this function. Otherwise, dependencies\n    returned from e.g. calling `gen_py_ast` should be assumed to already have\n    their location information hydrated.\"\"\"\n\n    @wraps(f)\n    def with_lineno_and_col(\n        ctx, node, *args, **kwargs\n    ) -> ",
        "right": ":\n        py_ast = f(ctx, node, *args, **kwargs)\n        return _ast_with_loc(py_ast, node.env, include_dependencies=True)\n\n    return with_lineno_and_col",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _is_dynamic(v) -> ",
        "right": ":\n    \"\"\"Return True if the Var holds a value which should be compiled to a dynamic\n    Var access.\"\"\"\n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.get(SYM_DYNAMIC_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _is_redefable(v) -> ",
        "right": ":\n    \"\"\"Return True if the Var can be redefined.\"\"\"\n    return (\n        Maybe(v.meta)\n        .map(lambda m: m.get(SYM_REDEF_META_KEY, None))  # type: ignore\n        .or_else_get(False)\n    )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def statementize(e) -> ",
        "right": ":\n    \"\"\"Transform non-statements into ast.Expr nodes so they can\n    stand alone as statements.\"\"\"\n    # noinspection PyPep8\n    if isinstance(\n        e,\n        (\n            ast.Assign,\n            ast.AnnAssign,\n            ast.AugAssign,\n            ast.Expr,\n            ast.Raise,\n            ast.Assert,\n            ast.Pass,\n            ast.Import,\n            ast.ImportFrom,\n            ast.If,\n            ast.For,\n            ast.While,\n            ast.Continue,\n            ast.Break,\n            ast.Try,\n            ast.ExceptHandler,\n            ast.With,\n            ast.FunctionDef,\n            ast.Return,\n            ast.Yield,\n            ast.YieldFrom,\n            ast.Global,\n            ast.ClassDef,\n            ast.AsyncFunctionDef,\n            ast.AsyncFor,\n            ast.AsyncWith,\n        ),\n    ):\n        return e\n    return ast.Expr(value=e)",
        "return_type_from_source": "ast.AST"
    },
    {
        "extra_left": [],
        "left": "def expressionize(\n    body,\n    fn_name,\n    args = None,\n    vargs = None,\n) -> ",
        "right": ":\n    \"\"\"Given a series of expression AST nodes, create a function AST node\n    with the given name that can be called and will return the result of\n    the final expression in the input body nodes.\n\n    This helps to fix the impedance mismatch of Python, which includes\n    statements and expressions, and Lisps, which have only expressions.\n    \"\"\"\n    args = Maybe(args).or_else_get([])\n    body_nodes = list(map(statementize, body.dependencies))\n    body_nodes.append(ast.Return(value=body.node))\n\n    return ast.FunctionDef(\n        name=fn_name,\n        args=ast.arguments(\n            args=args,\n            kwarg=None,\n            vararg=vargs,\n            kwonlyargs=[],\n            defaults=[],\n            kw_defaults=[],\n        ),\n        body=body_nodes,\n        decorator_list=[],\n        returns=None,\n    )",
        "return_type_from_source": "ast.FunctionDef"
    },
    {
        "extra_left": [],
        "left": "def __should_warn_on_redef(\n    ctx, defsym, safe_name, def_meta\n) -> ",
        "right": ":\n    \"\"\"Return True if the compiler should emit a warning about this name being redefined.\"\"\"\n    no_warn_on_redef = def_meta.entry(SYM_NO_WARN_ON_REDEF_META_KEY, False)\n    if no_warn_on_redef:\n        return False\n    elif safe_name in ctx.current_ns.module.__dict__:\n        return True\n    elif defsym in ctx.current_ns.interns:\n        var = ctx.current_ns.find(defsym)\n        assert var is not None, f\"Var {defsym} cannot be none here\"\n\n        if var.meta is not None and var.meta.entry(SYM_REDEF_META_KEY):\n            return False\n        elif var.is_bound:\n            return True\n        else:\n            return False\n    else:\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _do_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `do` expression.\"\"\"\n    assert node.op == NodeOp.DO\n    assert not node.is_body\n\n    body_ast = GeneratedPyAST.reduce(\n        *map(partial(gen_py_ast, ctx), chain(node.statements, [node.ret]))\n    )\n\n    fn_body_ast = []\n    do_result_name = genname(_DO_PREFIX)\n    fn_body_ast.extend(map(statementize, body_ast.dependencies))\n    fn_body_ast.append(\n        ast.Assign(\n            targets=[ast.Name(id=do_result_name, ctx=ast.Store())], value=body_ast.node\n        )\n    )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=do_result_name, ctx=ast.Load()), dependencies=fn_body_ast\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def __fn_name(s) -> ",
        "right": ":\n    \"\"\"Generate a safe Python function name from a function name symbol.\n    If no symbol is provided, generate a name with a default prefix.\"\"\"\n    return genname(\"__\" + munge(Maybe(s).or_else_get(_FN_PREFIX)))",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def __single_arity_fn_to_py_ast(\n    ctx,\n    node,\n    method,\n    def_name = None,\n    meta_node = None,\n) -> ",
        "right": ":\n    \"\"\"Return a Python AST node for a function with a single arity.\"\"\"\n    assert node.op == NodeOp.FN\n    assert method.op == NodeOp.FN_METHOD\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n    with ctx.new_symbol_table(py_fn_name), ctx.new_recur_point(\n        method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n    ):\n        # Allow named anonymous functions to recursively call themselves\n        if lisp_fn_name is not None:\n            ctx.symbol_table.new_symbol(\n                sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n            )\n\n        fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n            ctx, method.params, method.body\n        )\n        meta_deps, meta_decorators = __fn_meta(ctx, meta_node)\n        return GeneratedPyAST(\n            node=ast.Name(id=py_fn_name, ctx=ast.Load()),\n            dependencies=list(\n                chain(\n                    meta_deps,\n                    [\n                        py_fn_node(\n                            name=py_fn_name,\n                            args=ast.arguments(\n                                args=fn_args,\n                                kwarg=None,\n                                vararg=varg,\n                                kwonlyargs=[],\n                                defaults=[],\n                                kw_defaults=[],\n                            ),\n                            body=fn_body_ast,\n                            decorator_list=list(\n                                chain(\n                                    meta_decorators,\n                                    [_BASILISP_FN_FN_NAME],\n                                    [_TRAMPOLINE_FN_NAME]\n                                    if ctx.recur_point.has_recur\n                                    else [],\n                                )\n                            ),\n                            returns=None,\n                        )\n                    ],\n                )\n            ),\n        )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def __multi_arity_fn_to_py_ast(  # pylint: disable=too-many-locals\n    ctx,\n    node,\n    methods,\n    def_name = None,\n    meta_node = None,\n) -> ",
        "right": ":\n    \"\"\"Return a Python AST node for a function with multiple arities.\"\"\"\n    assert node.op == NodeOp.FN\n    assert all([method.op == NodeOp.FN_METHOD for method in methods])\n\n    lisp_fn_name = node.local.name if node.local is not None else None\n    py_fn_name = __fn_name(lisp_fn_name) if def_name is None else munge(def_name)\n\n    py_fn_node = ast.AsyncFunctionDef if node.is_async else ast.FunctionDef\n\n    arity_to_name = {}\n    rest_arity_name = None\n    fn_defs = []\n    for method in methods:\n        arity_name = f\"{py_fn_name}__arity{'_rest' if method.is_variadic else method.fixed_arity}\"\n        if method.is_variadic:\n            rest_arity_name = arity_name\n        else:\n            arity_to_name[method.fixed_arity] = arity_name\n\n        with ctx.new_symbol_table(arity_name), ctx.new_recur_point(\n            method.loop_id, RecurType.FN, is_variadic=node.is_variadic\n        ):\n            # Allow named anonymous functions to recursively call themselves\n            if lisp_fn_name is not None:\n                ctx.symbol_table.new_symbol(\n                    sym.symbol(lisp_fn_name), py_fn_name, LocalType.FN\n                )\n\n            fn_args, varg, fn_body_ast = __fn_args_to_py_ast(\n                ctx, method.params, method.body\n            )\n            fn_defs.append(\n                py_fn_node(\n                    name=arity_name,\n                    args=ast.arguments(\n                        args=fn_args,\n                        kwarg=None,\n                        vararg=varg,\n                        kwonlyargs=[],\n                        defaults=[],\n                        kw_defaults=[],\n                    ),\n                    body=fn_body_ast,\n                    decorator_list=[_TRAMPOLINE_FN_NAME]\n                    if ctx.recur_point.has_recur\n                    else [],\n                    returns=None,\n                )\n            )\n\n    dispatch_fn_ast = __multi_arity_dispatch_fn(\n        ctx,\n        py_fn_name,\n        arity_to_name,\n        default_name=rest_arity_name,\n        max_fixed_arity=node.max_fixed_arity,\n        meta_node=meta_node,\n        is_async=node.is_async,\n    )\n\n    return GeneratedPyAST(\n        node=dispatch_fn_ast.node,\n        dependencies=list(chain(fn_defs, dispatch_fn_ast.dependencies)),\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _fn_to_py_ast(\n    ctx,\n    node,\n    def_name = None,\n    meta_node = None,\n) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `fn` expression.\"\"\"\n    assert node.op == NodeOp.FN\n    if len(node.methods) == 1:\n        return __single_arity_fn_to_py_ast(\n            ctx, node, next(iter(node.methods)), def_name=def_name, meta_node=meta_node\n        )\n    else:\n        return __multi_arity_fn_to_py_ast(\n            ctx, node, node.methods, def_name=def_name, meta_node=meta_node\n        )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def __if_body_to_py_ast(\n    ctx, node, result_name\n) -> ",
        "right": ":\n    \"\"\"Generate custom `if` nodes to handle `recur` bodies.\n\n    Recur nodes can appear in the then and else expressions of `if` forms.\n    Recur nodes generate Python `continue` statements, which we would otherwise\n    attempt to insert directly into an expression. Python will complain if\n    it finds a statement in an expression AST slot, so we special case the\n    recur handling here.\"\"\"\n    if node.op == NodeOp.RECUR and ctx.recur_point.type == RecurType.LOOP:\n        assert isinstance(node, Recur)\n        return _recur_to_py_ast(ctx, node)\n    elif node.op == NodeOp.DO:\n        assert isinstance(node, Do)\n        if_body = _synthetic_do_to_py_ast(ctx, node.assoc(is_body=True))\n        return GeneratedPyAST(\n            node=ast.Assign(\n                targets=[ast.Name(id=result_name, ctx=ast.Store())], value=if_body.node\n            ),\n            dependencies=list(map(statementize, if_body.dependencies)),\n        )\n    else:\n        py_ast = gen_py_ast(ctx, node)\n        return GeneratedPyAST(\n            node=ast.Assign(\n                targets=[ast.Name(id=result_name, ctx=ast.Store())], value=py_ast.node\n            ),\n            dependencies=py_ast.dependencies,\n        )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _if_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Generate an intermediate if statement which assigns to a temporary\n    variable, which is returned as the expression value at the end of\n    evaluation.\n\n    Every expression in Basilisp is true if it is not the literal values nil\n    or false. This function compiles direct checks for the test value against\n    the Python values None and False to accommodate this behavior.\n\n    Note that the if and else bodies are switched in compilation so that we\n    can perform a short-circuit or comparison, rather than exhaustively checking\n    for both false and nil each time.\"\"\"\n    assert node.op == NodeOp.IF\n\n    test_ast = gen_py_ast(ctx, node.test)\n    result_name = genname(_IF_RESULT_PREFIX)\n\n    then_ast = __if_body_to_py_ast(ctx, node.then, result_name)\n    else_ast = __if_body_to_py_ast(ctx, node.else_, result_name)\n\n    test_name = genname(_IF_TEST_PREFIX)\n    test_assign = ast.Assign(\n        targets=[ast.Name(id=test_name, ctx=ast.Store())], value=test_ast.node\n    )\n\n    ifstmt = ast.If(\n        test=ast.BoolOp(\n            op=ast.Or(),\n            values=[\n                ast.Compare(\n                    left=ast.NameConstant(None),\n                    ops=[ast.Is()],\n                    comparators=[ast.Name(id=test_name, ctx=ast.Load())],\n                ),\n                ast.Compare(\n                    left=ast.NameConstant(False),\n                    ops=[ast.Is()],\n                    comparators=[ast.Name(id=test_name, ctx=ast.Load())],\n                ),\n            ],\n        ),\n        values=[],\n        body=list(map(statementize, chain(else_ast.dependencies, [else_ast.node]))),\n        orelse=list(map(statementize, chain(then_ast.dependencies, [then_ast.node]))),\n    )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=result_name, ctx=ast.Load()),\n        dependencies=list(chain(test_ast.dependencies, [test_assign, ifstmt])),\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _invoke_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a Basilisp function invocation.\"\"\"\n    assert node.op == NodeOp.INVOKE\n\n    fn_ast = gen_py_ast(ctx, node.fn)\n    args_deps, args_nodes = _collection_ast(ctx, node.args)\n\n    return GeneratedPyAST(\n        node=ast.Call(func=fn_ast.node, args=list(args_nodes), keywords=[]),\n        dependencies=list(chain(fn_ast.dependencies, args_deps)),\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _quote_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `quote` expression.\"\"\"\n    assert node.op == NodeOp.QUOTE\n    return _const_node_to_py_ast(ctx, node.expr)",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def __loop_recur_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST node for `recur` occurring inside a `loop`.\"\"\"\n    assert node.op == NodeOp.RECUR\n\n    recur_deps = []\n    recur_targets = []\n    recur_exprs = []\n    for name, expr in zip(ctx.recur_point.binding_names, node.exprs):\n        expr_ast = gen_py_ast(ctx, expr)\n        recur_deps.extend(expr_ast.dependencies)\n        recur_targets.append(ast.Name(id=name, ctx=ast.Store()))\n        recur_exprs.append(expr_ast.node)\n\n    if len(recur_targets) == 1:\n        assert len(recur_exprs) == 1\n        recur_deps.append(ast.Assign(targets=recur_targets, value=recur_exprs[0]))\n    else:\n        recur_deps.append(\n            ast.Assign(\n                targets=[ast.Tuple(elts=recur_targets, ctx=ast.Store())],\n                value=ast.Tuple(elts=recur_exprs, ctx=ast.Load()),\n            )\n        )\n    recur_deps.append(ast.Continue())\n\n    return GeneratedPyAST(node=ast.NameConstant(None), dependencies=recur_deps)",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _recur_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `recur` expression.\n\n    Note that `recur` nodes can only legally appear in two AST locations:\n      (1) in :then or :else expressions in :if nodes, and\n      (2) in :ret expressions in :do nodes\n\n    As such, both of these handlers special case the recur construct, as it\n    is the only case in which the code generator emits a statement rather than\n    an expression.\"\"\"\n    assert node.op == NodeOp.RECUR\n    assert ctx.recur_point is not None, \"Must have set a recur point to recur\"\n    handle_recur = _RECUR_TYPE_HANDLER.get(ctx.recur_point.type)\n    assert (\n        handle_recur is not None\n    ), f\"No recur point handler defined for {ctx.recur_point.type}\"\n    ctx.recur_point.has_recur = True\n    return handle_recur(ctx, node)",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _set_bang_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `set!` expression.\"\"\"\n    assert node.op == NodeOp.SET_BANG\n\n    val_temp_name = genname(\"set_bang_val\")\n    val_ast = gen_py_ast(ctx, node.val)\n\n    target = node.target\n    assert isinstance(\n        target, (HostField, Local, VarRef)\n    ), f\"invalid set! target type {type(target)}\"\n\n    if isinstance(target, HostField):\n        target_ast = _interop_prop_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, VarRef):\n        target_ast = _var_sym_to_py_ast(ctx, target, is_assigning=True)\n    elif isinstance(target, Local):\n        target_ast = _local_sym_to_py_ast(ctx, target, is_assigning=True)\n    else:  # pragma: no cover\n        raise GeneratorException(\n            f\"invalid set! target type {type(target)}\", lisp_ast=target\n        )\n\n    return GeneratedPyAST(\n        node=ast.Name(id=val_temp_name, ctx=ast.Load()),\n        dependencies=list(\n            chain(\n                val_ast.dependencies,\n                [\n                    ast.Assign(\n                        targets=[ast.Name(id=val_temp_name, ctx=ast.Store())],\n                        value=val_ast.node,\n                    )\n                ],\n                target_ast.dependencies,\n                [ast.Assign(targets=[target_ast.node], value=val_ast.node)],\n            )\n        ),\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _throw_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `throw` expression.\"\"\"\n    assert node.op == NodeOp.THROW\n\n    throw_fn = genname(_THROW_PREFIX)\n    exc_ast = gen_py_ast(ctx, node.exception)\n    raise_body = ast.Raise(exc=exc_ast.node, cause=None)\n\n    return GeneratedPyAST(\n        node=ast.Call(func=ast.Name(id=throw_fn, ctx=ast.Load()), args=[], keywords=[]),\n        dependencies=[\n            ast.FunctionDef(\n                name=throw_fn,\n                args=ast.arguments(\n                    args=[],\n                    kwarg=None,\n                    vararg=None,\n                    kwonlyargs=[],\n                    defaults=[],\n                    kw_defaults=[],\n                ),\n                body=list(chain(exc_ast.dependencies, [raise_body])),\n                decorator_list=[],\n                returns=None,\n            )\n        ],\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _try_to_py_ast(ctx, node) -> ",
        "right": ":\n    \"\"\"Return a Python AST Node for a `try` expression.\"\"\"\n    assert node.op == NodeOp.TRY\n\n    try_expr_name = genname(\"try_expr\")\n\n    body_ast = _synthetic_do_to_py_ast(ctx, node.body)\n    catch_handlers = list(\n        map(partial(__catch_to_py_ast, ctx, try_expr_name=try_expr_name), node.catches)\n    )\n\n    finallys = []\n    if node.finally_ is not None:\n        finally_ast = _synthetic_do_to_py_ast(ctx, node.finally_)\n        finallys.extend(map(statementize, finally_ast.dependencies))\n        finallys.append(statementize(finally_ast.node))\n\n    return GeneratedPyAST(\n        node=ast.Name(id=try_expr_name, ctx=ast.Load()),\n        dependencies=[\n            ast.Try(\n                body=list(\n                    chain(\n                        body_ast.dependencies,\n                        [\n                            ast.Assign(\n                                targets=[ast.Name(id=try_expr_name, ctx=ast.Store())],\n                                value=body_ast.node,\n                            )\n                        ],\n                    )\n                ),\n                handlers=catch_handlers,\n                orelse=[],\n                finalbody=finallys,\n            )\n        ],\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _local_sym_to_py_ast(\n    ctx, node, is_assigning = False\n) -> ",
        "right": ":\n    \"\"\"Generate a Python AST node for accessing a locally defined Python variable.\"\"\"\n    assert node.op == NodeOp.LOCAL\n\n    sym_entry = ctx.symbol_table.find_symbol(sym.symbol(node.name))\n    assert sym_entry is not None\n\n    if node.local == LocalType.FIELD:\n        this_entry = ctx.symbol_table.find_symbol(ctx.current_this)\n        assert this_entry is not None, \"Field type local must have this\"\n\n        return GeneratedPyAST(\n            node=_load_attr(\n                f\"{this_entry.munged}.{sym_entry.munged}\",\n                ctx=ast.Store() if is_assigning else ast.Load(),\n            )\n        )\n    else:\n        return GeneratedPyAST(\n            node=ast.Name(\n                id=sym_entry.munged, ctx=ast.Store() if is_assigning else ast.Load()\n            )\n        )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def __var_find_to_py_ast(\n    var_name, ns_name, py_var_ctx\n) -> ",
        "right": ":\n    \"\"\"Generate Var.find calls for the named symbol.\"\"\"\n    return GeneratedPyAST(\n        node=ast.Attribute(\n            value=ast.Call(\n                func=_FIND_VAR_FN_NAME,\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME,\n                        args=[ast.Str(var_name)],\n                        keywords=[ast.keyword(arg=\"ns\", value=ast.Str(ns_name))],\n                    )\n                ],\n                keywords=[],\n            ),\n            attr=\"value\",\n            ctx=py_var_ctx,\n        )\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _var_sym_to_py_ast(\n    ctx, node, is_assigning = False\n) -> ",
        "right": ":\n    \"\"\"Generate a Python AST node for accessing a Var.\n\n    If the Var is marked as :dynamic or :redef or the compiler option\n    USE_VAR_INDIRECTION is active, do not compile to a direct access.\n    If the corresponding function name is not defined in a Python module,\n    no direct variable access is possible and Var.find indirection must be\n    used.\"\"\"\n    assert node.op == NodeOp.VAR\n\n    var = node.var\n    ns = var.ns\n    ns_name = ns.name\n    ns_module = ns.module\n    safe_ns = munge(ns_name)\n    var_name = var.name.name\n    py_var_ctx = ast.Store() if is_assigning else ast.Load()\n\n    # Return the actual var, rather than its value if requested\n    if node.return_var:\n        return GeneratedPyAST(\n            node=ast.Call(\n                func=_FIND_VAR_FN_NAME,\n                args=[\n                    ast.Call(\n                        func=_NEW_SYM_FN_NAME,\n                        args=[ast.Str(var_name)],\n                        keywords=[ast.keyword(arg=\"ns\", value=ast.Str(ns_name))],\n                    )\n                ],\n                keywords=[],\n            )\n        )\n\n    # Check if we should use Var indirection\n    if ctx.use_var_indirection or _is_dynamic(var) or _is_redefable(var):\n        return __var_find_to_py_ast(var_name, ns_name, py_var_ctx)\n\n    # Otherwise, try to direct-link it like a Python variable\n    # Try without allowing builtins first\n    safe_name = munge(var_name)\n    if safe_name not in ns_module.__dict__:\n        # Try allowing builtins\n        safe_name = munge(var_name, allow_builtins=True)\n\n    if safe_name in ns_module.__dict__:\n        if ns is ctx.current_ns:\n            return GeneratedPyAST(node=ast.Name(id=safe_name, ctx=py_var_ctx))\n        return GeneratedPyAST(node=_load_attr(f\"{safe_ns}.{safe_name}\", ctx=py_var_ctx))\n\n    if ctx.warn_on_var_indirection:\n        logger.warning(f\"could not resolve a direct link to Var '{var_name}'\")\n\n    return __var_find_to_py_ast(var_name, ns_name, py_var_ctx)",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _interop_prop_to_py_ast(\n    ctx, node, is_assigning = False\n) -> ",
        "right": ":\n    \"\"\"Generate a Python AST node for Python interop property access.\"\"\"\n    assert node.op == NodeOp.HOST_FIELD\n\n    target_ast = gen_py_ast(ctx, node.target)\n\n    return GeneratedPyAST(\n        node=ast.Attribute(\n            value=target_ast.node,\n            attr=munge(node.field),\n            ctx=ast.Store() if is_assigning else ast.Load(),\n        ),\n        dependencies=target_ast.dependencies,\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _maybe_class_to_py_ast(_, node) -> ",
        "right": ":\n    \"\"\"Generate a Python AST node for accessing a potential Python module\n    variable name.\"\"\"\n    assert node.op == NodeOp.MAYBE_CLASS\n    return GeneratedPyAST(\n        node=ast.Name(\n            id=Maybe(_MODULE_ALIASES.get(node.class_)).or_else_get(node.class_),\n            ctx=ast.Load(),\n        )\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _maybe_host_form_to_py_ast(\n    _, node\n) -> ",
        "right": ":\n    \"\"\"Generate a Python AST node for accessing a potential Python module\n    variable name with a namespace.\"\"\"\n    assert node.op == NodeOp.MAYBE_HOST_FORM\n    return GeneratedPyAST(\n        node=_load_attr(\n            f\"{Maybe(_MODULE_ALIASES.get(node.class_)).or_else_get(node.class_)}.{node.field}\"\n        )\n    )",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _const_val_to_py_ast(ctx, form) -> ",
        "right": ":\n    \"\"\"Generate Python AST nodes for constant Lisp forms.\n\n    Nested values in collections for :const nodes are not parsed, so recursive\n    structures need to call into this function to generate Python AST nodes for\n    nested elements. For top-level :const Lisp AST nodes, see\n    `_const_node_to_py_ast`.\"\"\"\n    handle_value = _CONST_VALUE_HANDLERS.get(type(form))\n    if handle_value is None and isinstance(form, ISeq):\n        handle_value = _const_seq_to_py_ast  # type: ignore\n    assert handle_value is not None, \"A type handler must be defined for constants\"\n    return handle_value(ctx, form)",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _collection_literal_to_py_ast(\n    ctx, form\n) -> ",
        "right": ":\n    \"\"\"Turn a quoted collection literal of Lisp forms into Python AST nodes.\n\n    This function can only handle constant values. It does not call back into\n    the generic AST generators, so only constant values will be generated down\n    this path.\"\"\"\n    yield from map(partial(_const_val_to_py_ast, ctx), form)",
        "return_type_from_source": "Iterable[GeneratedPyAST]"
    },
    {
        "extra_left": [],
        "left": "def gen_py_ast(ctx, lisp_ast) -> ",
        "right": ":\n    \"\"\"Take a Lisp AST node as an argument and produce zero or more Python\n    AST nodes.\n\n    This is the primary entrypoint for generating AST nodes from Lisp\n    syntax. It may be called recursively to compile child forms.\"\"\"\n    op = lisp_ast.op\n    assert op is not None, \"Lisp AST nodes must have an :op key\"\n    handle_node = _NODE_HANDLERS.get(op)\n    assert (\n        handle_node is not None\n    ), f\"Lisp AST nodes :op has no handler defined for op {op}\"\n    return handle_node(ctx, lisp_ast)",
        "return_type_from_source": "GeneratedPyAST"
    },
    {
        "extra_left": [],
        "left": "def _module_imports(ctx) -> ",
        "right": ":\n    \"\"\"Generate the Python Import AST node for importing all required\n    language support modules.\"\"\"\n    # Yield `import basilisp` so code attempting to call fully qualified\n    # `basilisp.lang...` modules don't result in compiler errors\n    yield ast.Import(names=[ast.alias(name=\"basilisp\", asname=None)])\n    for imp in ctx.imports:\n        name = imp.key.name\n        alias = _MODULE_ALIASES.get(name, None)\n        yield ast.Import(names=[ast.alias(name=name, asname=alias)])",
        "return_type_from_source": "Iterable[ast.Import]"
    },
    {
        "extra_left": [],
        "left": "def _from_module_import() -> ",
        "right": ":\n    \"\"\"Generate the Python From ... Import AST node for importing\n    language support modules.\"\"\"\n    return ast.ImportFrom(\n        module=\"basilisp.lang.runtime\",\n        names=[ast.alias(name=\"Var\", asname=_VAR_ALIAS)],\n        level=0,\n    )",
        "return_type_from_source": "ast.ImportFrom"
    },
    {
        "extra_left": [],
        "left": "def _ns_var(\n    py_ns_var = _NS_VAR, lisp_ns_var = LISP_NS_VAR, lisp_ns_ns = CORE_NS\n) -> ",
        "right": ":\n    \"\"\"Assign a Python variable named `ns_var` to the value of the current\n    namespace.\"\"\"\n    return ast.Assign(\n        targets=[ast.Name(id=py_ns_var, ctx=ast.Store())],\n        value=ast.Call(\n            func=_FIND_VAR_FN_NAME,\n            args=[\n                ast.Call(\n                    func=_NEW_SYM_FN_NAME,\n                    args=[ast.Str(lisp_ns_var)],\n                    keywords=[ast.keyword(arg=\"ns\", value=ast.Str(lisp_ns_ns))],\n                )\n            ],\n            keywords=[],\n        ),\n    )",
        "return_type_from_source": "ast.Assign"
    },
    {
        "extra_left": [],
        "left": "def set(members, meta=None) -> ",
        "right": ":  # pylint:disable=redefined-builtin\n    \"\"\"Creates a new set.\"\"\"\n    return Set(pset(members), meta=meta)",
        "return_type_from_source": "Set[T]"
    },
    {
        "extra_left": [],
        "left": "def s(*members: T, meta=None) -> ",
        "right": ":\n    \"\"\"Creates a new set from members.\"\"\"\n    return Set(pset(members), meta=meta)",
        "return_type_from_source": "Set[T]"
    },
    {
        "extra_left": [],
        "left": "def visit_ExceptHandler(self, node) -> ",
        "right": ":\n        \"\"\"Eliminate dead code from except handler bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.ExceptHandler)\n        return ast.copy_location(\n            ast.ExceptHandler(\n                type=new_node.type,\n                name=new_node.name,\n                body=_filter_dead_code(new_node.body),\n            ),\n            new_node,\n        )",
        "return_type_from_source": "Optional[ast.AST]"
    },
    {
        "extra_left": [],
        "left": "def visit_Expr(self, node) -> ",
        "right": ":\n        \"\"\"Eliminate no-op constant expressions which are in the tree\n        as standalone statements.\"\"\"\n        if isinstance(\n            node.value,\n            (\n                ast.Constant,  # type: ignore\n                ast.Name,\n                ast.NameConstant,\n                ast.Num,\n                ast.Str,\n            ),\n        ):\n            return None\n        return node",
        "return_type_from_source": "Optional[ast.Expr]"
    },
    {
        "extra_left": [],
        "left": "def visit_FunctionDef(self, node) -> ",
        "right": ":\n        \"\"\"Eliminate dead code from function bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.FunctionDef)\n        return ast.copy_location(\n            ast.FunctionDef(\n                name=new_node.name,\n                args=new_node.args,\n                body=_filter_dead_code(new_node.body),\n                decorator_list=new_node.decorator_list,\n                returns=new_node.returns,\n            ),\n            new_node,\n        )",
        "return_type_from_source": "Optional[ast.AST]"
    },
    {
        "extra_left": [],
        "left": "def visit_While(self, node) -> ",
        "right": ":\n        \"\"\"Eliminate dead code from while bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.While)\n        return ast.copy_location(\n            ast.While(\n                test=new_node.test,\n                body=_filter_dead_code(new_node.body),\n                orelse=_filter_dead_code(new_node.orelse),\n            ),\n            new_node,\n        )",
        "return_type_from_source": "Optional[ast.AST]"
    },
    {
        "extra_left": [],
        "left": "def visit_Try(self, node) -> ",
        "right": ":\n        \"\"\"Eliminate dead code from except try bodies.\"\"\"\n        new_node = self.generic_visit(node)\n        assert isinstance(new_node, ast.Try)\n        return ast.copy_location(\n            ast.Try(\n                body=_filter_dead_code(new_node.body),\n                handlers=new_node.handlers,\n                orelse=_filter_dead_code(new_node.orelse),\n                finalbody=_filter_dead_code(new_node.finalbody),\n            ),\n            new_node,\n        )",
        "return_type_from_source": "Optional[ast.AST]"
    },
    {
        "extra_left": [],
        "left": "def _new_module(name, doc=None) -> ",
        "right": ":\n    \"\"\"Create a new empty Basilisp Python module.\n    Modules are created for each Namespace when it is created.\"\"\"\n    mod = types.ModuleType(name, doc=doc)\n    mod.__loader__ = None\n    mod.__package__ = None\n    mod.__spec__ = None\n    mod.__basilisp_bootstrapped__ = False  # type: ignore\n    return mod",
        "return_type_from_source": "types.ModuleType"
    },
    {
        "extra_left": [],
        "left": "def rest(o) -> ",
        "right": ":\n    \"\"\"If o is a ISeq, return the elements after the first in o. If o is None,\n    returns an empty seq. Otherwise, coerces o to a seq and returns the rest.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        s = o.rest\n        if s is None:\n            return lseq.EMPTY\n        return s\n    n = to_seq(o)\n    if n is None:\n        return lseq.EMPTY\n    return n.rest",
        "return_type_from_source": "Optional[ISeq]"
    },
    {
        "extra_left": [],
        "left": "def nthnext(coll, i) -> ",
        "right": ":\n    \"\"\"Returns the nth next sequence of coll.\"\"\"\n    while True:\n        if coll is None:\n            return None\n        if i == 0:\n            return to_seq(coll)\n        i -= 1\n        coll = next_(coll)",
        "return_type_from_source": "Optional[ISeq]"
    },
    {
        "extra_left": [],
        "left": "def cons(o, seq) -> ",
        "right": ":\n    \"\"\"Creates a new sequence where o is the first element and seq is the rest.\n    If seq is None, return a list containing o. If seq is not a ISeq, attempt\n    to coerce it to a ISeq and then cons o onto the resulting sequence.\"\"\"\n    if seq is None:\n        return llist.l(o)\n    if isinstance(seq, ISeq):\n        return seq.cons(o)\n    return Maybe(to_seq(seq)).map(lambda s: s.cons(o)).or_else(lambda: llist.l(o))",
        "return_type_from_source": "ISeq"
    },
    {
        "extra_left": [],
        "left": "def to_seq(o) -> ",
        "right": ":\n    \"\"\"Coerce the argument o to a ISeq. If o is None, return None.\"\"\"\n    if o is None:\n        return None\n    if isinstance(o, ISeq):\n        return _seq_or_nil(o)\n    if isinstance(o, ISeqable):\n        return _seq_or_nil(o.seq())\n    return _seq_or_nil(lseq.sequence(o))",
        "return_type_from_source": "Optional[ISeq]"
    },
    {
        "extra_left": [],
        "left": "def concat(*seqs) -> ",
        "right": ":\n    \"\"\"Concatenate the sequences given by seqs into a single ISeq.\"\"\"\n    allseqs = lseq.sequence(itertools.chain(*filter(None, map(to_seq, seqs))))\n    if allseqs is None:\n        return lseq.EMPTY\n    return allseqs",
        "return_type_from_source": "ISeq"
    },
    {
        "extra_left": [],
        "left": "def equals(v1, v2) -> ",
        "right": ":\n    \"\"\"Compare two objects by value. Unlike the standard Python equality operator,\n    this function does not consider 1 == True or 0 == False. All other equality\n    operations are the same and performed using Python's equality operator.\"\"\"\n    if isinstance(v1, (bool, type(None))) or isinstance(v2, (bool, type(None))):\n        return v1 is v2\n    return v1 == v2",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def divide(x, y) -> ",
        "right": ":\n    \"\"\"Division reducer. If both arguments are integers, return a Fraction.\n    Otherwise, return the true division of x and y.\"\"\"\n    if isinstance(x, int) and isinstance(y, int):\n        return Fraction(x, y)\n    return x / y",
        "return_type_from_source": "LispNumber"
    },
    {
        "extra_left": [],
        "left": "def sort(coll, f=None) -> ",
        "right": ":\n    \"\"\"Return a sorted sequence of the elements in coll. If a comparator\n    function f is provided, compare elements in coll using f.\"\"\"\n    return to_seq(sorted(coll, key=Maybe(f).map(functools.cmp_to_key).value))",
        "return_type_from_source": "Optional[ISeq]"
    },
    {
        "extra_left": [],
        "left": "def lrepr(o, human_readable = False) -> ",
        "right": ":\n    \"\"\"Produce a string representation of an object. If human_readable is False,\n    the string representation of Lisp objects is something that can be read back\n    in by the reader as the same object.\"\"\"\n    core_ns = Namespace.get(sym.symbol(CORE_NS))\n    assert core_ns is not None\n    return lobj.lrepr(\n        o,\n        human_readable=human_readable,\n        print_dup=core_ns.find(sym.symbol(_PRINT_DUP_VAR_NAME)).value,  # type: ignore\n        print_length=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_LENGTH_VAR_NAME)\n        ).value,\n        print_level=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_LEVEL_VAR_NAME)\n        ).value,\n        print_meta=core_ns.find(sym.symbol(_PRINT_META_VAR_NAME)).value,  # type: ignore\n        print_readably=core_ns.find(  # type: ignore\n            sym.symbol(_PRINT_READABLY_VAR_NAME)\n        ).value,\n    )",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _collect_args(args) -> ",
        "right": ":\n    \"\"\"Collect Python starred arguments into a Basilisp list.\"\"\"\n    if isinstance(args, tuple):\n        return llist.list(args)\n    raise TypeError(\"Python variadic arguments should always be a tuple\")",
        "return_type_from_source": "ISeq"
    },
    {
        "extra_left": [],
        "left": "def resolve_alias(s, ns = None) -> ",
        "right": ":\n    \"\"\"Resolve the aliased symbol in the current namespace.\"\"\"\n    if s in _SPECIAL_FORMS:\n        return s\n\n    ns = Maybe(ns).or_else(get_current_ns)\n    if s.ns is not None:\n        aliased_ns = ns.get_alias(sym.symbol(s.ns))\n        if aliased_ns is not None:\n            return sym.symbol(s.name, aliased_ns.name)\n        else:\n            return s\n    else:\n        which_var = ns.find(sym.symbol(s.name))\n        if which_var is not None:\n            return sym.symbol(which_var.name.name, which_var.ns.name)\n        else:\n            return sym.symbol(s.name, ns=ns.name)",
        "return_type_from_source": "sym.Symbol"
    },
    {
        "extra_left": [],
        "left": "def resolve_var(s, ns = None) -> ",
        "right": ":\n    \"\"\"Resolve the aliased symbol to a Var from the specified\n    namespace, or the current namespace if none is specified.\"\"\"\n    return Var.find(resolve_alias(s, ns))",
        "return_type_from_source": "Optional[Var]"
    },
    {
        "extra_left": [],
        "left": "def add_generated_python(\n    generated_python,\n    var_name = _GENERATED_PYTHON_VAR_NAME,\n    which_ns = None,\n) -> ",
        "right": ":\n    \"\"\"Add generated Python code to a dynamic variable in which_ns.\"\"\"\n    if which_ns is None:\n        which_ns = get_current_ns().name\n    ns_sym = sym.Symbol(var_name, ns=which_ns)\n    v = Maybe(Var.find(ns_sym)).or_else(\n        lambda: Var.intern(\n            sym.symbol(which_ns),  # type: ignore\n            sym.symbol(var_name),\n            \"\",\n            dynamic=True,\n            meta=lmap.map({_PRIVATE_META_KEY: True}),\n        )\n    )\n    v.value = v.value + generated_python",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def bootstrap(ns_var_name = NS_VAR_NAME, core_ns_name = CORE_NS) -> ",
        "right": ":\n    \"\"\"Bootstrap the environment with functions that are are difficult to\n    express with the very minimal lisp environment.\"\"\"\n    core_ns_sym = sym.symbol(core_ns_name)\n    ns_var_sym = sym.symbol(ns_var_name, ns=core_ns_name)\n    __NS = Maybe(Var.find(ns_var_sym)).or_else_raise(\n        lambda: RuntimeException(f\"Dynamic Var {ns_var_sym} not bound!\")\n    )\n\n    def in_ns(s):\n        ns = Namespace.get_or_create(s)\n        __NS.value = ns\n        return ns\n\n    Var.intern_unbound(core_ns_sym, sym.symbol(\"unquote\"))\n    Var.intern_unbound(core_ns_sym, sym.symbol(\"unquote-splicing\"))\n    Var.intern(\n        core_ns_sym, sym.symbol(\"in-ns\"), in_ns, meta=lmap.map({_REDEF_META_KEY: True})\n    )\n    Var.intern(\n        core_ns_sym,\n        sym.symbol(_PRINT_GENERATED_PY_VAR_NAME),\n        False,\n        dynamic=True,\n        meta=lmap.map({_PRIVATE_META_KEY: True}),\n    )\n    Var.intern(\n        core_ns_sym,\n        sym.symbol(_GENERATED_PYTHON_VAR_NAME),\n        \"\",\n        dynamic=True,\n        meta=lmap.map({_PRIVATE_META_KEY: True}),\n    )\n\n    # Dynamic Vars for controlling printing\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_DUP_VAR_NAME), lobj.PRINT_DUP, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_LENGTH_VAR_NAME), lobj.PRINT_LENGTH, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_LEVEL_VAR_NAME), lobj.PRINT_LEVEL, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym, sym.symbol(_PRINT_META_VAR_NAME), lobj.PRINT_META, dynamic=True\n    )\n    Var.intern(\n        core_ns_sym,\n        sym.symbol(_PRINT_READABLY_VAR_NAME),\n        lobj.PRINT_READABLY,\n        dynamic=True,\n    )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def add_alias(self, alias, namespace) -> ",
        "right": ":\n        \"\"\"Add a Symbol alias for the given Namespace.\"\"\"\n        self._aliases.swap(lambda m: m.assoc(alias, namespace))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def intern(self, sym, var, force = False) -> ",
        "right": ":\n        \"\"\"Intern the Var given in this namespace mapped by the given Symbol.\n        If the Symbol already maps to a Var, this method _will not overwrite_\n        the existing Var mapping unless the force keyword argument is given\n        and is True.\"\"\"\n        m = self._interns.swap(Namespace._intern, sym, var, force=force)\n        return m.entry(sym)",
        "return_type_from_source": "Var"
    },
    {
        "extra_left": [],
        "left": "def _intern(\n        m, sym, new_var, force = False\n    ) -> ",
        "right": ":\n        \"\"\"Swap function used by intern to atomically intern a new variable in\n        the symbol mapping for this Namespace.\"\"\"\n        var = m.entry(sym, None)\n        if var is None or force:\n            return m.assoc(sym, new_var)\n        return m",
        "return_type_from_source": "lmap.Map"
    },
    {
        "extra_left": [],
        "left": "def find(self, sym) -> ",
        "right": ":\n        \"\"\"Find Vars mapped by the given Symbol input or None if no Vars are\n        mapped by that Symbol.\"\"\"\n        v = self.interns.entry(sym, None)\n        if v is None:\n            return self.refers.entry(sym, None)\n        return v",
        "return_type_from_source": "Optional[Var]"
    },
    {
        "extra_left": [],
        "left": "def add_import(\n        self, sym, module, *aliases: sym.Symbol\n    ) -> ",
        "right": ":\n        \"\"\"Add the Symbol as an imported Symbol in this Namespace. If aliases are given,\n        the aliases will be applied to the \"\"\"\n        self._imports.swap(lambda m: m.assoc(sym, module))\n        if aliases:\n            self._import_aliases.swap(\n                lambda m: m.assoc(\n                    *itertools.chain.from_iterable([(alias, sym) for alias in aliases])\n                )\n            )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_import(self, sym) -> ",
        "right": ":\n        \"\"\"Return the module if a moduled named by sym has been imported into\n        this Namespace, None otherwise.\n\n        First try to resolve a module directly with the given name. If no module\n        can be resolved, attempt to resolve the module using import aliases.\"\"\"\n        mod = self.imports.entry(sym, None)\n        if mod is None:\n            alias = self.import_aliases.get(sym, None)\n            if alias is None:\n                return None\n            return self.imports.entry(alias, None)\n        return mod",
        "return_type_from_source": "Optional[types.ModuleType]"
    },
    {
        "extra_left": [],
        "left": "def add_refer(self, sym, var) -> ",
        "right": ":\n        \"\"\"Refer var in this namespace under the name sym.\"\"\"\n        if not var.is_private:\n            self._refers.swap(lambda s: s.assoc(sym, var))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_refer(self, sym) -> ",
        "right": ":\n        \"\"\"Get the Var referred by Symbol or None if it does not exist.\"\"\"\n        return self.refers.entry(sym, None)",
        "return_type_from_source": "Optional[Var]"
    },
    {
        "extra_left": [],
        "left": "def __refer_all(cls, refers, other_ns_interns) -> ",
        "right": ":\n        \"\"\"Refer all _public_ interns from another namespace.\"\"\"\n        final_refers = refers\n        for entry in other_ns_interns:\n            s = entry.key\n            var = entry.value\n            if not var.is_private:\n                final_refers = final_refers.assoc(s, var)\n        return final_refers",
        "return_type_from_source": "lmap.Map"
    },
    {
        "extra_left": [],
        "left": "def __get_or_create(\n        ns_cache,\n        name,\n        module = None,\n        core_ns_name=CORE_NS,\n    ) -> ",
        "right": ":\n        \"\"\"Private swap function used by `get_or_create` to atomically swap\n        the new namespace map into the global cache.\"\"\"\n        ns = ns_cache.entry(name, None)\n        if ns is not None:\n            return ns_cache\n        new_ns = Namespace(name, module=module)\n        if name.name != core_ns_name:\n            core_ns = ns_cache.entry(sym.symbol(core_ns_name), None)\n            assert core_ns is not None, \"Core namespace not loaded yet!\"\n            new_ns.refer_all(core_ns)\n        return ns_cache.assoc(name, new_ns)",
        "return_type_from_source": "lmap.Map"
    },
    {
        "extra_left": [],
        "left": "def __complete_alias(\n        self, prefix, name_in_ns = None\n    ) -> ",
        "right": ":\n        \"\"\"Return an iterable of possible completions matching the given\n        prefix from the list of aliased namespaces. If name_in_ns is given,\n        further attempt to refine the list to matching names in that namespace.\"\"\"\n        candidates = filter(\n            Namespace.__completion_matcher(prefix), [(s, n) for s, n in self.aliases]\n        )\n        if name_in_ns is not None:\n            for _, candidate_ns in candidates:\n                for match in candidate_ns.__complete_interns(\n                    name_in_ns, include_private_vars=False\n                ):\n                    yield f\"{prefix}/{match}\"\n        else:\n            for alias, _ in candidates:\n                yield f\"{alias}/\"",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def __complete_imports_and_aliases(\n        self, prefix, name_in_module = None\n    ) -> ",
        "right": ":\n        \"\"\"Return an iterable of possible completions matching the given\n        prefix from the list of imports and aliased imports. If name_in_module\n        is given, further attempt to refine the list to matching names in that\n        namespace.\"\"\"\n        imports = self.imports\n        aliases = lmap.map(\n            {\n                alias: imports.entry(import_name)\n                for alias, import_name in self.import_aliases\n            }\n        )\n\n        candidates = filter(\n            Namespace.__completion_matcher(prefix), itertools.chain(aliases, imports)\n        )\n        if name_in_module is not None:\n            for _, module in candidates:\n                for name in module.__dict__:\n                    if name.startswith(name_in_module):\n                        yield f\"{prefix}/{name}\"\n        else:\n            for candidate_name, _ in candidates:\n                yield f\"{candidate_name}/\"",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def __complete_refers(self, value) -> ",
        "right": ":\n        \"\"\"Return an iterable of possible completions matching the given\n        prefix from the list of referred Vars.\"\"\"\n        return map(\n            lambda entry: f\"{entry[0].name}\",\n            filter(\n                Namespace.__completion_matcher(value), [(s, v) for s, v in self.refers]\n            ),\n        )",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def complete(self, text) -> ",
        "right": ":\n        \"\"\"Return an iterable of possible completions for the given text in\n        this namespace.\"\"\"\n        assert not text.startswith(\":\")\n\n        if \"/\" in text:\n            prefix, suffix = text.split(\"/\", maxsplit=1)\n            results = itertools.chain(\n                self.__complete_alias(prefix, name_in_ns=suffix),\n                self.__complete_imports_and_aliases(prefix, name_in_module=suffix),\n            )\n        else:\n            results = itertools.chain(\n                self.__complete_alias(text),\n                self.__complete_imports_and_aliases(text),\n                self.__complete_interns(text),\n                self.__complete_refers(text),\n            )\n\n        return results",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def args(self) -> ",
        "right": ":\n        \"\"\"Return the arguments for a trampolined function. If the function\n        that is being trampolined has varargs, unroll the final argument if\n        it is a sequence.\"\"\"\n        if not self._has_varargs:\n            return self._args\n\n        try:\n            final = self._args[-1]\n            if isinstance(final, ISeq):\n                inits = self._args[:-1]\n                return tuple(itertools.chain(inits, final))\n            return self._args\n        except IndexError:\n            return ()",
        "return_type_from_source": "Tuple"
    },
    {
        "extra_left": [],
        "left": "def list(members, meta=None) -> ",
        "right": ":  # pylint:disable=redefined-builtin\n    \"\"\"Creates a new list.\"\"\"\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )",
        "return_type_from_source": "List"
    },
    {
        "extra_left": [],
        "left": "def l(*members, meta=None) -> ",
        "right": ":\n    \"\"\"Creates a new list from members.\"\"\"\n    return List(  # pylint: disable=abstract-class-instantiated\n        plist(iterable=members), meta=meta\n    )",
        "return_type_from_source": "List"
    },
    {
        "extra_left": [],
        "left": "def strongly_connected_components(graph) -> ",
        "right": ":\n\t\"\"\"Find the strongly connected components in a graph using Tarjan's algorithm.\n\t\n\tThe `graph` argument should be a dictionary mapping node names to sequences of successor nodes.\n\t\"\"\"\n\t\n\tassert check_argument_types()\n\t\n\tresult = []\n\tstack = []\n\tlow = {}\n\t\n\tdef visit(node):\n\t\tif node in low: return\n\t\t\n\t\tnum = len(low)\n\t\tlow[node] = num\n\t\tstack_pos = len(stack)\n\t\tstack.append(node)\n\t\t\n\t\tfor successor in graph[node]:\n\t\t\tvisit(successor)\n\t\t\tlow[node] = min(low[node], low[successor])\n\t\t\n\t\tif num == low[node]:\n\t\t\tcomponent = tuple(stack[stack_pos:])\n\t\t\tdel stack[stack_pos:]\n\t\t\t\n\t\t\tresult.append(component)\n\t\t\t\n\t\t\tfor item in component:\n\t\t\t\tlow[item] = len(graph)\n\t\n\tfor node in graph:\n\t\tvisit(node)\n\t\n\treturn result",
        "return_type_from_source": "List"
    },
    {
        "extra_left": [],
        "left": "def robust_topological_sort(graph) -> ",
        "right": ":\n\t\"\"\"Identify strongly connected components then perform a topological sort of those components.\"\"\"\n\t\n\tassert check_argument_types()\n\t\n\tcomponents = strongly_connected_components(graph)\n\t\n\tnode_component = {}\n\tfor component in components:\n\t\tfor node in component:\n\t\t\tnode_component[node] = component\n\t\n\tcomponent_graph = {}\n\tfor component in components:\n\t\tcomponent_graph[component] = []\n\t\n\tfor node in graph:\n\t\tnode_c = node_component[node]\n\t\tfor successor in graph[node]:\n\t\t\tsuccessor_c = node_component[successor]\n\t\t\tif node_c != successor_c:\n\t\t\t\tcomponent_graph[node_c].append(successor_c) \n\t\n\treturn topological_sort(component_graph)",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def cached(method) -> ",
        "right": ":\n    \"\"\"alternative to reify and property decorators. caches the value when it's\n    generated. It cashes it as instance._name_of_the_property.\n    \"\"\"\n    name = \"_\" + method.__name__\n\n    @property\n    def wrapper(self):\n        try:\n            return getattr(self, name)\n        except AttributeError:\n            val = method(self)\n            setattr(self, name, val)\n            return val\n\n    return wrapper",
        "return_type_from_source": "property"
    },
    {
        "extra_left": [],
        "left": "def __traceback(self) -> ",
        "right": ":\n        \"\"\"Get outer traceback text for logging.\"\"\"\n        if not self.log_traceback:\n            return \"\"\n        exc_info = sys.exc_info()\n        stack = traceback.extract_stack()\n        exc_tb = traceback.extract_tb(exc_info[2])\n        full_tb = stack[:1] + exc_tb  # cut decorator and build full traceback\n        exc_line = traceback.format_exception_only(*exc_info[:2])\n        # Make standard traceback string\n        tb_text = \"\\nTraceback (most recent call last):\\n\" + \"\".join(traceback.format_list(full_tb)) + \"\".join(exc_line)\n        return tb_text",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def __get_obj_source(self, instance, owner = None) -> ",
        "right": ":\n        \"\"\"Get object repr block.\"\"\"\n        if self.log_object_repr:\n            return f\"{instance!r}\"\n        return f\"<{owner.__name__ if owner is not None else instance.__class__.__name__}() at 0x{id(instance):X}>\"",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _get_logger_for_instance(self, instance) -> ",
        "right": ":\n        \"\"\"Get logger for log calls.\n\n        :param instance: Owner class instance. Filled only if instance created, else None.\n        :type instance: typing.Optional[owner]\n        :return: logger instance\n        :rtype: logging.Logger\n        \"\"\"\n        if self.logger is not None:  # pylint: disable=no-else-return\n            return self.logger\n        elif hasattr(instance, \"logger\") and isinstance(instance.logger, logging.Logger):\n            return instance.logger\n        elif hasattr(instance, \"log\") and isinstance(instance.log, logging.Logger):\n            return instance.log\n        return _LOGGER",
        "return_type_from_source": "logging.Logger"
    },
    {
        "extra_left": [],
        "left": "def logger(self, logger) -> ",
        "right": ":\n        \"\"\"Logger instance to use as override.\"\"\"\n        if logger is None or isinstance(logger, logging.Logger):\n            self.__logger = logger\n        else:\n            self.__logger = logging.getLogger(logger)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def gen_keywords(*args: Union[ANSIColors, ANSIStyles], **kwargs: Union[ANSIColors, ANSIStyles]) -> ",
        "right": ":\n    '''generate single escape sequence mapping.'''\n    fields = tuple()\n    values = tuple()\n    for tpl in args:\n        fields += tpl._fields\n        values += tpl\n    for prefix, tpl in kwargs.items():\n        fields += tuple(map(lambda x: '_'.join([prefix, x]), tpl._fields))\n        values += tpl\n    return namedtuple('ANSISequences', fields)(*values)",
        "return_type_from_source": "tuple"
    },
    {
        "extra_left": [],
        "left": "def annihilate(predicate, stack) -> ",
        "right": ":\n    '''Squash and reduce the input stack.\n    Removes the elements of input that match predicate and only keeps the last\n    match at the end of the stack.\n    '''\n    extra = tuple(filter(lambda x: x not in predicate, stack))\n    head = reduce(lambda x, y: y if y in predicate else x, stack, None)\n    return extra + (head,) if head else extra",
        "return_type_from_source": "tuple"
    },
    {
        "extra_left": [],
        "left": "def dedup(stack) -> ",
        "right": ":\n    '''Remove duplicates from the stack in first-seen order.'''\n    # Initializes with an accumulator and then reduces the stack with first match\n    # deduplication.\n    reducer = lambda x, y: x if y in x else x + (y,)\n    return reduce(reducer, stack, tuple())",
        "return_type_from_source": "tuple"
    },
    {
        "extra_left": [],
        "left": "def fold_map(self, fa, z, f, g=operator.add) -> ",
        "right": ":\n        ''' map `f` over the traversable, then fold over the result\n        using the supplied initial element `z` and operation `g`,\n        defaulting to addition for the latter.\n        '''\n        mapped = Functor.fatal(type(fa)).map(fa, f)\n        return self.fold_left(mapped)(z)(g)",
        "return_type_from_source": "Z"
    },
    {
        "extra_left": [],
        "left": "def _sign_web3_transaction(tx, v, r, s) -> ",
        "right": ":\n        \"\"\"\n        Signed transaction that compatible with `w3.eth.sendRawTransaction`\n        Is not used because `pyEthereum` implementation of Transaction was found to be more\n        robust regarding invalid signatures\n        \"\"\"\n        unsigned_transaction = serializable_unsigned_transaction_from_dict(tx)\n        rlp_encoded_transaction = encode_transaction(unsigned_transaction, vrs=(v, r, s))\n\n        # To get the address signing, just do ecrecover_to_pub(unsigned_transaction.hash(), v, r, s)\n        return rlp_encoded_transaction, unsigned_transaction.hash()",
        "return_type_from_source": "(bytes, HexBytes)"
    },
    {
        "extra_left": [],
        "left": "def estimate_tx_gas_with_web3(self, safe_address, to, value, data) -> ",
        "right": ":\n        \"\"\"\n        Estimate tx gas using web3\n        \"\"\"\n        return self.ethereum_client.estimate_gas(safe_address, to, value, data, block_identifier='pending')",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def estimate_tx_gas(self, safe_address, to, value, data, operation) -> ",
        "right": ":\n        \"\"\"\n        Estimate tx gas. Use the max of calculation using safe method and web3 if operation == CALL or\n        use just the safe calculation otherwise\n        \"\"\"\n        # Costs to route through the proxy and nested calls\n        proxy_gas = 1000\n        # https://github.com/ethereum/solidity/blob/dfe3193c7382c80f1814247a162663a97c3f5e67/libsolidity/codegen/ExpressionCompiler.cpp#L1764\n        # This was `false` before solc 0.4.21 -> `m_context.evmVersion().canOverchargeGasForCall()`\n        # So gas needed by caller will be around 35k\n        old_call_gas = 35000\n        safe_gas_estimation = (self.estimate_tx_gas_with_safe(safe_address, to, value, data, operation)\n                               + proxy_gas + old_call_gas)\n        # We cannot estimate DELEGATECALL (different storage)\n        if SafeOperation(operation) == SafeOperation.CALL:\n            try:\n                web3_gas_estimation = (self.estimate_tx_gas_with_web3(safe_address, to, value, data)\n                                       + proxy_gas + old_call_gas)\n            except ValueError:\n                web3_gas_estimation = 0\n            return max(safe_gas_estimation, web3_gas_estimation)\n\n        else:\n            return safe_gas_estimation",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "async def readline(self) -> ",
        "right": ":\n        \"\"\"\n        Reads one line\n\n        >>> # Keeps waiting for a linefeed incase there is none in the buffer\n        >>> await test.readline()\n\n        :returns: bytes forming a line\n        \"\"\"\n        while True:\n            line = self._serial_instance.readline()\n            if not line:\n                await asyncio.sleep(self._asyncio_sleep_time)\n            else:\n                return line",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def merge(self, status) -> ",
        "right": ":\n        \"\"\"Merge the failure message from another status into this one.\n\n        Whichever status represents parsing that has gone the farthest is\n        retained. If both statuses have gone the same distance, then the\n        expected values from both are retained.\n\n        Args:\n            status: The status to merge into this one.\n\n        Returns:\n            This ``Status`` which may have ``farthest`` and ``expected``\n            updated accordingly.\n        \"\"\"\n        if status is None or status.farthest is None:\n            # No new message; simply return unchanged\n            pass\n        elif self.farthest is None:\n            # No current message to compare to; use the message from status\n            self.farthest = status.farthest\n            self.expected = status.expected\n        elif status.farthest.position < self.farthest.position:\n            # New message is not farther; keep current message\n            pass\n        elif status.farthest.position > self.farthest.position:\n            # New message is farther than current message; replace with new message\n            self.farthest = status.farthest\n            self.expected = status.expected\n        else:\n            # New message and current message are equally far; merge messages\n            self.expected = status.expected + self.expected\n\n        return self",
        "return_type_from_source": "'Status[Input, Output]'"
    },
    {
        "extra_left": [],
        "left": "def completely_parse_reader(parser, reader) -> ",
        "right": ":\n    \"\"\"Consume reader and return Success only on complete consumption.\n\n    This is a helper function for ``parse`` methods, which return ``Success``\n    when the input is completely consumed and ``Failure`` with an appropriate\n    message otherwise.\n\n    Args:\n        parser: The parser doing the consuming\n        reader: The input being consumed\n\n    Returns:\n        A parsing ``Result``\n    \"\"\"\n    result = (parser << eof).consume(reader)\n\n    if isinstance(result, Continue):\n        return Success(result.value)\n    else:\n        used = set()\n        unique_expected = []\n        for expected_lambda in result.expected:\n            expected = expected_lambda()\n            if expected not in used:\n                used.add(expected)\n                unique_expected.append(expected)\n\n        return Failure(result.farthest.expected_error(' or '.join(unique_expected)))",
        "return_type_from_source": "Result[Output]"
    },
    {
        "extra_left": [],
        "left": "def lit(literal, *literals: Sequence[Sequence[Input]]) -> ",
        "right": ":\n    \"\"\"Match a literal sequence.\n\n    In the `TextParsers`` context, this matches the literal string\n    provided. In the ``GeneralParsers`` context, this matches a sequence of\n    input.\n\n    If multiple literals are provided, they are treated as alternatives. e.g.\n    ``lit('+', '-')`` is the same as ``lit('+') | lit('-')``.\n\n    Args:\n        literal: A literal to match\n        *literals: Alternative literals to match\n\n    Returns:\n        A ``LiteralParser`` in the ``GeneralContext``, a ``LiteralStringParser``\n        in the ``TextParsers`` context, and an ``AlternativeParser`` if multiple\n        arguments are provided.\n    \"\"\"\n    if len(literals) > 0:\n        return AlternativeParser(options.handle_literal(literal), *map(options.handle_literal, literals))\n    else:\n        return options.handle_literal(literal)",
        "return_type_from_source": "Parser"
    },
    {
        "extra_left": [],
        "left": "def opt(parser) -> ",
        "right": ":\n    \"\"\"Optionally match a parser.\n\n    An ``OptionalParser`` attempts to match ``parser``. If it succeeds, it\n    returns a list of length one with the value returned by the parser as the\n    only element. If it fails, it returns an empty list.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return OptionalParser(parser)",
        "return_type_from_source": "OptionalParser"
    },
    {
        "extra_left": [],
        "left": "def rep1(parser) -> ",
        "right": ":\n    \"\"\"Match a parser one or more times repeatedly.\n\n    This matches ``parser`` multiple times in a row. If it matches as least\n    once, it returns a list of values from each time ``parser`` matched. If it\n    does not match ``parser`` at all, it fails.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return RepeatedOnceParser(parser)",
        "return_type_from_source": "RepeatedOnceParser"
    },
    {
        "extra_left": [],
        "left": "def rep(parser) -> ",
        "right": ":\n    \"\"\"Match a parser zero or more times repeatedly.\n\n    This matches ``parser`` multiple times in a row. A list is returned\n    containing the value from each match. If there are no matches, an empty list\n    is returned.\n\n    Args:\n        parser: Parser or literal\n    \"\"\"\n    if isinstance(parser, str):\n        parser = lit(parser)\n    return RepeatedParser(parser)",
        "return_type_from_source": "RepeatedParser"
    },
    {
        "extra_left": [],
        "left": "def rep1sep(parser, separator) \\ -> ",
        "right": ":\n    \"\"\"Match a parser one or more times separated by another parser.\n\n    This matches repeated sequences of ``parser`` separated by ``separator``.\n    If there is at least one match, a list containing the values of the\n    ``parser`` matches is returned. The values from ``separator`` are discarded.\n    If it does not match ``parser`` at all, it fails.\n\n    Args:\n        parser: Parser or literal\n        separator: Parser or literal\n    \"\"\"\n    if isinstance(parser, str):\n        parser = lit(parser)\n    if isinstance(separator, str):\n        separator = lit(separator)\n    return RepeatedOnceSeparatedParser(parser, separator)",
        "return_type_from_source": "RepeatedOnceSeparatedParser"
    },
    {
        "extra_left": [],
        "left": "def repsep(parser, separator) \\ -> ",
        "right": ":\n    \"\"\"Match a parser zero or more times separated by another parser.\n\n    This matches repeated sequences of ``parser`` separated by ``separator``. A\n    list is returned containing the value from each match of ``parser``. The\n    values from ``separator`` are discarded. If there are no matches, an empty\n    list is returned.\n\n    Args:\n        parser: Parser or literal\n        separator: Parser or literal\n    \"\"\"\n    if isinstance(parser, str):\n        parser = lit(parser)\n    if isinstance(separator, str):\n        separator = lit(separator)\n    return RepeatedSeparatedParser(parser, separator)",
        "return_type_from_source": "RepeatedSeparatedParser"
    },
    {
        "extra_left": [],
        "left": "def chain_check(cls, timestamp) -> ",
        "right": ":\n        \"\"\"\n        Given a record timestamp, verify the chain integrity.\n\n        :param timestamp: UNIX time / POSIX time / Epoch time\n        :return: 'True' if the timestamp fits the chain. 'False' otherwise.\n        \"\"\"\n\n        # Creation is messy.\n        # You want genius, you get madness; two sides of the same coin.\n        # ... I'm sure this can be cleaned up. However, let's test it first.\n\n        record = cls.get_record(timestamp)\n\n        if isinstance(record, NistBeaconValue) is False:\n            # Don't you dare try to play me\n            return False\n\n        prev_record = cls.get_previous(record.timestamp)\n        next_record = cls.get_next(record.timestamp)\n\n        if prev_record is None and next_record is None:\n            # Uh, how did you manage to do this?\n            # I'm not even mad, that's amazing.\n            return False\n\n        if (\n                isinstance(prev_record, NistBeaconValue) and\n                isinstance(next_record, NistBeaconValue)\n        ):\n            # Majority case, somewhere in the middle of the chain\n            # True if:\n            #   - All three records have proper signatures\n            #   - The requested record's previous output equals previous\n            #   - The next possible record's previous output equals the record\n            return (\n                record.valid_signature and\n                prev_record.valid_signature and\n                next_record.valid_signature and\n                record.previous_output_value == prev_record.output_value and\n                next_record.previous_output_value == record.output_value\n            )\n\n        if (\n                prev_record is None and\n                isinstance(next_record, NistBeaconValue)\n        ):\n            # Edge case, this was potentially the first record of all time\n            return (\n                record.valid_signature and\n                next_record.valid_signature and\n                cls._INIT_RECORD == record and\n                next_record.previous_output_value == record.output_value\n            )\n\n        if (\n                isinstance(prev_record, NistBeaconValue) and\n                next_record is None\n        ):\n            # Edge case, this was potentially the latest and greatest\n            return (\n                record.valid_signature and\n                prev_record.valid_signature and\n                record.previous_output_value == prev_record.output_value\n            )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def from_json(cls, input_json) -> ",
        "right": ":\n        \"\"\"\n        Convert a string of JSON which represents a NIST randomness beacon\n        value into a 'NistBeaconValue' object.\n\n        :param input_json: JSON to build a 'Nist RandomnessBeaconValue' from\n        :return: A 'NistBeaconValue' object, 'None' otherwise\n        \"\"\"\n\n        try:\n            data_dict = json.loads(input_json)\n        except ValueError:\n            return None\n\n        # Our required values are \"must haves\". This makes it simple\n        # to verify we loaded everything out of JSON correctly.\n        required_values = {\n            cls._KEY_FREQUENCY: None,\n            cls._KEY_OUTPUT_VALUE: None,\n            cls._KEY_PREVIOUS_OUTPUT_VALUE: None,\n            cls._KEY_SEED_VALUE: None,\n            cls._KEY_SIGNATURE_VALUE: None,\n            cls._KEY_STATUS_CODE: None,\n            cls._KEY_TIMESTAMP: None,\n            cls._KEY_VERSION: None,\n        }\n\n        for key in required_values:\n            if key in data_dict:\n                required_values[key] = data_dict[key]\n\n        # Confirm that the required values are set, and not 'None'\n        if None in required_values.values():\n            return None\n\n        # We have all the required values, return a node object\n        return cls(\n            version=required_values[cls._KEY_VERSION],\n            frequency=int(required_values[cls._KEY_FREQUENCY]),\n            timestamp=int(required_values[cls._KEY_TIMESTAMP]),\n            seed_value=required_values[cls._KEY_SEED_VALUE],\n            previous_output_value=required_values[\n                cls._KEY_PREVIOUS_OUTPUT_VALUE\n            ],\n            signature_value=required_values[cls._KEY_SIGNATURE_VALUE],\n            output_value=required_values[cls._KEY_OUTPUT_VALUE],\n            status_code=required_values[cls._KEY_STATUS_CODE],\n        )",
        "return_type_from_source": "'NistBeaconValue'"
    },
    {
        "extra_left": [],
        "left": "def from_xml(cls, input_xml) -> ",
        "right": ":\n        \"\"\"\n        Convert a string of XML which represents a NIST Randomness Beacon value\n        into a 'NistBeaconValue' object.\n\n        :param input_xml: XML to build a 'NistBeaconValue' from\n        :return: A 'NistBeaconValue' object, 'None' otherwise\n        \"\"\"\n\n        invalid_result = None\n\n        understood_namespaces = {\n            'nist-0.1': 'http://beacon.nist.gov/record/0.1/',\n        }\n\n        # Our required values are \"must haves\". This makes it simple\n        # to verify we loaded everything out of XML correctly.\n        required_values = {\n            cls._KEY_FREQUENCY: None,\n            cls._KEY_OUTPUT_VALUE: None,\n            cls._KEY_PREVIOUS_OUTPUT_VALUE: None,\n            cls._KEY_SEED_VALUE: None,\n            cls._KEY_SIGNATURE_VALUE: None,\n            cls._KEY_STATUS_CODE: None,\n            cls._KEY_TIMESTAMP: None,\n            cls._KEY_VERSION: None,\n        }\n\n        # First attempt to load the xml, return 'None' on ParseError\n        try:\n            tree = ElementTree.ElementTree(ElementTree.fromstring(input_xml))\n        except ElementTree.ParseError:\n            return invalid_result\n\n        # Using the required values, let's load the xml values in\n        for key in required_values:\n            discovered_element = tree.find(\n                \"{0}:{1}\".format('nist-0.1', key),\n                namespaces=understood_namespaces,\n            )\n\n            if not isinstance(discovered_element, ElementTree.Element):\n                continue\n\n            # Bad pylint message - https://github.com/PyCQA/pylint/issues/476\n            # pylint: disable=no-member\n            required_values[key] = discovered_element.text\n\n        # Confirm that the required values are set, and not 'None'\n        if None in required_values.values():\n            return invalid_result\n\n        # We have all the required values, return a node object\n        return cls(\n            version=required_values[cls._KEY_VERSION],\n            frequency=int(required_values[cls._KEY_FREQUENCY]),\n            timestamp=int(required_values[cls._KEY_TIMESTAMP]),\n            seed_value=required_values[cls._KEY_SEED_VALUE],\n            previous_output_value=required_values[\n                cls._KEY_PREVIOUS_OUTPUT_VALUE\n            ],\n            signature_value=required_values[cls._KEY_SIGNATURE_VALUE],\n            output_value=required_values[cls._KEY_OUTPUT_VALUE],\n            status_code=required_values[cls._KEY_STATUS_CODE],\n        )",
        "return_type_from_source": "'NistBeaconValue'"
    },
    {
        "extra_left": [],
        "left": "def get_hash(\n            cls,\n            version,\n            frequency,\n            timestamp,\n            seed_value,\n            prev_output,\n            status_code,\n    ) -> ",
        "right": ":\n        \"\"\"\n        Given required properties from a NistBeaconValue,\n        compute the SHA512Hash object.\n\n        :param version: NistBeaconValue.version\n        :param frequency: NistBeaconValue.frequency\n        :param timestamp: NistBeaconValue.timestamp\n        :param seed_value: NistBeaconValue.seed_value\n        :param prev_output: NistBeaconValue.previous_output_value\n        :param status_code: NistBeaconValue.status_code\n\n        :return: SHA512 Hash for NistBeaconValue signature verification\n        \"\"\"\n        return SHA512.new(\n            version.encode() +\n            struct.pack(\n                '>1I1Q64s64s1I',\n                frequency,\n                timestamp,\n                binascii.a2b_hex(seed_value),\n                binascii.a2b_hex(prev_output),\n                int(status_code),\n            )\n        )",
        "return_type_from_source": "SHA512Hash"
    },
    {
        "extra_left": [],
        "left": "def verify(\n            cls,\n            timestamp,\n            message_hash,\n            signature,\n    ) -> ",
        "right": ":\n        \"\"\"\n        Verify a given NIST message hash and signature for a beacon value.\n\n        :param timestamp: The timestamp of the record being verified.\n        :param message_hash:\n            The hash that was carried out over the message.\n            This is an object belonging to the `Crypto.Hash` module.\n        :param signature: The signature that needs to be validated.\n        :return: True if verification is correct. False otherwise.\n        \"\"\"\n\n        # Determine verifier type to use based on timestamp.\n        if timestamp < 1496176860:\n            verifier = cls._VERIFIER_20130905\n        elif timestamp < 1502202360:\n            verifier = None\n        else:\n            verifier = cls._VERIFIER_20170808\n\n        # If a verifier exists to handle this problem, use it directly.\n        # Else, we cannot verify the record and must mark it invalid.\n        if verifier:\n            result = verifier.verify(\n                message_hash,\n                signature,\n            )\n        else:\n            result = False\n\n        # Convert 1 to 'True', 'False' otherwise\n        if isinstance(result, int):\n            result = True if result == 1 else False\n\n        return result",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_version(self) -> ",
        "right": ":\n        \"\"\"\n        Open the file referenced in this object, and scrape the version.\n\n        :return:\n            The version as a string, an empty string if there is no match\n            to the magic_line, or any file exception messages encountered.\n        \"\"\"\n\n        try:\n            f = open(self.file_path, 'r')\n            lines = f.readlines()\n            f.close()\n        except Exception as e:\n            return str(e)\n\n        result = ''\n\n        for line in lines:\n            if self.magic_line in line:\n                start = len(self.magic_line)\n                end = len(line) - self.strip_end_chars\n                result = line[start:end]\n                break\n\n        return result",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def build_payment_parameters(amount, client_ref) -> ",
        "right": ":\n    \"\"\"\n    Builds the parameters needed to present the user with a datatrans payment form.\n\n    :param amount: The amount and currency we want the user to pay\n    :param client_ref: A unique reference for this payment\n    :return: The parameters needed to display the datatrans form\n    \"\"\"\n    merchant_id = web_merchant_id\n    amount, currency = money_to_amount_and_currency(amount)\n    refno = client_ref\n    sign = sign_web(merchant_id, amount, currency, refno)\n\n    parameters = PaymentParameters(\n        merchant_id=merchant_id,\n        amount=amount,\n        currency=currency,\n        refno=refno,\n        sign=sign,\n        use_alias=False,\n    )\n\n    logger.info('build-payment-parameters', parameters=parameters)\n\n    return parameters",
        "return_type_from_source": "PaymentParameters"
    },
    {
        "extra_left": [],
        "left": "def build_register_credit_card_parameters(client_ref) -> ",
        "right": ":\n    \"\"\"\n    Builds the parameters needed to present the user with a datatrans form to register a credit card.\n    Contrary to a payment form, datatrans will not show an amount.\n\n    :param client_ref: A unique reference for this alias capture.\n    :return: The parameters needed to display the datatrans form\n    \"\"\"\n\n    amount = 0\n    currency = 'CHF'  # Datatrans requires this value to be filled, so we use this arbitrary currency.\n    merchant_id = web_merchant_id\n    refno = client_ref\n    sign = sign_web(merchant_id, amount, currency, refno)\n\n    parameters = PaymentParameters(\n        merchant_id=merchant_id,\n        amount=amount,\n        currency=currency,\n        refno=refno,\n        sign=sign,\n        use_alias=True,\n    )\n\n    logger.info('building-payment-parameters', parameters=parameters)\n\n    return parameters",
        "return_type_from_source": "PaymentParameters"
    },
    {
        "extra_left": [],
        "left": "def pay_with_alias(amount, alias_registration_id, client_ref) -> ",
        "right": ":\n    \"\"\"\n    Charges money using datatrans, given a previously registered credit card alias.\n\n    :param amount: The amount and currency we want to charge\n    :param alias_registration_id: The alias registration to use\n    :param client_ref: A unique reference for this charge\n    :return: a Payment (either successful or not)\n    \"\"\"\n    if amount.amount <= 0:\n        raise ValueError('Pay with alias takes a strictly positive amount')\n\n    alias_registration = AliasRegistration.objects.get(pk=alias_registration_id)\n\n    logger.info('paying-with-alias', amount=amount, client_ref=client_ref,\n                alias_registration=alias_registration)\n\n    request_xml = build_pay_with_alias_request_xml(amount, client_ref, alias_registration)\n\n    logger.info('sending-pay-with-alias-request', url=datatrans_authorize_url, data=request_xml)\n\n    response = requests.post(\n        url=datatrans_authorize_url,\n        headers={'Content-Type': 'application/xml'},\n        data=request_xml)\n\n    logger.info('processing-pay-with-alias-response', response=response.content)\n\n    charge_response = parse_pay_with_alias_response_xml(response.content)\n    charge_response.save()\n    charge_response.send_signal()\n\n    return charge_response",
        "return_type_from_source": "Payment"
    },
    {
        "extra_left": [],
        "left": "def nvim_io_recover(self, io) -> ",
        "right": ":\n        '''calls `map` to shift the recover execution to flat_map_nvim_io\n        '''\n        return eval_step(self.vim)(io.map(lambda a: a))",
        "return_type_from_source": "NvimIO[B]"
    },
    {
        "extra_left": [],
        "left": "def exclusive_ns(guard, desc, thunk, *a: Any) -> ",
        "right": ":\n    '''this is the central unsafe function, using a lock and updating the state in `guard` in-place.\n    '''\n    yield guard.acquire()\n    log.debug2(lambda: f'exclusive: {desc}')\n    state, response = yield N.ensure_failure(thunk(*a).run(guard.state), guard.release)\n    yield N.delay(lambda v: unsafe_update_state(guard, state))\n    yield guard.release()\n    log.debug2(lambda: f'release: {desc}')\n    yield N.pure(response)",
        "return_type_from_source": "Do"
    },
    {
        "extra_left": [],
        "left": "def position(self) -> ",
        "right": ":\n        \"\"\"The current position of the cursor.\"\"\"\n        return Position(self._index, self._lineno, self._col_offset)",
        "return_type_from_source": "Position"
    },
    {
        "extra_left": [],
        "left": "def max_readed_position(self) -> ",
        "right": ":\n        \"\"\"The index of the deepest character readed.\"\"\"\n        return Position(self._maxindex, self._maxline, self._maxcol)",
        "return_type_from_source": "Position"
    },
    {
        "extra_left": [],
        "left": "def last_readed_line(self) -> ",
        "right": ":\n        \"\"\"Usefull string to compute error message.\"\"\"\n        mpos = self._cursor.max_readed_position\n        mindex = mpos.index\n        # search last \\n\n        prevline = mindex - 1 if mindex == self.eos_index else mindex\n        while prevline >= 0 and self._content[prevline] != '\\n':\n            prevline -= 1\n        # search next \\n\n        nextline = mindex\n        while nextline < self.eos_index and self._content[nextline] != '\\n':\n            nextline += 1\n        last_line = self._content[prevline + 1:nextline]\n        return last_line",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def incpos(self, length=1) -> ",
        "right": ":\n        \"\"\"Increment the cursor to the next character.\"\"\"\n        if length < 0:\n            raise ValueError(\"length must be positive\")\n        i = 0\n        while (i < length):\n            if self._cursor.index < self._len:\n                if self.peek_char == '\\n':\n                    self._cursor.step_next_line()\n                self._cursor.step_next_char()\n            i += 1\n        return self._cursor.index",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def save_context(self) -> ",
        "right": ":\n        \"\"\"Save current position.\"\"\"\n        self._contexts.append(self._cursor.position)\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def restore_context(self) -> ",
        "right": ":\n        \"\"\"Rollback to previous saved position.\"\"\"\n        self._cursor.position = self._contexts.pop()\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def to_fmt(self, with_from=False) -> ",
        "right": ":\n    \"\"\"\n    Return a Fmt representation of Translator for pretty-printing\n    \"\"\"\n    txt = fmt.sep(\"\\n\", [\n        fmt.sep(\n            \" \",\n            [\n                self._type_source,\n                \"to\",\n                self._type_target,\n                '=',\n                self._fun.to_fmt()\n            ]\n        ),\n        self._notify.get_content(with_from)\n    ])\n    return txt",
        "return_type_from_source": "fmt.indentable"
    },
    {
        "extra_left": [],
        "left": "def count_vars(self) -> ",
        "right": ":\n        \"\"\" Count var define by this scope \"\"\"\n        n = 0\n        for s in self._hsig.values():\n            if hasattr(s, 'is_var') and s.is_var:\n                n += 1\n        return n",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def count_funs(self) -> ",
        "right": ":\n        \"\"\" Count function define by this scope \"\"\"\n        n = 0\n        for s in self._hsig.values():\n            if hasattr(s, 'is_fun') and s.is_fun:\n                n += 1\n        return n",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def update(self, sig) -> ",
        "right": ":\n        \"\"\" Update the Set with values of another Set \"\"\"\n        values = sig\n        if hasattr(sig, 'values'):\n            values = sig.values()\n        for s in values:\n            if self.is_namespace:\n                s.set_parent(self)\n            if isinstance(s, Scope):\n                s.state = StateScope.EMBEDDED\n            self._hsig[s.internal_name()] = s\n        self.__update_count()\n        return self",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def union(self, sig) -> ",
        "right": ":\n        \"\"\" Create a new Set produce by the union of 2 Set \"\"\"\n        new = Scope(sig=self._hsig.values(), state=self.state)\n        new |= sig\n        return new",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def intersection_update(self, oset) -> ",
        "right": ":\n        \"\"\" Update Set with common values of another Set \"\"\"\n        keys = list(self._hsig.keys())\n        for k in keys:\n            if k not in oset:\n                del self._hsig[k]\n            else:\n                self._hsig[k] = oset.get(k)\n        return self",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def intersection(self, sig) -> ",
        "right": ":\n        \"\"\" Create a new Set produce by the intersection of 2 Set \"\"\"\n        new = Scope(sig=self._hsig.values(), state=self.state)\n        new &= sig\n        return new",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def difference_update(self, oset) -> ",
        "right": ":\n        \"\"\" Remove values common with another Set \"\"\"\n        keys = list(self._hsig.keys())\n        for k in keys:\n            if k in oset:\n                del self._hsig[k]\n        return self",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def difference(self, sig) -> ",
        "right": ":\n        \"\"\" Create a new Set produce by a Set subtracted by another Set \"\"\"\n        new = Scope(sig=self._hsig.values(), state=self.state)\n        new -= sig\n        return new",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def symmetric_difference_update(self, oset) -> ",
        "right": ":\n        \"\"\" Remove common values\n            and Update specific values from another Set\n        \"\"\"\n        skey = set()\n        keys = list(self._hsig.keys())\n        for k in keys:\n            if k in oset:\n                skey.add(k)\n        for k in oset._hsig.keys():\n            if k not in skey:\n                self._hsig[k] = oset.get(k)\n        for k in skey:\n            del self._hsig[k]\n        return self",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def symmetric_difference(self, sig) -> ",
        "right": ":\n        \"\"\" Create a new Set with values present in only one Set \"\"\"\n        new = Scope(sig=self._hsig.values(), state=self.state)\n        new ^= sig\n        return new",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def add(self, it) -> ",
        "right": ":\n        \"\"\" Add it to the Set \"\"\"\n        if isinstance(it, Scope):\n            it.state = StateScope.EMBEDDED\n        txt = it.internal_name()\n        it.set_parent(self)\n        if self.is_namespace:\n            txt = it.internal_name()\n        if txt == \"\":\n            txt = '_' + str(len(self._hsig))\n        if txt in self._hsig:\n            raise KeyError(\"Already exists %s\" % txt)\n        self._hsig[txt] = it\n        self.__update_count()\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def remove(self, it) -> ",
        "right": ":\n        \"\"\" Remove it but raise KeyError if not found \"\"\"\n        txt = it.internal_name()\n        if txt not in self._hsig:\n            raise KeyError(it.show_name() + ' not in Set')\n        sig = self._hsig[txt]\n        if isinstance(sig, Scope):\n            sig.state = StateScope.LINKED\n        del self._hsig[txt]\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def discard(self, it) -> ",
        "right": ":\n        \"\"\" Remove it only if present \"\"\"\n        txt = it.internal_name()\n        if txt in self._hsig:\n            sig = self._hsig[txt]\n            if isinstance(sig, Scope):\n                sig.state = StateScope.LINKED\n            del self._hsig[txt]\n            return True\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def values(self) -> ",
        "right": ":\n        \"\"\" Retrieve all values \"\"\"\n        if self.state == StateScope.EMBEDDED and self.parent is not None:\n            return list(self._hsig.values()) + list(self.parent().values())\n        else:\n            return self._hsig.values()",
        "return_type_from_source": "[Signature]"
    },
    {
        "extra_left": [],
        "left": "def first(self) -> ",
        "right": ":\n        \"\"\" Retrieve the first Signature ordered by mangling descendant \"\"\"\n        k = sorted(self._hsig.keys())\n        return self._hsig[k[0]]",
        "return_type_from_source": "Signature"
    },
    {
        "extra_left": [],
        "left": "def last(self) -> ",
        "right": ":\n        \"\"\" Retrieve the last Signature ordered by mangling descendant \"\"\"\n        k = sorted(self._hsig.keys())\n        return self._hsig[k[-1]]",
        "return_type_from_source": "Signature"
    },
    {
        "extra_left": [],
        "left": "def get(self, key, default=None) -> ",
        "right": ":\n        \"\"\" Get a signature instance by its internal_name \"\"\"\n        item = default\n        if key in self._hsig:\n            item = self._hsig[key]\n        return item",
        "return_type_from_source": "Signature"
    },
    {
        "extra_left": [],
        "left": "def get_by_symbol_name(self, name) -> ",
        "right": ":\n        \"\"\" Retrieve a Set of all signature by symbol name \"\"\"\n        lst = []\n        for s in self.values():\n            if s.name == name:\n                # create an EvalCtx only when necessary\n                lst.append(EvalCtx.from_sig(s))\n        # include parent\n        # TODO: see all case of local redefinition for\n        #       global overloads\n        # possible algos... take all with different internal_name\n        if len(lst) == 0:\n            p = self.get_parent()\n            if p is not None:\n                return p.get_by_symbol_name(name)\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope",
        "return_type_from_source": "Scope"
    },
    {
        "extra_left": [],
        "left": "def getsig_by_symbol_name(self, name) -> ",
        "right": ":\n        \"\"\" Retrieve the unique Signature of a symbol.\n        Fail if the Signature is not unique\n        \"\"\"\n        subscope = self.get_by_symbol_name(name)\n        if len(subscope) != 1:\n            raise KeyError(\"%s have multiple candidates in scope\" % name)\n        v = list(subscope.values())\n        return v[0]",
        "return_type_from_source": "Signature"
    },
    {
        "extra_left": [],
        "left": "def get_all_polymorphic_return(self) -> ",
        "right": ":\n        \"\"\" For now, polymorphic return type are handle by symbol artefact.\n\n        - -> possible multi-polymorphic but with different constraint attached!\n        \"\"\"\n        lst = []\n        for s in self.values():\n            if hasattr(s, 'tret') and s.tret.is_polymorphic:\n                # encapsulate s into a EvalCtx for meta-var resolution\n                lst.append(EvalCtx.from_sig(s))\n        rscope = Scope(sig=lst, state=StateScope.LINKED, is_namespace=False)\n        # inherit type/translation from parent\n        rscope.set_parent(self)\n        return rscope",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def callInjector(self, old, trans) -> ",
        "right": ":\n        \"\"\" If don't have injector call from parent \"\"\"\n        if self.astTranslatorInjector is None:\n            if self.parent is not None:\n                # TODO: think if we forward for all StateScope\n                # forward to parent scope\n                return self.parent().callInjector(old, trans)\n            else:\n                raise TypeError(\"Must define an Translator Injector\")\n        return self.astTranslatorInjector(old, trans)",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def normalize(ast) -> ",
        "right": ":\n    \"\"\"\n    Normalize an AST nodes.\n\n    all builtins containers are replace by referencable subclasses\n    \"\"\"\n    res = ast\n    typemap = {DictNode, ListNode, TupleNode}\n    if type(ast) is dict:\n        res = DictNode(ast)\n    elif type(ast) is list:\n        res = ListNode(ast)\n    elif type(ast) is tuple:\n        res = TupleNode(ast)\n    # in-depth change\n    if hasattr(res, 'items'):\n        for k, v in res.items():\n            res[k] = normalize(v)\n    elif hasattr(res, '__getitem__'):\n        for idx, v in zip(range(len(res)), res):\n            res[idx] = normalize(v)\n    if type(res) not in typemap and hasattr(res, '__dict__'):\n        subattr = vars(res)\n        for k, v in subattr.items():\n            setattr(res, k, normalize(v))\n    return res",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def get_compute_sig(self) -> ",
        "right": ":\n        \"\"\"\n        Compute a signature Using resolution!!!\n\n        TODO: discuss of relevance of a final generation for a signature\n        \"\"\"\n        tret = []\n        tparams = []\n        for t in self.tret.components:\n            if t in self.resolution and self.resolution[t] is not None:\n                tret.append(self.resolution[t]().show_name())\n            else:\n                tret.append(t)\n        if hasattr(self, 'tparams'):\n            for p in self.tparams:\n                tp = []\n                for t in p.components:\n                    if t in self.resolution and self.resolution[t] is not None:\n                        tp.append(self.resolution[t]().show_name())\n                    else:\n                        tp.append(t)\n                tparams.append(\" \".join(tp))\n            if self.variadic:\n                if self._variadic_types is None:\n                    raise ValueError(\"Can't compute the sig \"\n                                     + \"with unresolved variadic argument\"\n                                     )\n                for p in self._variadic_types:\n                    tp = []\n                    for t in p.components:\n                        if (t in self.resolution\n                            and self.resolution[t] is not None\n                        ):\n                            tp.append(self.resolution[t]().show_name())\n                        else:\n                            tp.append(t)\n                    tparams.append(\" \".join(tp))\n        ret = Fun(self.name, \" \".join(tret), tparams)\n        # transform as-is into our internal Signature (Val, Var, whatever)\n        ret.__class__ = self._sig.__class__\n        return ret",
        "return_type_from_source": "Signature"
    },
    {
        "extra_left": [],
        "left": "def get_resolved_names(self, type_name) -> ",
        "right": ":\n        \"\"\"\n        Use self.resolution to subsitute type_name.\n        Allow to instanciate polymorphic type ?1, ?toto\n        \"\"\"\n        if not isinstance(type_name, TypeName):\n            raise Exception(\"Take a TypeName as parameter not a %s\"\n                            % type(type_name))\n        rnames = []\n        for name in type_name.components:\n            if name not in self.resolution:\n                raise Exception(\"Unknown type %s in a EvalCtx\" % name)\n            rname = self.resolution[name]\n            if rname is not None:\n                rname = rname().show_name()\n            else:\n                rname = name\n            rnames.append(rname)\n        return rnames",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def bind(self, dst, src) -> ",
        "right": ":\n    \"\"\"Allow to alias a node to another name.\n\n    Useful to bind a node to _ as return of Rule::\n\n        R = [\n            __scope__:L [item:I #add_item(L, I]* #bind('_', L)\n        ]\n\n    It's also the default behaviour of ':>'\n\n    \"\"\"\n\n    for m in self.rule_nodes.maps:\n        for k, v in m.items():\n            if k == dst:\n                m[k] = src\n                return True\n    raise Exception('%s not found' % dst)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def read_eol(self) -> ",
        "right": ":\n    \"\"\"Return True if the parser can consume an EOL byte sequence.\"\"\"\n    if self.read_eof():\n        return False\n    self._stream.save_context()\n    self.read_char('\\r')\n    if self.read_char('\\n'):\n        return self._stream.validate_context()\n    return self._stream.restore_context()",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def push_rule_nodes(self) -> ",
        "right": ":\n        \"\"\"Push context variable to store rule nodes.\"\"\"\n        if self.rule_nodes is None:\n            self.rule_nodes = collections.ChainMap()\n            self.tag_cache = collections.ChainMap()\n            self.id_cache = collections.ChainMap()\n        else:\n            self.rule_nodes = self.rule_nodes.new_child()\n            self.tag_cache = self.tag_cache.new_child()\n            self.id_cache = self.id_cache.new_child()\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def pop_rule_nodes(self) -> ",
        "right": ":\n        \"\"\"Pop context variable that store rule nodes\"\"\"\n        self.rule_nodes = self.rule_nodes.parents\n        self.tag_cache = self.tag_cache.parents\n        self.id_cache = self.id_cache.parents\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def value(self, n) -> ",
        "right": ":\n        \"\"\"Return the text value of the node\"\"\"\n        id_n = id(n)\n        idcache = self.id_cache\n        if id_n not in idcache:\n            return \"\"\n        name = idcache[id_n]\n        tag_cache = self.tag_cache\n        if name not in tag_cache:\n            raise Exception(\"Incoherent tag cache\")\n        tag = tag_cache[name]\n        k = \"%d:%d\" % (tag._begin, tag._end)\n        valcache = self._streams[-1].value_cache\n        if k not in valcache:\n            valcache[k] = str(tag)\n        return valcache[k]",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def begin_tag(self, name) -> ",
        "right": ":\n        \"\"\"Save the current index under the given name.\"\"\"\n        # Check if we could attach tag cache to current rule_nodes scope\n        self.tag_cache[name] = Tag(self._stream, self._stream.index)\n        return True",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def end_tag(self, name) -> ",
        "right": ":\n        \"\"\"Extract the string between saved and current index.\"\"\"\n        self.tag_cache[name].set_end(self._stream.index)\n        return True",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def set_rules(cls, rules) -> ",
        "right": ":\n        \"\"\"\n        Merge internal rules set with the given rules\n        \"\"\"\n        cls._rules = cls._rules.new_child()\n        for rule_name, rule_pt in rules.items():\n            if '.' not in rule_name:\n                rule_name = cls.__module__ \\\n                    + '.' + cls.__name__ \\\n                    + '.' + rule_name\n            meta.set_one(cls._rules, rule_name, rule_pt)\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def set_hooks(cls, hooks) -> ",
        "right": ":\n        \"\"\"\n        Merge internal hooks set with the given hooks\n        \"\"\"\n        cls._hooks = cls._hooks.new_child()\n        for hook_name, hook_pt in hooks.items():\n            if '.' not in hook_name:\n                hook_name = cls.__module__ \\\n                    + '.' + cls.__name__ \\\n                    + '.' + hook_name\n            meta.set_one(cls._hooks, hook_name, hook_pt)\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def set_directives(cls, directives) -> ",
        "right": ":\n        \"\"\"\n        Merge internal directives set with the given directives.\n        For working directives, attach it only in the dsl.Parser class\n        \"\"\"\n        meta._directives = meta._directives.new_child()\n        for dir_name, dir_pt in directives.items():\n            meta.set_one(meta._directives, dir_name, dir_pt)\n            dir_pt.ns_name = dir_name\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def eval_rule(self, name) -> ",
        "right": ":\n        \"\"\"Evaluate a rule by name.\"\"\"\n        # context created by caller\n        n = Node()\n        id_n = id(n)\n        self.rule_nodes['_'] = n\n        self.id_cache[id_n] = '_'\n        # TODO: other behavior for  empty rules?\n        if name not in self.__class__._rules:\n            self.diagnostic.notify(\n                error.Severity.ERROR,\n                \"Unknown rule : %s\" % name,\n                error.LocationInfo.from_stream(self._stream, is_error=True)\n            )\n            raise self.diagnostic\n        self._lastRule = name\n        rule_to_eval = self.__class__._rules[name]\n        # TODO: add packrat cache here, same rule - same pos == same res\n        res = rule_to_eval(self)\n        if res:\n            res = self.rule_nodes['_']\n        return res",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def eval_hook(self, name, ctx) -> ",
        "right": ":\n        \"\"\"Evaluate the hook by its name\"\"\"\n        if name not in self.__class__._hooks:\n            # TODO: don't always throw error, could have return True by default\n            self.diagnostic.notify(\n                error.Severity.ERROR,\n                \"Unknown hook : %s\" % name,\n                error.LocationInfo.from_stream(self._stream, is_error=True)\n            )\n            raise self.diagnostic\n        self._lastRule = '#' + name\n        res = self.__class__._hooks[name](self, *ctx)\n        if type(res) is not bool:\n            raise TypeError(\"Your hook %r didn't return a bool value\" % name)\n        return res",
        "return_type_from_source": "Node"
    },
    {
        "extra_left": [],
        "left": "def peek_text(self, text) -> ",
        "right": ":\n        \"\"\"Same as readText but doesn't consume the stream.\"\"\"\n        start = self._stream.index\n        stop = start + len(text)\n        if stop > self._stream.eos_index:\n            return False\n        return self._stream[self._stream.index:stop] == text",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def one_char(self) -> ",
        "right": ":\n        \"\"\"Read one byte in stream\"\"\"\n        if self.read_eof():\n            return False\n        self._stream.incpos()\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def read_char(self, c) -> ",
        "right": ":\n        \"\"\"\n        Consume the c head byte, increment current index and return True\n        else return False. It use peekchar and it's the same as '' in BNF.\n        \"\"\"\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if c == self._stream.peek_char:\n            self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.restore_context()",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def read_until_eof(self) -> ",
        "right": ":\n        \"\"\"Consume all the stream. Same as EOF in BNF.\"\"\"\n        if self.read_eof():\n            return True\n        # TODO: read ALL\n        self._stream.save_context()\n        while not self.read_eof():\n            self._stream.incpos()\n        return self._stream.validate_context()",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def ignore_blanks(self) -> ",
        "right": ":\n        \"\"\"Consume whitespace characters.\"\"\"\n        self._stream.save_context()\n        if not self.read_eof() and self._stream.peek_char in \" \\t\\v\\f\\r\\n\":\n            while (not self.read_eof()\n                   and self._stream.peek_char in \" \\t\\v\\f\\r\\n\"):\n                self._stream.incpos()\n            return self._stream.validate_context()\n        return self._stream.validate_context()",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def add_ruleclause_name(self, ns_name, rid) -> ",
        "right": ":\n    \"\"\"Create a tree.Rule\"\"\"\n    ns_name.parser_tree = parsing.Rule(self.value(rid))\n    return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def add_rules(self, bnf, r) -> ",
        "right": ":\n    \"\"\"Attach a parser tree to the dict of rules\"\"\"\n    bnf[r.rulename] = r.parser_tree\n    return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def add_rule(self, rule, rn, alts) -> ",
        "right": ":\n    \"\"\"Add the rule name\"\"\"\n    rule.rulename = self.value(rn)\n    rule.parser_tree = alts.parser_tree\n    return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def add_sequences(self, sequences, cla) -> ",
        "right": ":\n    \"\"\"Create a tree.Seq\"\"\"\n    if not hasattr(sequences, 'parser_tree'):\n        # forward sublevel of sequence as is\n        sequences.parser_tree = cla.parser_tree\n    else:\n        oldnode = sequences\n        if isinstance(oldnode.parser_tree, parsing.Seq):\n            oldpt = list(oldnode.parser_tree.ptlist)\n        else:\n            oldpt = [oldnode.parser_tree]\n        oldpt.append(cla.parser_tree)\n        sequences.parser_tree = parsing.Seq(*tuple(oldpt))\n    return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def add_alt(self, alternatives, alt) -> ",
        "right": ":\n    \"\"\"Create a tree.Alt\"\"\"\n    if not hasattr(alternatives, 'parser_tree'):\n        # forward sublevel of alt as is\n        if hasattr(alt, 'parser_tree'):\n            alternatives.parser_tree = alt.parser_tree\n        else:\n            alternatives.parser_tree = alt\n    else:\n        oldnode = alternatives\n        if isinstance(oldnode.parser_tree, parsing.Alt):\n            oldpt = list(oldnode.parser_tree.ptlist)\n        else:\n            oldpt = [oldnode.parser_tree]\n        oldpt.append(alt.parser_tree)\n        alternatives.parser_tree = parsing.Alt(*tuple(oldpt))\n    return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_rules(self) -> ",
        "right": ":\n        \"\"\"\n        Parse the DSL and provide a dictionnaries of all resulting rules.\n        Call by the MetaGrammar class.\n\n        TODO: could be done in the rules property of parsing.BasicParser???\n        \"\"\"\n        res = None\n        try:\n            res = self.eval_rule('bnf_dsl')\n            if not res:\n                # we fail to parse, but error is not set\n                self.diagnostic.notify(\n                    error.Severity.ERROR,\n                    \"Parse error in '%s' in EBNF bnf\" % self._lastRule,\n                    error.LocationInfo.from_maxstream(self._stream)\n                )\n                raise self.diagnostic\n        except error.Diagnostic as d:\n            d.notify(\n                error.Severity.ERROR,\n                \"Parse error in '%s' in EBNF bnf\" % self._lastRule\n            )\n            raise d\n        return res",
        "return_type_from_source": "parsing.Node"
    },
    {
        "extra_left": [],
        "left": "def ignore_cxx(self) -> ",
        "right": ":\n    \"\"\"Consume comments and whitespace characters.\"\"\"\n    self._stream.save_context()\n    while not self.read_eof():\n        idxref = self._stream.index\n        if self._stream.peek_char in \" \\t\\v\\f\\r\\n\":\n            while (not self.read_eof()\n                    and self._stream.peek_char in \" \\t\\v\\f\\r\\n\"):\n                self._stream.incpos()\n        if self.peek_text(\"//\"):\n            while not self.read_eof() and not self.peek_char(\"\\n\"):\n                self._stream.incpos()\n            if not self.read_char(\"\\n\") and self.read_eof():\n                return self._stream.validate_context()\n        if self.peek_text(\"/*\"):\n            while not self.read_eof() and not self.peek_text(\"*/\"):\n                self._stream.incpos()\n            if not self.read_text(\"*/\") and self.read_eof():\n                return self._stream.restore_context()\n        if idxref == self._stream.index:\n            break\n    return self._stream.validate_context()",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def to_dot(self) -> ",
        "right": ":\n        \"\"\"\n        Provide a '.dot' representation of all State in the register.\n        \"\"\"\n        txt = \"\"\n        txt += \"digraph S%d {\\n\" % id(self)\n        if self.label is not None:\n            txt += '\\tlabel=\"%s\";\\n' % (self.label + '\\l').replace('\\n', '\\l')\n        txt += \"\\trankdir=LR;\\n\"\n        #txt += '\\tlabelloc=\"t\";\\n'\n        txt += '\\tgraph [labeljust=l, labelloc=t, nojustify=true];\\n'\n        txt += \"\\tesep=1;\\n\"\n        txt += '\\tranksep=\"equally\";\\n'\n        txt += \"\\tnode [shape = circle];\\n\"\n        txt += \"\\tsplines = ortho;\\n\"\n        for s in self.states.values():\n            txt += s[1].to_dot()\n        txt += \"}\\n\"\n        return txt",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def to_fmt(self) -> ",
        "right": ":\n        \"\"\"\n        Provide a useful representation of the register.\n        \"\"\"\n        infos = fmt.end(\";\\n\", [])\n        s = fmt.sep(', ', [])\n        for ids in sorted(self.states.keys()):\n            s.lsdata.append(str(ids))\n        infos.lsdata.append(fmt.block('(', ')', [s]))\n        infos.lsdata.append(\"events:\" + repr(self.events))\n        infos.lsdata.append(\n            \"named_events:\" + repr(list(self.named_events.keys()))\n        )\n        infos.lsdata.append(\"uid_events:\" + repr(list(self.uid_events.keys())))\n        return infos",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def parserrule_topython(parser,\n                        rulename) -> ",
        "right": ":\n    \"\"\"Generates code for a rule.\n\n    def rulename(self):\n        <code for the rule>\n        return True\n    \"\"\"\n    visitor = RuleVisitor()\n    rule = parser._rules[rulename]\n    fn_args = ast.arguments([ast.arg('self', None)], None, None, [], None,\n                            None, [], [])\n    body = visitor._clause(rule_topython(rule))\n    body.append(ast.Return(ast.Name('True', ast.Load())))\n    return ast.FunctionDef(rulename, fn_args, body, [], None)",
        "return_type_from_source": "ast.FunctionDef"
    },
    {
        "extra_left": [],
        "left": "def __exit_scope(self) -> ",
        "right": ":\n        \"\"\"Create the appropriate scope exiting statement.\n\n        The documentation only shows one level and always uses\n        'return False' in examples.\n\n        'raise AltFalse()' within a try.\n        'break' within a loop.\n        'return False' otherwise.\n        \"\"\"\n        if self.in_optional:\n            return ast.Pass()\n        if self.in_try:\n            return ast.Raise(\n                ast.Call(ast.Name('AltFalse', ast.Load()), [], [], None, None),\n                None)\n        if self.in_loop:\n            return ast.Break()\n        return ast.Return(ast.Name('False', ast.Load()))",
        "return_type_from_source": "ast.stmt"
    },
    {
        "extra_left": [],
        "left": "def _clause(self, pt) -> ",
        "right": ":\n        \"\"\"Normalize a test expression into a statements list.\n\n        Statements list are returned as-is.\n        Expression is packaged as:\n        if not expr:\n            return False\n        \"\"\"\n        if isinstance(pt, list):\n            return pt\n        return [ast.If(ast.UnaryOp(ast.Not(), pt),\n                       [self.__exit_scope()],\n                       [])]",
        "return_type_from_source": "[ast.stmt]"
    },
    {
        "extra_left": [],
        "left": "def visit_Call(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code calling the function.\n\n        fn(*args)\n        \"\"\"\n        return ast.Call(\n            ast.Attribute(\n                ast.Name('self', ast.Load),\n                node.callObject.__name__,\n                ast.Load()),\n            [ast.Str(param) for param in node.params],\n            [],\n            None,\n            None)",
        "return_type_from_source": "ast.expr"
    },
    {
        "extra_left": [],
        "left": "def visit_CallTrue(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code calling the function and returning True.\n\n        lambda: fn(*args) or True\n        \"\"\"\n        return ast.Lambda(\n            ast.arguments([], None, None, [], None, None, [], []),\n            ast.BoolOp(\n                ast.Or(),\n                [\n                    self.visit_Call(node),\n                    ast.Name('True', ast.Load())]))",
        "return_type_from_source": "ast.expr"
    },
    {
        "extra_left": [],
        "left": "def visit_Hook(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code calling a hook.\n\n        self.evalHook('hookname', self.ruleNodes[-1])\n        \"\"\"\n        return ast.Call(\n            ast.Attribute(\n                ast.Name('self', ast.Load()), 'evalHook', ast.Load()),\n            [\n                ast.Str(node.name),\n                ast.Subscript(\n                    ast.Attribute(\n                        ast.Name('self', ast.Load()), 'ruleNodes', ast.Load()),\n                    ast.Index(ast.UnaryOp(ast.USub(), ast.Num(1))),\n                    ast.Load())],\n            [],\n            None,\n            None)",
        "return_type_from_source": "ast.expr"
    },
    {
        "extra_left": [],
        "left": "def visit_Rule(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code calling a rule.\n\n        self.evalRule('rulename')\n        \"\"\"\n        return ast.Call(\n            ast.Attribute(ast.Name('self', ast.Load()),\n                          'evalRule', ast.Load()),\n            [ast.Str(node.name)], [], None, None)",
        "return_type_from_source": "ast.expr"
    },
    {
        "extra_left": [],
        "left": "def visit_Alt(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code for alternatives.\n\n        try:\n            try:\n                <code for clause>  #raise AltFalse when alternative is False\n                raise AltTrue()\n            except AltFalse:\n                pass\n            return False\n        except AltTrue:\n            pass\n        \"\"\"\n        clauses = [self.visit(clause) for clause in node.ptlist]\n        for clause in clauses:\n            if not isinstance(clause, ast.expr):\n                break\n        else:\n            return ast.BoolOp(ast.Or(), clauses)\n        res = ast.Try([], [ast.ExceptHandler(\n            ast.Name('AltTrue', ast.Load()), None, [ast.Pass()])], [], [])\n        alt_true = [ast.Raise(ast.Call(\n            ast.Name('AltTrue', ast.Load()), [], [], None, None), None)]\n        alt_false = [ast.ExceptHandler(\n            ast.Name('AltFalse', ast.Load()), None, [ast.Pass()])]\n        self.in_try += 1\n        for clause in node.ptlist:\n            res.body.append(\n                ast.Try(self._clause(self.visit(clause)) + alt_true,\n                        alt_false, [], []))\n        self.in_try -= 1\n        res.body.append(self.__exit_scope())\n        return [res]",
        "return_type_from_source": "[ast.stmt]"
    },
    {
        "extra_left": [],
        "left": "def visit_Rep0N(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code for a clause repeated 0 or more times.\n\n        #If all clauses can be inlined\n        while clause:\n            pass\n\n        while True:\n            <code for the clause>\n        \"\"\"\n        cl_ast = self.visit(node.pt)\n        if isinstance(cl_ast, ast.expr):\n            return [ast.While(cl_ast, [ast.Pass()], [])]\n        self.in_loop += 1\n        clause = self._clause(self.visit(node.pt))\n        self.in_loop -= 1\n        return [ast.While(ast.Name('True', ast.Load()), clause, [])]",
        "return_type_from_source": "[ast.stmt]"
    },
    {
        "extra_left": [],
        "left": "def visit_Rep1N(self, node) -> ",
        "right": ":\n        \"\"\"Generates python code for a clause repeated 1 or more times.\n\n        <code for the clause>\n        while True:\n            <code for the clause>\n        \"\"\"\n        clause = self.visit(node.pt)\n        if isinstance(clause, ast.expr):\n            return (self._clause(clause) + self.visit_Rep0N(node))\n        self.in_loop += 1\n        clause = self._clause(self.visit(node.pt))\n        self.in_loop -= 1\n        return self._clause(self.visit(node.pt)) + [\n            ast.While(ast.Name('True', ast.Load()), clause, [])]",
        "return_type_from_source": "[ast.stmt]"
    },
    {
        "extra_left": [],
        "left": "def catend(dst, src, indent) -> ",
        "right": ":\n    \"\"\"cat two strings but handle \\n for tabulation\"\"\"\n    res = dst\n    txtsrc = src\n    if not isinstance(src, str):\n        txtsrc = str(src)\n    for c in list(txtsrc):\n        if len(res) > 0 and res[-1] == '\\n':\n            res += (indentable.char_indent * indentable.num_indent) * \\\n                   (indent - 1) + c\n        else:\n            res += c\n    return res",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def populate_state_register(all_seq, sr) -> ",
        "right": ":\n    \"\"\" function that create a state for all instance\n        of MatchExpr in the given list and connect each others.\n    \"\"\"\n    # Basic State\n    s0 = state.State(sr)\n    # loop on himself\n    s0.matchDefault(s0)\n    # this is default\n    sr.set_default_state(s0)\n    # use Edge to store connection\n    e0 = Edge(s0)\n    for seq in all_seq:\n        r = ref(e0)\n        # merge all sequences into one tree automata\n        populate_from_sequence(seq, r, sr)\n    # return edge for debug purpose\n    return e0",
        "return_type_from_source": "Edge"
    },
    {
        "extra_left": [],
        "left": "def from_string(bnf, entry=None, *optional_inherit) -> ",
        "right": ":\n    \"\"\"\n    Create a Grammar from a string\n    \"\"\"\n    inherit = [Grammar] + list(optional_inherit)\n    scope = {'grammar': bnf, 'entry': entry}\n    return build_grammar(tuple(inherit), scope)",
        "return_type_from_source": "Grammar"
    },
    {
        "extra_left": [],
        "left": "def from_file(fn, entry=None, *optional_inherit) -> ",
        "right": ":\n    \"\"\"\n    Create a Grammar from a file\n    \"\"\"\n    import os.path\n    if os.path.exists(fn):\n        f = open(fn, 'r')\n        bnf = f.read()\n        f.close()\n        inherit = [Grammar] + list(optional_inherit)\n        scope = {'grammar': bnf, 'entry': entry, 'source': fn}\n        return build_grammar(tuple(inherit), scope)\n    raise Exception(\"File not Found!\")",
        "return_type_from_source": "Grammar"
    },
    {
        "extra_left": [],
        "left": "def parse(self, source=None, entry=None) -> ",
        "right": ":\n        \"\"\"Parse source using the grammar\"\"\"\n        self.from_string = True\n        if source is not None:\n            self.parsed_stream(source)\n        if entry is None:\n            entry = self.entry\n        if entry is None:\n            raise ValueError(\"No entry rule name defined for {}\".format(\n                self.__class__.__name__))\n        return self._do_parse(entry)",
        "return_type_from_source": "parsing.Node"
    },
    {
        "extra_left": [],
        "left": "def parse_file(self, filename, entry=None) -> ",
        "right": ":\n        \"\"\"Parse filename using the grammar\"\"\"\n        self.from_string = False\n        import os.path\n        with open(filename, 'r') as f:\n            self.parsed_stream(f.read(), os.path.abspath(filename))\n        if entry is None:\n            entry = self.entry\n        if entry is None:\n            raise ValueError(\"No entry rule name defined for {}\".format(\n                self.__class__.__name__))\n        return self._do_parse(entry)",
        "return_type_from_source": "parsing.Node"
    },
    {
        "extra_left": [],
        "left": "async def fetch_bikes() -> ",
        "right": ":\n    \"\"\"\n    Gets the full list of bikes from the bikeregister site.\n    The data is hidden behind a form post request and so\n    we need to extract an xsrf and session token with bs4.\n\n    todo add pytest tests\n\n    :return: All the currently registered bikes.\n    :raise ApiError: When there was an error connecting to the API.\n    \"\"\"\n    async with ClientSession() as session:\n        try:\n            async with session.get('https://www.bikeregister.com/stolen-bikes') as request:\n                document = document_fromstring(await request.text())\n        except ClientConnectionError as con_err:\n            logger.debug(f\"Could not connect to {con_err.host}\")\n            raise ApiError(f\"Could not connect to {con_err.host}\")\n\n        token = document.xpath(\"//input[@name='_token']\")\n        if len(token) != 1:\n            raise ApiError(f\"Couldn't extract token from page.\")\n        else:\n            token = token[0].value\n        xsrf_token = request.cookies[\"XSRF-TOKEN\"]\n        laravel_session = request.cookies[\"laravel_session\"]\n\n        # get the bike data\n        headers = {\n            'cookie': f'XSRF-TOKEN={xsrf_token}; laravel_session={laravel_session}',\n            'origin': 'https://www.bikeregister.com',\n            'accept-encoding': 'gzip, deflate, br',\n            'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8',\n            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0',\n            'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',\n            'accept': '*/*',\n            'referer': 'https://www.bikeregister.com/stolen-bikes',\n            'authority': 'www.bikeregister.com',\n            'x-requested-with': 'XMLHttpRequest',\n        }\n\n        data = [\n            ('_token', token),\n            ('make', ''),\n            ('model', ''),\n            ('colour', ''),\n            ('reporting_period', '1'),\n        ]\n\n        try:\n            async with session.post('https://www.bikeregister.com/stolen-bikes', headers=headers, data=data) as request:\n                bikes = json.loads(await request.text())\n        except ClientConnectionError as con_err:\n            logger.debug(f\"Could not connect to {con_err.host}\")\n            raise ApiError(f\"Could not connect to {con_err.host}\")\n        except json.JSONDecodeError as dec_err:\n            logger.error(f\"Could not decode data: {dec_err.msg}\")\n            raise ApiError(f\"Could not decode data: {dec_err.msg}\")\n\n        return bikes\n\n    # if cant open a session\n    return []",
        "return_type_from_source": "List[dict]"
    },
    {
        "extra_left": [],
        "left": "def submonitor(self, units, *args, **kargs) -> ",
        "right": ":\n        \"\"\"\n        Create a sub monitor that stands for N units of work in this monitor\n        The sub task should call .begin (or use @monitored / with .task) before calling updates\n        \"\"\"\n        submonitor = ProgressMonitor(*args, **kargs)\n        self.sub_monitors[submonitor] = units\n        submonitor.add_listener(self._submonitor_update)\n        return submonitor",
        "return_type_from_source": "'ProgressMonitor'"
    },
    {
        "extra_left": [],
        "left": "async def _handle_response(self, response, await_final_result) -> ",
        "right": ":\n        \"\"\"\n        Handles the response returned from the CloudStack API. Some CloudStack API are implemented asynchronous, which\n        means that the API call returns just a job id. The actually expected API response is postponed and a specific\n        asyncJobResults API has to be polled using the job id to get the final result once the API call has been\n        processed.\n\n        :param response: The response returned by the aiohttp call.\n        :type response: aiohttp.client_reqrep.ClientResponse\n        :param await_final_result: Specifier that indicates whether the function should poll the asyncJobResult API\n                                   until the asynchronous API call has been processed\n        :type await_final_result: bool\n        :return: Dictionary containing the JSON response of the API call\n        :rtype: dict\n        \"\"\"\n        try:\n            data = await response.json()\n        except aiohttp.client_exceptions.ContentTypeError:\n            text = await response.text()\n            logging.debug('Content returned by server not of type \"application/json\"\\n Content: {}'.format(text))\n            raise CloudStackClientException(message=\"Could not decode content. Server did not return json content!\")\n        else:\n            data = self._transform_data(data)\n            if response.status != 200:\n                raise CloudStackClientException(message=\"Async CloudStack call failed!\",\n                                                error_code=data.get(\"errorcode\", response.status),\n                                                error_text=data.get(\"errortext\"),\n                                                response=data)\n\n        while await_final_result and ('jobid' in data):\n            await asyncio.sleep(self.async_poll_latency)\n            data = await self.queryAsyncJobResult(jobid=data['jobid'])\n            if data['jobstatus']:  # jobstatus is 0 for pending async CloudStack calls\n                if not data['jobresultcode']:  # exit code is zero\n                    try:\n                        return data['jobresult']\n                    except KeyError:\n                        pass\n                logging.debug(\"Async CloudStack call returned {}\".format(str(data)))\n                raise CloudStackClientException(message=\"Async CloudStack call failed!\",\n                                                error_code=data.get(\"errorcode\"),\n                                                error_text=data.get(\"errortext\"),\n                                                response=data)\n\n        return data",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _sign(self, url_parameters) -> ",
        "right": ":\n        \"\"\"\n        According to the CloudStack documentation, each request needs to be signed in order to authenticate the user\n        account executing the API command. The signature is generated using a combination of the api secret and a SHA-1\n        hash of the url parameters including the command string. In order to generate a unique identifier, the url\n        parameters have to be transformed to lower case and ordered alphabetically.\n\n        :param url_parameters: The url parameters of the API call including the command string\n        :type url_parameters: dict\n        :return: The url parameters including a new key, which contains the signature\n        :rtype: dict\n        \"\"\"\n        if url_parameters:\n            url_parameters.pop('signature', None)  # remove potential existing signature from url parameters\n            request_string = urlencode(sorted(url_parameters.items()), safe='.-*_', quote_via=quote).lower()\n            digest = hmac.new(self.api_secret.encode('utf-8'), request_string.encode('utf-8'), hashlib.sha1).digest()\n            url_parameters['signature'] = base64.b64encode(digest).decode('utf-8').strip()\n        return url_parameters",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def _transform_data(data) -> ",
        "right": ":\n        \"\"\"\n        Each CloudStack API call returns a nested dictionary structure. The first level contains only one key indicating\n        the API that originated the response. This function removes that first level from the data returned to the\n        caller.\n\n        :param data: Response of the API call\n        :type data: dict\n        :return: Simplified response without the information about the API that originated the response.\n        :rtype: dict\n        \"\"\"\n        for key in data.keys():\n            return_value = data[key]\n            if isinstance(return_value, dict):\n                return return_value\n        return data",
        "return_type_from_source": "dict"
    }
]