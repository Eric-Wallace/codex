[
    {
        "extra_left": [],
        "left": "def setdefault(self, key, value) -> ",
        "right": ":\n        \"\"\"\n        If the header `key` does not exist, then set it to `value`.\n        Returns the header value.\n        \"\"\"\n        set_key = key.lower().encode(\"latin-1\")\n        set_value = value.encode(\"latin-1\")\n\n        for idx, (item_key, item_value) in enumerate(self._list):\n            if item_key == set_key:\n                return item_value.decode(\"latin-1\")\n        self._list.append((set_key, set_value))\n        return value",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def append(self, key, value) -> ",
        "right": ":\n        \"\"\"\n        Append a header, preserving any duplicate entries.\n        \"\"\"\n        append_key = key.lower().encode(\"latin-1\")\n        append_value = value.encode(\"latin-1\")\n        self._list.append((append_key, append_value))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def parse_docstring(self, func_or_method) -> ",
        "right": ":\n        \"\"\"\n        Given a function, parse the docstring as YAML and return a dictionary of info.\n        \"\"\"\n        docstring = func_or_method.__doc__\n        if not docstring:\n            return {}\n\n        # We support having regular docstrings before the schema\n        # definition. Here we return just the schema part from\n        # the docstring.\n        docstring = docstring.split(\"---\")[-1]\n\n        parsed = yaml.safe_load(docstring)\n\n        if not isinstance(parsed, dict):\n            # A regular docstring (not yaml formatted) can return\n            # a simple string here, which wouldn't follow the schema.\n            return {}\n\n        return parsed",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def get_directories(\n        self, directory = None, packages = None\n    ) -> ",
        "right": ":\n        \"\"\"\n        Given `directory` and `packages` arugments, return a list of all the\n        directories that should be used for serving static files from.\n        \"\"\"\n        directories = []\n        if directory is not None:\n            directories.append(directory)\n\n        for package in packages or []:\n            spec = importlib.util.find_spec(package)\n            assert spec is not None, f\"Package {package!r} could not be found.\"\n            assert (\n                spec.origin is not None\n            ), \"Directory 'statics' in package {package!r} could not be found.\"\n            directory = os.path.normpath(os.path.join(spec.origin, \"..\", \"statics\"))\n            assert os.path.isdir(\n                directory\n            ), \"Directory 'statics' in package {package!r} could not be found.\"\n            directories.append(directory)\n\n        return directories",
        "return_type_from_source": "typing.List[str]"
    },
    {
        "extra_left": [],
        "left": "async def get_response(self, path, scope) -> ",
        "right": ":\n        \"\"\"\n        Returns an HTTP response, given the incoming path, method and request headers.\n        \"\"\"\n        if scope[\"method\"] not in (\"GET\", \"HEAD\"):\n            return PlainTextResponse(\"Method Not Allowed\", status_code=405)\n\n        if path.startswith(\"..\"):\n            # Most clients will normalize the path, so we shouldn't normally\n            # get this, but don't allow misbehaving clients to break out of\n            # the static files directory.\n            return PlainTextResponse(\"Not Found\", status_code=404)\n\n        full_path, stat_result = await self.lookup_path(path)\n\n        if stat_result and stat.S_ISREG(stat_result.st_mode):\n            # We have a static file to serve.\n            return self.file_response(full_path, stat_result, scope)\n\n        elif stat_result and stat.S_ISDIR(stat_result.st_mode) and self.html:\n            # We're in HTML mode, and have got a directory URL.\n            # Check if we have 'index.html' file to serve.\n            index_path = os.path.join(path, \"index.html\")\n            full_path, stat_result = await self.lookup_path(index_path)\n            if stat_result is not None and stat.S_ISREG(stat_result.st_mode):\n                if not scope[\"path\"].endswith(\"/\"):\n                    # Directory URLs should redirect to always end in \"/\".\n                    url = URL(scope=scope)\n                    url = url.replace(path=url.path + \"/\")\n                    return RedirectResponse(url=url)\n                return self.file_response(full_path, stat_result, scope)\n\n        if self.html:\n            # Check for '404.html' if we're in HTML mode.\n            full_path, stat_result = await self.lookup_path(\"404.html\")\n            if stat_result is not None and stat.S_ISREG(stat_result.st_mode):\n                return self.file_response(\n                    full_path, stat_result, scope, status_code=404\n                )\n\n        return PlainTextResponse(\"Not Found\", status_code=404)",
        "return_type_from_source": "Response"
    },
    {
        "extra_left": [],
        "left": "async def check_config(self) -> ",
        "right": ":\n        \"\"\"\n        Perform a one-off configuration check that StaticFiles is actually\n        pointed at a directory, so that we can raise loud errors rather than\n        just returning 404 responses.\n        \"\"\"\n        if self.directory is None:\n            return\n\n        try:\n            stat_result = await aio_stat(self.directory)\n        except FileNotFoundError:\n            raise RuntimeError(\n                f\"StaticFiles directory '{self.directory}' does not exist.\"\n            )\n        if not (stat.S_ISDIR(stat_result.st_mode) or stat.S_ISLNK(stat_result.st_mode)):\n            raise RuntimeError(\n                f\"StaticFiles path '{self.directory}' is not a directory.\"\n            )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def is_not_modified(\n        self, response_headers, request_headers\n    ) -> ",
        "right": ":\n        \"\"\"\n        Given the request and response headers, return `True` if an HTTP\n        \"Not Modified\" response could be returned instead.\n        \"\"\"\n        try:\n            if_none_match = request_headers[\"if-none-match\"]\n            etag = response_headers[\"etag\"]\n            if if_none_match == etag:\n                return True\n        except KeyError:\n            pass\n\n        try:\n            if_modified_since = parsedate(request_headers[\"if-modified-since\"])\n            last_modified = parsedate(response_headers[\"last-modified\"])\n            if (\n                if_modified_since is not None\n                and last_modified is not None\n                and if_modified_since >= last_modified\n            ):\n                return True\n        except KeyError:\n            pass\n\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def build_environ(scope, body) -> ",
        "right": ":\n    \"\"\"\n    Builds a scope and request body into a WSGI environ object.\n    \"\"\"\n    environ = {\n        \"REQUEST_METHOD\": scope[\"method\"],\n        \"SCRIPT_NAME\": scope.get(\"root_path\", \"\"),\n        \"PATH_INFO\": scope[\"path\"],\n        \"QUERY_STRING\": scope[\"query_string\"].decode(\"ascii\"),\n        \"SERVER_PROTOCOL\": f\"HTTP/{scope['http_version']}\",\n        \"wsgi.version\": (1, 0),\n        \"wsgi.url_scheme\": scope.get(\"scheme\", \"http\"),\n        \"wsgi.input\": io.BytesIO(body),\n        \"wsgi.errors\": sys.stdout,\n        \"wsgi.multithread\": True,\n        \"wsgi.multiprocess\": True,\n        \"wsgi.run_once\": False,\n    }\n\n    # Get server name and port - required in WSGI, not in ASGI\n    server = scope.get(\"server\") or (\"localhost\", 80)\n    environ[\"SERVER_NAME\"] = server[0]\n    environ[\"SERVER_PORT\"] = server[1]\n\n    # Get client IP address\n    if scope.get(\"client\"):\n        environ[\"REMOTE_ADDR\"] = scope[\"client\"][0]\n\n    # Go through headers and make them into environ entries\n    for name, value in scope.get(\"headers\", []):\n        name = name.decode(\"latin1\")\n        if name == \"content-length\":\n            corrected_name = \"CONTENT_LENGTH\"\n        elif name == \"content-type\":\n            corrected_name = \"CONTENT_TYPE\"\n        else:\n            corrected_name = f\"HTTP_{name}\".upper().replace(\"-\", \"_\")\n        # HTTPbis say only ASCII chars are allowed in headers, but we latin1 just in case\n        value = value.decode(\"latin1\")\n        if corrected_name in environ:\n            value = environ[corrected_name] + \",\" + value\n        environ[corrected_name] = value\n    return environ",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "async def receive(self) -> ",
        "right": ":\n        \"\"\"\n        Receive ASGI websocket messages, ensuring valid state transitions.\n        \"\"\"\n        if self.client_state == WebSocketState.CONNECTING:\n            message = await self._receive()\n            message_type = message[\"type\"]\n            assert message_type == \"websocket.connect\"\n            self.client_state = WebSocketState.CONNECTED\n            return message\n        elif self.client_state == WebSocketState.CONNECTED:\n            message = await self._receive()\n            message_type = message[\"type\"]\n            assert message_type in {\"websocket.receive\", \"websocket.disconnect\"}\n            if message_type == \"websocket.disconnect\":\n                self.client_state = WebSocketState.DISCONNECTED\n            return message\n        else:\n            raise RuntimeError(\n                'Cannot call \"receive\" once a disconnect message has been received.'\n            )",
        "return_type_from_source": "Message"
    },
    {
        "extra_left": [],
        "left": "async def send(self, message) -> ",
        "right": ":\n        \"\"\"\n        Send ASGI websocket messages, ensuring valid state transitions.\n        \"\"\"\n        if self.application_state == WebSocketState.CONNECTING:\n            message_type = message[\"type\"]\n            assert message_type in {\"websocket.accept\", \"websocket.close\"}\n            if message_type == \"websocket.close\":\n                self.application_state = WebSocketState.DISCONNECTED\n            else:\n                self.application_state = WebSocketState.CONNECTED\n            await self._send(message)\n        elif self.application_state == WebSocketState.CONNECTED:\n            message_type = message[\"type\"]\n            assert message_type in {\"websocket.send\", \"websocket.close\"}\n            if message_type == \"websocket.close\":\n                self.application_state = WebSocketState.DISCONNECTED\n            await self._send(message)\n        else:\n            raise RuntimeError('Cannot call \"send\" once a close message has been sent.')",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def SetClipboardText(text) -> ",
        "right": ":\n    \"\"\"\n    Return bool, True if succeed otherwise False.\n    \"\"\"\n    if ctypes.windll.user32.OpenClipboard(0):\n        ctypes.windll.user32.EmptyClipboard()\n        textByteLen = (len(text) + 1) * 2\n        hClipboardData = ctypes.windll.kernel32.GlobalAlloc(0, textByteLen)  # GMEM_FIXED=0\n        hDestText = ctypes.windll.kernel32.GlobalLock(hClipboardData)\n        ctypes.cdll.msvcrt.wcsncpy(ctypes.c_wchar_p(hDestText), ctypes.c_wchar_p(text), textByteLen // 2)\n        ctypes.windll.kernel32.GlobalUnlock(hClipboardData)\n        # system owns hClipboardData after calling SetClipboardData,\n        # application can not write to or free the data once ownership has been transferred to the system\n        ctypes.windll.user32.SetClipboardData(13, hClipboardData)  # CF_TEXT=1, CF_UNICODETEXT=13\n        ctypes.windll.user32.CloseClipboard()\n        return True\n    return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def ResetConsoleColor() -> ",
        "right": ":\n    \"\"\"\n    Reset to the default text color on console window.\n    Return bool, True if succeed otherwise False.\n    \"\"\"\n    if sys.stdout:\n        sys.stdout.flush()\n    bool(ctypes.windll.kernel32.SetConsoleTextAttribute(_ConsoleOutputHandle, _DefaultConsoleColor))",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def WindowFromPoint(x, y) -> ",
        "right": ":\n    \"\"\"\n    WindowFromPoint from Win32.\n    Return int, a native window handle.\n    \"\"\"\n    return ctypes.windll.user32.WindowFromPoint(ctypes.wintypes.POINT(x, y))",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def mouse_event(dwFlags, dx, dy, dwData, dwExtraInfo) -> ",
        "right": ":\n    \"\"\"mouse_event from Win32.\"\"\"\n    ctypes.windll.user32.mouse_event(dwFlags, dx, dy, dwData, dwExtraInfo)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def keybd_event(bVk, bScan, dwFlags, dwExtraInfo) -> ",
        "right": ":\n    \"\"\"keybd_event from Win32.\"\"\"\n    ctypes.windll.user32.keybd_event(bVk, bScan, dwFlags, dwExtraInfo)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def PostMessage(handle, msg, wParam, lParam) -> ",
        "right": ":\n    \"\"\"\n    PostMessage from Win32.\n    Return bool, True if succeed otherwise False.\n    \"\"\"\n    return bool(ctypes.windll.user32.PostMessageW(ctypes.c_void_p(handle), msg, wParam, lParam))",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def SendMessage(handle, msg, wParam, lParam) -> ",
        "right": ":\n    \"\"\"\n    SendMessage from Win32.\n    Return int, the return value specifies the result of the message processing;\n                it depends on the message sent.\n    \"\"\"\n    return ctypes.windll.user32.SendMessageW(ctypes.c_void_p(handle), msg, wParam, lParam)",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def GetConsoleOriginalTitle() -> ",
        "right": ":\n    \"\"\"\n    GetConsoleOriginalTitle from Win32.\n    Return str.\n    Only available on Windows Vista or higher.\n    \"\"\"\n    if IsNT6orHigher:\n        arrayType = ctypes.c_wchar * MAX_PATH\n        values = arrayType()\n        ctypes.windll.kernel32.GetConsoleOriginalTitleW(values, MAX_PATH)\n        return values.value\n    else:\n        raise RuntimeError('GetConsoleOriginalTitle is not supported on Windows XP or lower.')",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def GetConsoleTitle() -> ",
        "right": ":\n    \"\"\"\n    GetConsoleTitle from Win32.\n    Return str.\n    \"\"\"\n    arrayType = ctypes.c_wchar * MAX_PATH\n    values = arrayType()\n    ctypes.windll.kernel32.GetConsoleTitleW(values, MAX_PATH)\n    return values.value",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def IsDesktopLocked() -> ",
        "right": ":\n    \"\"\"\n    Check if desktop is locked.\n    Return bool.\n    Desktop is locked if press Win+L, Ctrl+Alt+Del or in remote desktop mode.\n    \"\"\"\n    isLocked = False\n    desk = ctypes.windll.user32.OpenDesktopW(ctypes.c_wchar_p('Default'), 0, 0, 0x0100)  # DESKTOP_SWITCHDESKTOP = 0x0100\n    if desk:\n        isLocked = not ctypes.windll.user32.SwitchDesktop(desk)\n        ctypes.windll.user32.CloseDesktop(desk)\n    return isLocked",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def IsProcess64Bit(processId) -> ",
        "right": ":\n    \"\"\"\n    Return True if process is 64 bit.\n    Return False if process is 32 bit.\n    Return None if unknown, maybe caused by having no acess right to the process.\n    \"\"\"\n    try:\n        func = ctypes.windll.ntdll.ZwWow64ReadVirtualMemory64  #only 64 bit OS has this function\n    except Exception as ex:\n        return False\n    try:\n        IsWow64Process = ctypes.windll.kernel32.IsWow64Process\n        IsWow64Process.argtypes = (ctypes.c_void_p, ctypes.POINTER(ctypes.c_int))\n    except Exception as ex:\n        return False\n    hProcess = ctypes.windll.kernel32.OpenProcess(0x1000, 0, processId)  #PROCESS_QUERY_INFORMATION=0x0400,PROCESS_QUERY_LIMITED_INFORMATION=0x1000\n    if hProcess:\n        is64Bit = ctypes.c_int32()\n        if IsWow64Process(hProcess, ctypes.byref(is64Bit)):\n            ctypes.windll.kernel32.CloseHandle(ctypes.c_void_p(hProcess))\n            return False if is64Bit.value else True\n        else:\n            ctypes.windll.kernel32.CloseHandle(ctypes.c_void_p(hProcess))",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _CreateInput(structure) -> ",
        "right": ":\n    \"\"\"\n    Create Win32 struct `INPUT` for `SendInput`.\n    Return `INPUT`.\n    \"\"\"\n    if isinstance(structure, MOUSEINPUT):\n        return INPUT(InputType.Mouse, _INPUTUnion(mi=structure))\n    if isinstance(structure, KEYBDINPUT):\n        return INPUT(InputType.Keyboard, _INPUTUnion(ki=structure))\n    if isinstance(structure, HARDWAREINPUT):\n        return INPUT(InputType.Hardware, _INPUTUnion(hi=structure))\n    raise TypeError('Cannot create INPUT structure!')",
        "return_type_from_source": "INPUT"
    },
    {
        "extra_left": [],
        "left": "def MouseInput(dx, dy, mouseData = 0, dwFlags = MouseEventFlag.LeftDown, time_ = 0) -> ",
        "right": ":\n    \"\"\"\n    Create Win32 struct `MOUSEINPUT` for `SendInput`.\n    Return `INPUT`.\n    \"\"\"\n    return _CreateInput(MOUSEINPUT(dx, dy, mouseData, dwFlags, time_, None))",
        "return_type_from_source": "INPUT"
    },
    {
        "extra_left": [],
        "left": "def KeyboardInput(wVk, wScan, dwFlags = KeyboardEventFlag.KeyDown, time_ = 0) -> ",
        "right": ":\n    \"\"\"Create Win32 struct `KEYBDINPUT` for `SendInput`.\"\"\"\n    return _CreateInput(KEYBDINPUT(wVk, wScan, dwFlags, time_, None))",
        "return_type_from_source": "INPUT"
    },
    {
        "extra_left": [],
        "left": "def HardwareInput(uMsg, param = 0) -> ",
        "right": ":\n    \"\"\"Create Win32 struct `HARDWAREINPUT` for `SendInput`.\"\"\"\n    return _CreateInput(HARDWAREINPUT(uMsg, param & 0xFFFF, param >> 16 & 0xFFFF))",
        "return_type_from_source": "INPUT"
    },
    {
        "extra_left": [],
        "left": "def ControlFromPoint(x, y) -> ",
        "right": ":\n    \"\"\"\n    Call IUIAutomation ElementFromPoint x,y. May return None if mouse is over cmd's title bar icon.\n    Return `Control` subclass or None.\n    \"\"\"\n    element = _AutomationClient.instance().IUIAutomation.ElementFromPoint(ctypes.wintypes.POINT(x, y))\n    return Control.CreateControlFromElement(element)",
        "return_type_from_source": "Control"
    },
    {
        "extra_left": [],
        "left": "def ControlFromPoint2(x, y) -> ",
        "right": ":\n    \"\"\"\n    Get a native handle from point x,y and call IUIAutomation.ElementFromHandle.\n    Return `Control` subclass.\n    \"\"\"\n    return Control.CreateControlFromElement(_AutomationClient.instance().IUIAutomation.ElementFromHandle(WindowFromPoint(x, y)))",
        "return_type_from_source": "Control"
    },
    {
        "extra_left": [],
        "left": "def DeleteLog() -> ",
        "right": ":\n        \"\"\"Delete log file.\"\"\"\n        if os.path.exists(Logger.FileName):\n            os.remove(Logger.FileName)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def GetAllPixelColors(self) -> ",
        "right": ":\n        \"\"\"\n        Return `ctypes.Array`, an iterable array of int values in argb.\n        \"\"\"\n        return self.GetPixelColorsOfRect(0, 0, self.Width, self.Height)",
        "return_type_from_source": "ctypes.Array"
    },
    {
        "extra_left": [],
        "left": "def GetChildren(self) -> ",
        "right": ":\n        \"\"\"\n        Return list, a list of `Control` subclasses.\n        \"\"\"\n        children = []\n        child = self.GetFirstChildControl()\n        while child:\n            children.append(child)\n            child = child.GetNextSiblingControl()\n        return children",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def SetWindowText(self, text) -> ",
        "right": ":\n        \"\"\"\n        Call native SetWindowText if control has a valid native handle.\n        \"\"\"\n        handle = self.NativeWindowHandle\n        if handle:\n            return SetWindowText(handle, text)\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def IsTopLevel(self) -> ",
        "right": ":\n        \"\"\"Determine whether current control is top level.\"\"\"\n        handle = self.NativeWindowHandle\n        if handle:\n            return GetAncestor(handle, GAFlag.Root) == handle\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def GetTopLevelControl(self) -> ",
        "right": ":\n        \"\"\"\n        Get the top level control which current control lays.\n        If current control is top level, return self.\n        If current control is root control, return None.\n        Return `PaneControl` or `WindowControl` or None.\n        \"\"\"\n        handle = self.NativeWindowHandle\n        if handle:\n            topHandle = GetAncestor(handle, GAFlag.Root)\n            if topHandle:\n                if topHandle == handle:\n                    return self\n                else:\n                    return ControlFromHandle(topHandle)\n            else:\n                #self is root control\n                pass\n        else:\n            control = self\n            while True:\n                control = control.GetParentControl()\n                handle = control.NativeWindowHandle\n                if handle:\n                    topHandle = GetAncestor(handle, GAFlag.Root)\n                    return ControlFromHandle(topHandle)",
        "return_type_from_source": "'Control'"
    },
    {
        "extra_left": [],
        "left": "def Maximize(self, waitTime = OPERATION_WAIT_TIME) -> ",
        "right": ":\n        \"\"\"\n        Set top level window maximize.\n        \"\"\"\n        if self.IsTopLevel():\n            return self.ShowWindow(SW.ShowMaximized, waitTime)\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def MoveToCenter(self) -> ",
        "right": ":\n        \"\"\"\n        Move window to screen center.\n        \"\"\"\n        if self.IsTopLevel():\n            rect = self.BoundingRectangle\n            screenWidth, screenHeight = GetScreenSize()\n            x, y = (screenWidth - rect.width()) // 2, (screenHeight - rect.height()) // 2\n            if x < 0: x = 0\n            if y < 0: y = 0\n            return SetWindowPos(self.NativeWindowHandle, SWP.HWND_Top, x, y, 0, 0, SWP.SWP_NoSize)\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def SetActive(self, waitTime = OPERATION_WAIT_TIME) -> ",
        "right": ":\n        \"\"\"Set top level window active.\"\"\"\n        if self.IsTopLevel():\n            handle = self.NativeWindowHandle\n            if IsIconic(handle):\n                ret = ShowWindow(handle, SW.Restore)\n            elif not IsWindowVisible(handle):\n                ret = ShowWindow(handle, SW.Show)\n            ret = SetForegroundWindow(handle)  # may fail if foreground windows's process is not python\n            time.sleep(waitTime)\n            return ret\n        return False",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _raw_read(self, where, size=1) -> ",
        "right": ":\n        \"\"\"\n        Selects bytes from memory. Attempts to do so faster than via read_bytes.\n\n        :param where: address to read from\n        :param size: number of bytes to read\n        :return: the bytes in memory\n        \"\"\"\n        map = self.memory.map_containing(where)\n        start = map._get_offset(where)\n        mapType = type(map)\n        if mapType is FileMap:\n            end = map._get_offset(where + size)\n\n            if end > map._mapped_size:\n                logger.warning(f\"Missing {end - map._mapped_size} bytes at the end of {map._filename}\")\n\n            raw_data = map._data[map._get_offset(where): min(end, map._mapped_size)]\n            if len(raw_data) < end:\n                raw_data += b'\\x00' * (end - len(raw_data))\n\n            data = b''\n            for offset in sorted(map._overlay.keys()):\n                data += raw_data[len(data):offset]\n                data += map._overlay[offset]\n            data += raw_data[len(data):]\n\n        elif mapType is AnonMap:\n            data = bytes(map._data[start:start + size])\n        else:\n            data = b''.join(self.memory[where:where + size])\n        assert len(data) == size, 'Raw read resulted in wrong data read which should never happen'\n        return data",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def must_be_true(self, constraints, expression) -> ",
        "right": ":\n        \"\"\"Check if expression is True and that it can not be False with current constraints\"\"\"\n        solutions = self.get_all_values(constraints, expression, maxcnt=2, silent=True)\n        return solutions == [True]",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def _solver_version(self) -> ",
        "right": ":\n        \"\"\"\n        If we fail to parse the version, we assume z3's output has changed, meaning it's a newer\n        version than what's used now, and therefore ok.\n\n        Anticipated version_cmd_output format: 'Z3 version 4.4.2'\n                                               'Z3 version 4.4.5 - 64 bit - build hashcode $Z3GITHASH'\n        \"\"\"\n        self._reset()\n        if self._received_version is None:\n            self._send('(get-info :version)')\n            self._received_version = self._recv()\n        key, version = shlex.split(self._received_version[1:-1])\n        return Version(*map(int, version.split('.')))",
        "return_type_from_source": "Version"
    },
    {
        "extra_left": [],
        "left": "def _recv(self) -> ",
        "right": ":\n        \"\"\"Reads the response from the solver\"\"\"\n        buf, left, right = self.__readline_and_count()\n        bufl = [buf]\n\n        while left != right:\n            buf, l, r = self.__readline_and_count()\n            bufl.append(buf)\n            left += l\n            right += r\n\n        buf = ''.join(bufl).strip()\n\n        logger.debug('<%s', buf)\n        if '(error' in bufl[0]:\n            raise Exception(f\"Error in smtlib: {bufl[0]}\")\n        return buf",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _is_sat(self) -> ",
        "right": ":\n        \"\"\"\n        Check the satisfiability of the current state\n\n        :return: whether current state is satisfiable or not.\n        \"\"\"\n        logger.debug(\"Solver.check() \")\n        start = time.time()\n        self._send('(check-sat)')\n        status = self._recv()\n        logger.debug(\"Check took %s seconds (%s)\", time.time() - start, status)\n        if status not in ('sat', 'unsat', 'unknown'):\n            raise SolverError(status)\n        if consider_unknown_as_unsat:\n            if status == 'unknown':\n                logger.info('Found an unknown core, probably a solver timeout')\n                status = 'unsat'\n\n        if status == 'unknown':\n            raise SolverUnknown(status)\n\n        return status == 'sat'",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_group(name) -> ",
        "right": ":\n    \"\"\"\n    Get a configuration variable group named |name|\n    \"\"\"\n    global _groups\n\n    if name in _groups:\n        return _groups[name]\n\n    group = _Group(name)\n    _groups[name] = group\n\n    return group",
        "return_type_from_source": "_Group"
    },
    {
        "extra_left": [],
        "left": "def get_description(self, name) -> ",
        "right": ":\n        \"\"\"\n        Return the description, or a help string of variable identified by |name|.\n        \"\"\"\n        if name not in self._vars:\n            raise ConfigError(f\"{self.name}.{name} not defined.\")\n\n        return self._vars[name].description",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def function_signature_for_name_and_inputs(name, inputs) -> ",
        "right": ":\n        \"\"\"Returns the function signature for the specified name and Solidity JSON metadata inputs array.\n\n        The ABI specification defines the function signature as the function name followed by the parenthesised list of\n        parameter types separated by single commas and no spaces.\n        See https://solidity.readthedocs.io/en/latest/abi-spec.html#function-selector\n        \"\"\"\n        return name + SolidityMetadata.tuple_signature_for_components(inputs)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_constructor_arguments(self) -> ",
        "right": ":\n        \"\"\"Returns the tuple type signature for the arguments of the contract constructor.\"\"\"\n        item = self._constructor_abi_item\n        return '()' if item is None else self.tuple_signature_for_components(item['inputs'])",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_func_return_types(self, hsh) -> ",
        "right": ":\n        \"\"\"Returns the tuple type signature for the output values of the function\n        associated with the selector ``hsh``.\n\n        If no normal contract function has the specified selector,\n        the empty tuple type signature ``'()'`` is returned.\n        \"\"\"\n        if not isinstance(hsh, (bytes, bytearray)):\n            raise TypeError('The selector argument must be a concrete byte array')\n        abi = self.get_abi(hsh)\n        outputs = abi.get('outputs')\n        return '()' if outputs is None else SolidityMetadata.tuple_signature_for_components(outputs)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_func_signature(self, hsh) -> ",
        "right": ":\n        \"\"\"Returns the signature of the normal function with the selector ``hsh``,\n        or ``None`` if no such function exists.\n\n        This function returns ``None`` for any selector that will be dispatched to a fallback function.\n        \"\"\"\n        if not isinstance(hsh, (bytes, bytearray)):\n            raise TypeError('The selector argument must be a concrete byte array')\n        return self._function_signatures_by_selector.get(hsh)",
        "return_type_from_source": "Optional[str]"
    },
    {
        "extra_left": [],
        "left": "def get_languages() -> ",
        "right": ":\n    \"\"\"Get supported languages.\"\"\"\n    try:\n        languages = cache['languages']\n    except KeyError:\n        languages = LanguageTool._get_languages()\n        cache['languages'] = languages\n    return languages",
        "return_type_from_source": "set"
    },
    {
        "extra_left": [],
        "left": "def check(self, text, srctext=None) -> ",
        "right": ":\n        \"\"\"Match text against enabled rules.\"\"\"\n        root = self._get_root(self._url, self._encode(text, srctext))\n        return [Match(e.attrib) for e in root if e.tag == 'error']",
        "return_type_from_source": "[Match]"
    },
    {
        "extra_left": [],
        "left": "def proc_elms(**kwargs) -> ",
        "right": ":\n    \"\"\"\n    Bloomberg overrides for elements\n\n    Args:\n        **kwargs: overrides\n\n    Returns:\n        list of tuples\n\n    Examples:\n        >>> proc_elms(PerAdj='A', Per='W')\n        [('periodicityAdjustment', 'ACTUAL'), ('periodicitySelection', 'WEEKLY')]\n        >>> proc_elms(Days='A', Fill='B')\n        [('nonTradingDayFillOption', 'ALL_CALENDAR_DAYS'), ('nonTradingDayFillMethod', 'NIL_VALUE')]\n        >>> proc_elms(CshAdjNormal=False, CshAdjAbnormal=True)\n        [('adjustmentNormal', False), ('adjustmentAbnormal', True)]\n        >>> proc_elms(Per='W', Quote='Average', start_date='2018-01-10')\n        [('periodicitySelection', 'WEEKLY'), ('overrideOption', 'OVERRIDE_OPTION_GPA')]\n        >>> proc_elms(QuoteType='Y')\n        [('pricingOption', 'PRICING_OPTION_YIELD')]\n        >>> proc_elms(QuoteType='Y', cache=True)\n        [('pricingOption', 'PRICING_OPTION_YIELD')]\n    \"\"\"\n    return [\n        (ELEM_KEYS.get(k, k), ELEM_VALS.get(ELEM_KEYS.get(k, k), dict()).get(v, v))\n        for k, v in kwargs.items()\n        if (k in list(ELEM_KEYS.keys()) + list(ELEM_KEYS.values()))\n        and (k not in PRSV_COLS)\n    ]",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def format_earning(data, header) -> ",
        "right": ":\n    \"\"\"\n    Standardized earning outputs and add percentage by each blocks\n\n    Args:\n        data: earning data block\n        header: earning headers\n\n    Returns:\n        pd.DataFrame\n\n    Examples:\n        >>> format_earning(\n        ...     data=pd.read_pickle('xbbg/tests/data/sample_earning.pkl'),\n        ...     header=pd.read_pickle('xbbg/tests/data/sample_earning_header.pkl')\n        ... ).round(2)\n                         level  fy2017  fy2017_pct\n        Asia-Pacific       1.0  3540.0       66.43\n        \u00a0\u00a0\u00a0China           2.0  1747.0       49.35\n        \u00a0\u00a0\u00a0Japan           2.0  1242.0       35.08\n        \u00a0\u00a0\u00a0Singapore       2.0   551.0       15.56\n        United States      1.0  1364.0       25.60\n        Europe             1.0   263.0        4.94\n        Other Countries    1.0   162.0        3.04\n    \"\"\"\n    if data.dropna(subset=['value']).empty: return pd.DataFrame()\n\n    res = pd.concat([\n        grp.loc[:, ['value']].set_index(header.value)\n        for _, grp in data.groupby(data.position)\n    ], axis=1)\n    res.index.name = None\n    res.columns = res.iloc[0]\n    res = res.iloc[1:].transpose().reset_index().apply(\n        pd.to_numeric, downcast='float', errors='ignore'\n    )\n    res.rename(\n        columns=lambda vv: '_'.join(vv.lower().split()).replace('fy_', 'fy'),\n        inplace=True,\n    )\n\n    years = res.columns[res.columns.str.startswith('fy')]\n    lvl_1 = res.level == 1\n    for yr in years:\n        res.loc[:, yr] = res.loc[:, yr].round(1)\n        pct = f'{yr}_pct'\n        res.loc[:, pct] = 0.\n        res.loc[lvl_1, pct] = res.loc[lvl_1, pct].astype(float).round(1)\n        res.loc[lvl_1, pct] = res.loc[lvl_1, yr] / res.loc[lvl_1, yr].sum() * 100\n        sub_pct = []\n        for _, snap in res[::-1].iterrows():\n            if snap.level > 2: continue\n            if snap.level == 1:\n                if len(sub_pct) == 0: continue\n                sub = pd.concat(sub_pct, axis=1).transpose()\n                res.loc[sub.index, pct] = \\\n                    res.loc[sub.index, yr] / res.loc[sub.index, yr].sum() * 100\n                sub_pct = []\n            if snap.level == 2: sub_pct.append(snap)\n\n    res.set_index('segment_name', inplace=True)\n    res.index.name = None\n    return res",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def format_output(data, source, col_maps=None) -> ",
        "right": ":\n    \"\"\"\n    Format `pdblp` outputs to column-based results\n\n    Args:\n        data: `pdblp` result\n        source: `bdp` or `bds`\n        col_maps: rename columns with these mappings\n\n    Returns:\n        pd.DataFrame\n\n    Examples:\n        >>> format_output(\n        ...     data=pd.read_pickle('xbbg/tests/data/sample_bdp.pkl'),\n        ...     source='bdp'\n        ... ).reset_index()\n                  ticker                        name\n        0  QQQ US Equity  INVESCO QQQ TRUST SERIES 1\n        1  SPY US Equity      SPDR S&P 500 ETF TRUST\n        >>> format_output(\n        ...     data=pd.read_pickle('xbbg/tests/data/sample_dvd.pkl'),\n        ...     source='bds', col_maps={'Dividend Frequency': 'dvd_freq'}\n        ... ).loc[:, ['ex_date', 'dividend_amount', 'dvd_freq']].reset_index()\n                ticker     ex_date  dividend_amount dvd_freq\n        0  C US Equity  2018-02-02             0.32  Quarter\n    \"\"\"\n    if data.empty: return pd.DataFrame()\n    if source == 'bdp': req_cols = ['ticker', 'field', 'value']\n    else: req_cols = ['ticker', 'field', 'name', 'value', 'position']\n    if any(col not in data for col in req_cols): return pd.DataFrame()\n    if data.dropna(subset=['value']).empty: return pd.DataFrame()\n\n    if source == 'bdp':\n        res = pd.DataFrame(pd.concat([\n            pd.Series({**{'ticker': t}, **grp.set_index('field').value.to_dict()})\n            for t, grp in data.groupby('ticker')\n        ], axis=1, sort=False)).transpose().set_index('ticker')\n    else:\n        res = pd.DataFrame(pd.concat([\n            grp.loc[:, ['name', 'value']].set_index('name')\n            .transpose().reset_index(drop=True).assign(ticker=t)\n            for (t, _), grp in data.groupby(['ticker', 'position'])\n        ], sort=False)).reset_index(drop=True).set_index('ticker')\n        res.columns.name = None\n\n    if col_maps is None: col_maps = dict()\n    return res.rename(\n        columns=lambda vv: col_maps.get(\n            vv, vv.lower().replace(' ', '_').replace('-', '_')\n        )\n    ).apply(pd.to_numeric, errors='ignore', downcast='float')",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def format_intraday(data, ticker, **kwargs) -> ",
        "right": ":\n    \"\"\"\n    Format intraday data\n\n    Args:\n        data: pd.DataFrame from bdib\n        ticker: ticker\n\n    Returns:\n        pd.DataFrame\n\n    Examples:\n        >>> format_intraday(\n        ...     data=pd.read_parquet('xbbg/tests/data/sample_bdib.parq'),\n        ...     ticker='SPY US Equity',\n        ... ).xs('close', axis=1, level=1, drop_level=False)\n        ticker                    SPY US Equity\n        field                             close\n        2018-12-28 09:30:00-05:00        249.67\n        2018-12-28 09:31:00-05:00        249.54\n        2018-12-28 09:32:00-05:00        249.22\n        2018-12-28 09:33:00-05:00        249.01\n        2018-12-28 09:34:00-05:00        248.86\n        >>> format_intraday(\n        ...     data=pd.read_parquet('xbbg/tests/data/sample_bdib.parq'),\n        ...     ticker='SPY US Equity', price_only=True\n        ... )\n        ticker                     SPY US Equity\n        2018-12-28 09:30:00-05:00         249.67\n        2018-12-28 09:31:00-05:00         249.54\n        2018-12-28 09:32:00-05:00         249.22\n        2018-12-28 09:33:00-05:00         249.01\n        2018-12-28 09:34:00-05:00         248.86\n    \"\"\"\n    if data.empty: return pd.DataFrame()\n    data.columns = pd.MultiIndex.from_product([\n        [ticker], data.rename(columns=dict(numEvents='num_trds')).columns\n    ], names=['ticker', 'field'])\n    data.index.name = None\n    if kwargs.get('price_only', False):\n        kw_xs = dict(axis=1, level=1)\n        close = data.xs('close', **kw_xs)\n        volume = data.xs('volume', **kw_xs).iloc[:, 0]\n        return close.loc[volume > 0] if volume.min() > 0 else close\n    else: return data",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def info_qry(tickers, flds) -> ",
        "right": ":\n    \"\"\"\n    Logging info for given tickers and fields\n\n    Args:\n        tickers: tickers\n        flds: fields\n\n    Returns:\n        str\n\n    Examples:\n        >>> print(info_qry(\n        ...     tickers=['NVDA US Equity'], flds=['Name', 'Security_Name']\n        ... ))\n        tickers: ['NVDA US Equity']\n        fields:  ['Name', 'Security_Name']\n    \"\"\"\n    full_list = '\\n'.join([f'tickers: {tickers[:8]}'] + [\n        f'         {tickers[n:(n + 8)]}' for n in range(8, len(tickers), 8)\n    ])\n    return f'{full_list}\\nfields:  {flds}'",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def bdh(\n        tickers, flds=None, start_date=None, end_date='today', adjust=None, **kwargs\n) -> ",
        "right": ":\n    \"\"\"\n    Bloomberg historical data\n\n    Args:\n        tickers: ticker(s)\n        flds: field(s)\n        start_date: start date\n        end_date: end date - default today\n        adjust: `all`, `dvd`, `normal`, `abn` (=abnormal), `split`, `-` or None\n                exact match of above words will adjust for corresponding events\n                Case 0: `-` no adjustment for dividend or split\n                Case 1: `dvd` or `normal|abn` will adjust for all dividends except splits\n                Case 2: `adjust` will adjust for splits and ignore all dividends\n                Case 3: `all` == `dvd|split` == adjust for all\n                Case 4: None == Bloomberg default OR use kwargs\n        **kwargs: overrides\n\n    Returns:\n        pd.DataFrame\n\n    Examples:\n        >>> res = bdh(\n        ...     tickers='VIX Index', flds=['High', 'Low', 'Last_Price'],\n        ...     start_date='2018-02-05', end_date='2018-02-07',\n        ... ).round(2).transpose()\n        >>> res.index.name = None\n        >>> res.columns.name = None\n        >>> res\n                              2018-02-05  2018-02-06  2018-02-07\n        VIX Index High             38.80       50.30       31.64\n                  Low              16.80       22.42       21.17\n                  Last_Price       37.32       29.98       27.73\n        >>> bdh(\n        ...     tickers='AAPL US Equity', flds='Px_Last',\n        ...     start_date='20140605', end_date='20140610', adjust='-'\n        ... ).round(2)\n        ticker     AAPL US Equity\n        field             Px_Last\n        2014-06-05         647.35\n        2014-06-06         645.57\n        2014-06-09          93.70\n        2014-06-10          94.25\n        >>> bdh(\n        ...     tickers='AAPL US Equity', flds='Px_Last',\n        ...     start_date='20140606', end_date='20140609',\n        ...     CshAdjNormal=False, CshAdjAbnormal=False, CapChg=False,\n        ... ).round(2)\n        ticker     AAPL US Equity\n        field             Px_Last\n        2014-06-06         645.57\n        2014-06-09          93.70\n    \"\"\"\n    logger = logs.get_logger(bdh, level=kwargs.pop('log', logs.LOG_LEVEL))\n\n    # Dividend adjustments\n    if isinstance(adjust, str) and adjust:\n        if adjust == 'all':\n            kwargs['CshAdjNormal'] = True\n            kwargs['CshAdjAbnormal'] = True\n            kwargs['CapChg'] = True\n        else:\n            kwargs['CshAdjNormal'] = 'normal' in adjust or 'dvd' in adjust\n            kwargs['CshAdjAbnormal'] = 'abn' in adjust or 'dvd' in adjust\n            kwargs['CapChg'] = 'split' in adjust\n\n    con, _ = create_connection()\n    elms = assist.proc_elms(**kwargs)\n    ovrds = assist.proc_ovrds(**kwargs)\n\n    if isinstance(tickers, str): tickers = [tickers]\n    if flds is None: flds = ['Last_Price']\n    if isinstance(flds, str): flds = [flds]\n    e_dt = utils.fmt_dt(end_date, fmt='%Y%m%d')\n    if start_date is None:\n        start_date = pd.Timestamp(e_dt) - relativedelta(months=3)\n    s_dt = utils.fmt_dt(start_date, fmt='%Y%m%d')\n\n    logger.info(\n        f'loading historical data from Bloomberg:\\n'\n        f'{assist.info_qry(tickers=tickers, flds=flds)}'\n    )\n\n    logger.debug(\n        f'\\nflds={flds}\\nelms={elms}\\novrds={ovrds}\\nstart_date={s_dt}\\nend_date={e_dt}'\n    )\n    res = con.bdh(\n        tickers=tickers, flds=flds, elms=elms, ovrds=ovrds, start_date=s_dt, end_date=e_dt\n    )\n    res.index.name = None\n    if (len(flds) == 1) and kwargs.get('keep_one', False):\n        return res.xs(flds[0], axis=1, level=1)\n    return res",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def intraday(ticker, dt, session='', **kwargs) -> ",
        "right": ":\n    \"\"\"\n    Bloomberg intraday bar data within market session\n\n    Args:\n        ticker: ticker\n        dt: date\n        session: examples include\n                 day_open_30, am_normal_30_30, day_close_30, allday_exact_0930_1000\n        **kwargs:\n            ref: reference ticker or exchange for timezone\n            keep_tz: if keep tz if reference ticker / exchange is given\n            start_time: start time\n            end_time: end time\n            typ: [TRADE, BID, ASK, BID_BEST, ASK_BEST, BEST_BID, BEST_ASK]\n\n    Returns:\n        pd.DataFrame\n    \"\"\"\n    from xbbg.core import intervals\n\n    cur_data = bdib(ticker=ticker, dt=dt, typ=kwargs.get('typ', 'TRADE'))\n    if cur_data.empty: return pd.DataFrame()\n\n    fmt = '%H:%M:%S'\n    ss = intervals.SessNA\n    ref = kwargs.get('ref', None)\n    exch = pd.Series() if ref is None else const.exch_info(ticker=ref)\n    if session: ss = intervals.get_interval(\n        ticker=kwargs.get('ref', ticker), session=session\n    )\n\n    start_time = kwargs.get('start_time', None)\n    end_time = kwargs.get('end_time', None)\n    if ss != intervals.SessNA:\n        start_time = pd.Timestamp(ss.start_time).strftime(fmt)\n        end_time = pd.Timestamp(ss.end_time).strftime(fmt)\n\n    if start_time and end_time:\n        kw = dict(start_time=start_time, end_time=end_time)\n        if not exch.empty:\n            cur_tz = cur_data.index.tz\n            res = cur_data.tz_convert(exch.tz).between_time(**kw)\n            if kwargs.get('keep_tz', False):\n                res = res.tz_convert(cur_tz)\n            return pd.DataFrame(res)\n        return pd.DataFrame(cur_data.between_time(**kw))\n\n    return cur_data",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def earning(\n        ticker, by='Geo', typ='Revenue', ccy=None, level=None, **kwargs\n) -> ",
        "right": ":\n    \"\"\"\n    Earning exposures by Geo or Products\n\n    Args:\n        ticker: ticker name\n        by: [G(eo), P(roduct)]\n        typ: type of earning, start with `PG_` in Bloomberg FLDS - default `Revenue`\n        ccy: currency of earnings\n        level: hierarchy level of earnings\n\n    Returns:\n        pd.DataFrame\n\n    Examples:\n        >>> data = earning('AMD US Equity', Eqy_Fund_Year=2017, Number_Of_Periods=1)\n        >>> data.round(2)\n                         level  fy2017  fy2017_pct\n        Asia-Pacific       1.0  3540.0       66.43\n        \u00a0\u00a0\u00a0China           2.0  1747.0       49.35\n        \u00a0\u00a0\u00a0Japan           2.0  1242.0       35.08\n        \u00a0\u00a0\u00a0Singapore       2.0   551.0       15.56\n        United States      1.0  1364.0       25.60\n        Europe             1.0   263.0        4.94\n        Other Countries    1.0   162.0        3.04\n    \"\"\"\n    ovrd = 'G' if by[0].upper() == 'G' else 'P'\n    new_kw = dict(raw=True, Product_Geo_Override=ovrd)\n    header = bds(tickers=ticker, flds='PG_Bulk_Header', **new_kw, **kwargs)\n    if ccy: kwargs['Eqy_Fund_Crncy'] = ccy\n    if level: kwargs['PG_Hierarchy_Level'] = level\n    data = bds(tickers=ticker, flds=f'PG_{typ}', **new_kw, **kwargs)\n    return assist.format_earning(data=data, header=header)",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def active_futures(ticker, dt) -> ",
        "right": ":\n    \"\"\"\n    Active futures contract\n\n    Args:\n        ticker: futures ticker, i.e., ESA Index, Z A Index, CLA Comdty, etc.\n        dt: date\n\n    Returns:\n        str: ticker name\n    \"\"\"\n    t_info = ticker.split()\n    prefix, asset = ' '.join(t_info[:-1]), t_info[-1]\n    info = const.market_info(f'{prefix[:-1]}1 {asset}')\n\n    f1, f2 = f'{prefix[:-1]}1 {asset}', f'{prefix[:-1]}2 {asset}'\n    fut_2 = fut_ticker(gen_ticker=f2, dt=dt, freq=info['freq'])\n    fut_1 = fut_ticker(gen_ticker=f1, dt=dt, freq=info['freq'])\n\n    fut_tk = bdp(tickers=[fut_1, fut_2], flds='Last_Tradeable_Dt', cache=True)\n\n    if pd.Timestamp(dt).month < pd.Timestamp(fut_tk.last_tradeable_dt[0]).month: return fut_1\n\n    d1 = bdib(ticker=f1, dt=dt)\n    d2 = bdib(ticker=f2, dt=dt)\n\n    return fut_1 if d1[f1].volume.sum() > d2[f2].volume.sum() else fut_2",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def fut_ticker(gen_ticker, dt, freq, log=logs.LOG_LEVEL) -> ",
        "right": ":\n    \"\"\"\n    Get proper ticker from generic ticker\n\n    Args:\n        gen_ticker: generic ticker\n        dt: date\n        freq: futures contract frequency\n        log: level of logs\n\n    Returns:\n        str: exact futures ticker\n    \"\"\"\n    logger = logs.get_logger(fut_ticker, level=log)\n    dt = pd.Timestamp(dt)\n    t_info = gen_ticker.split()\n\n    asset = t_info[-1]\n    if asset in ['Index', 'Curncy', 'Comdty']:\n        ticker = ' '.join(t_info[:-1])\n        prefix, idx, postfix = ticker[:-1], int(ticker[-1]) - 1, asset\n\n    elif asset == 'Equity':\n        ticker = t_info[0]\n        prefix, idx, postfix = ticker[:-1], int(ticker[-1]) - 1, ' '.join(t_info[1:])\n\n    else:\n        logger.error(f'unkonwn asset type for ticker: {gen_ticker}')\n        return ''\n\n    month_ext = 4 if asset == 'Comdty' else 2\n    months = pd.date_range(start=dt, periods=max(idx + month_ext, 3), freq=freq)\n    logger.debug(f'pulling expiry dates for months: {months}')\n\n    def to_fut(month):\n        return prefix + const.Futures[month.strftime('%b')] + \\\n               month.strftime('%y')[-1] + ' ' + postfix\n\n    fut = [to_fut(m) for m in months]\n    logger.debug(f'trying futures: {fut}')\n    # noinspection PyBroadException\n    try:\n        fut_matu = bdp(tickers=fut, flds='last_tradeable_dt', cache=True)\n    except Exception as e1:\n        logger.error(f'error downloading futures contracts (1st trial) {e1}:\\n{fut}')\n        # noinspection PyBroadException\n        try:\n            fut = fut[:-1]\n            logger.debug(f'trying futures (2nd trial): {fut}')\n            fut_matu = bdp(tickers=fut, flds='last_tradeable_dt', cache=True)\n        except Exception as e2:\n            logger.error(f'error downloading futures contracts (2nd trial) {e2}:\\n{fut}')\n            return ''\n\n    sub_fut = fut_matu[pd.DatetimeIndex(fut_matu.last_tradeable_dt) > dt]\n    logger.debug(f'futures full chain:\\n{fut_matu.to_string()}')\n    logger.debug(f'getting index {idx} from:\\n{sub_fut.to_string()}')\n    return sub_fut.index.values[idx]",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def check_hours(tickers, tz_exch, tz_loc=DEFAULT_TZ) -> ",
        "right": ":\n    \"\"\"\n    Check exchange hours vs local hours\n\n    Args:\n        tickers: list of tickers\n        tz_exch: exchange timezone\n        tz_loc: local timezone\n\n    Returns:\n        Local and exchange hours\n    \"\"\"\n    cols = ['Trading_Day_Start_Time_EOD', 'Trading_Day_End_Time_EOD']\n    con, _ = create_connection()\n    hours = con.ref(tickers=tickers, flds=cols)\n    cur_dt = pd.Timestamp('today').strftime('%Y-%m-%d ')\n    hours.loc[:, 'local'] = hours.value.astype(str).str[:-3]\n    hours.loc[:, 'exch'] = pd.DatetimeIndex(\n        cur_dt + hours.value.astype(str)\n    ).tz_localize(tz_loc).tz_convert(tz_exch).strftime('%H:%M')\n\n    hours = pd.concat([\n        hours.set_index(['ticker', 'field']).exch.unstack().loc[:, cols],\n        hours.set_index(['ticker', 'field']).local.unstack().loc[:, cols],\n    ], axis=1)\n    hours.columns = ['Exch_Start', 'Exch_End', 'Local_Start', 'Local_End']\n\n    return hours",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def hist_file(ticker, dt, typ='TRADE') -> ",
        "right": ":\n    \"\"\"\n    Data file location for Bloomberg historical data\n\n    Args:\n        ticker: ticker name\n        dt: date\n        typ: [TRADE, BID, ASK, BID_BEST, ASK_BEST, BEST_BID, BEST_ASK]\n\n    Returns:\n        file location\n\n    Examples:\n        >>> os.environ['BBG_ROOT'] = ''\n        >>> hist_file(ticker='ES1 Index', dt='2018-08-01') == ''\n        True\n        >>> os.environ['BBG_ROOT'] = '/data/bbg'\n        >>> hist_file(ticker='ES1 Index', dt='2018-08-01')\n        '/data/bbg/Index/ES1 Index/TRADE/2018-08-01.parq'\n    \"\"\"\n    data_path = os.environ.get(assist.BBG_ROOT, '').replace('\\\\', '/')\n    if not data_path: return ''\n    asset = ticker.split()[-1]\n    proper_ticker = ticker.replace('/', '_')\n    cur_dt = pd.Timestamp(dt).strftime('%Y-%m-%d')\n    return f'{data_path}/{asset}/{proper_ticker}/{typ}/{cur_dt}.parq'",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def ref_file(\n        ticker, fld, has_date=False, cache=False, ext='parq', **kwargs\n) -> ",
        "right": ":\n    \"\"\"\n    Data file location for Bloomberg reference data\n\n    Args:\n        ticker: ticker name\n        fld: field\n        has_date: whether add current date to data file\n        cache: if has_date is True, whether to load file from latest cached\n        ext: file extension\n        **kwargs: other overrides passed to ref function\n\n    Returns:\n        file location\n\n    Examples:\n        >>> import shutil\n        >>>\n        >>> os.environ['BBG_ROOT'] = ''\n        >>> ref_file('BLT LN Equity', fld='Crncy') == ''\n        True\n        >>> os.environ['BBG_ROOT'] = '/data/bbg'\n        >>> ref_file('BLT LN Equity', fld='Crncy', cache=True)\n        '/data/bbg/Equity/BLT LN Equity/Crncy/ovrd=None.parq'\n        >>> ref_file('BLT LN Equity', fld='Crncy')\n        ''\n        >>> cur_dt = utils.cur_time(tz=utils.DEFAULT_TZ)\n        >>> ref_file(\n        ...     'BLT LN Equity', fld='DVD_Hist_All', has_date=True, cache=True,\n        ... ).replace(cur_dt, '[cur_date]')\n        '/data/bbg/Equity/BLT LN Equity/DVD_Hist_All/asof=[cur_date], ovrd=None.parq'\n        >>> ref_file(\n        ...     'BLT LN Equity', fld='DVD_Hist_All', has_date=True,\n        ...     cache=True, DVD_Start_Dt='20180101',\n        ... ).replace(cur_dt, '[cur_date]')[:-5]\n        '/data/bbg/Equity/BLT LN Equity/DVD_Hist_All/asof=[cur_date], DVD_Start_Dt=20180101'\n        >>> sample = 'asof=2018-11-02, DVD_Start_Dt=20180101, DVD_End_Dt=20180501.pkl'\n        >>> root_path = 'xbbg/tests/data'\n        >>> sub_path = f'{root_path}/Equity/AAPL US Equity/DVD_Hist_All'\n        >>> os.environ['BBG_ROOT'] = root_path\n        >>> for tmp_file in files.all_files(sub_path): os.remove(tmp_file)\n        >>> files.create_folder(sub_path)\n        >>> sample in shutil.copy(f'{root_path}/{sample}', sub_path)\n        True\n        >>> new_file = ref_file(\n        ...     'AAPL US Equity', 'DVD_Hist_All', DVD_Start_Dt='20180101',\n        ...     has_date=True, cache=True, ext='pkl'\n        ... )\n        >>> new_file.split('/')[-1] == f'asof={cur_dt}, DVD_Start_Dt=20180101.pkl'\n        True\n        >>> old_file = 'asof=2018-11-02, DVD_Start_Dt=20180101, DVD_End_Dt=20180501.pkl'\n        >>> old_full = '/'.join(new_file.split('/')[:-1] + [old_file])\n        >>> updated_file = old_full.replace('2018-11-02', cur_dt)\n        >>> updated_file in shutil.copy(old_full, updated_file)\n        True\n        >>> exist_file = ref_file(\n        ...     'AAPL US Equity', 'DVD_Hist_All', DVD_Start_Dt='20180101',\n        ...     has_date=True, cache=True, ext='pkl'\n        ... )\n        >>> exist_file == updated_file\n        False\n        >>> exist_file = ref_file(\n        ...     'AAPL US Equity', 'DVD_Hist_All', DVD_Start_Dt='20180101',\n        ...     DVD_End_Dt='20180501', has_date=True, cache=True, ext='pkl'\n        ... )\n        >>> exist_file == updated_file\n        True\n    \"\"\"\n    data_path = os.environ.get(assist.BBG_ROOT, '').replace('\\\\', '/')\n    if (not data_path) or (not cache): return ''\n\n    proper_ticker = ticker.replace('/', '_')\n    cache_days = kwargs.pop('cache_days', 10)\n    root = f'{data_path}/{ticker.split()[-1]}/{proper_ticker}/{fld}'\n\n    if len(kwargs) > 0: info = utils.to_str(kwargs)[1:-1].replace('|', '_')\n    else: info = 'ovrd=None'\n\n    # Check date info\n    if has_date:\n        cur_dt = utils.cur_time()\n        missing = f'{root}/asof={cur_dt}, {info}.{ext}'\n        to_find = re.compile(rf'{root}/asof=(.*), {info}\\.pkl')\n        cur_files = list(filter(to_find.match, sorted(\n            files.all_files(path_name=root, keyword=info, ext=ext)\n        )))\n        if len(cur_files) > 0:\n            upd_dt = to_find.match(cur_files[-1]).group(1)\n            diff = pd.Timestamp('today') - pd.Timestamp(upd_dt)\n            if diff >= pd.Timedelta(days=cache_days): return missing\n            return sorted(cur_files)[-1]\n        else: return missing\n\n    else: return f'{root}/{info}.{ext}'",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def exch_info(ticker) -> ",
        "right": ":\n    \"\"\"\n    Exchange info for given ticker\n\n    Args:\n        ticker: ticker or exchange\n\n    Returns:\n        pd.Series\n\n    Examples:\n        >>> exch_info('SPY US Equity')\n        tz        America/New_York\n        allday      [04:00, 20:00]\n        day         [09:30, 16:00]\n        pre         [04:00, 09:30]\n        post        [16:01, 20:00]\n        dtype: object\n        >>> exch_info('ES1 Index')\n        tz        America/New_York\n        allday      [18:00, 17:00]\n        day         [08:00, 17:00]\n        dtype: object\n        >>> exch_info('Z 1 Index')\n        tz         Europe/London\n        allday    [01:00, 21:00]\n        day       [01:00, 21:00]\n        dtype: object\n        >>> exch_info('TESTTICKER Corp').empty\n        True\n        >>> exch_info('US')\n        tz        America/New_York\n        allday      [04:00, 20:00]\n        day         [09:30, 16:00]\n        pre         [04:00, 09:30]\n        post        [16:01, 20:00]\n        dtype: object\n    \"\"\"\n    logger = logs.get_logger(exch_info, level='debug')\n    if ' ' not in ticker.strip():\n        ticker = f'XYZ {ticker.strip()} Equity'\n    info = param.load_info(cat='exch').get(\n        market_info(ticker=ticker).get('exch', ''), dict()\n    )\n    if ('allday' in info) and ('day' not in info):\n        info['day'] = info['allday']\n\n    if any(req not in info for req in ['tz', 'allday', 'day']):\n        logger.error(f'required exchange info cannot be found in {ticker} ...')\n        return pd.Series()\n\n    for ss in ValidSessions:\n        if ss not in info: continue\n        info[ss] = [param.to_hour(num=s) for s in info[ss]]\n\n    return pd.Series(info)",
        "return_type_from_source": "pd.Series"
    },
    {
        "extra_left": [],
        "left": "def market_info(ticker) -> ",
        "right": ":\n    \"\"\"\n    Get info for given market\n\n    Args:\n        ticker: Bloomberg full ticker\n\n    Returns:\n        dict\n\n    Examples:\n        >>> info = market_info('SHCOMP Index')\n        >>> info['exch']\n        'EquityChina'\n        >>> info = market_info('ICICIC=1 IS Equity')\n        >>> info['freq'], info['is_fut']\n        ('M', True)\n        >>> info = market_info('INT1 Curncy')\n        >>> info['freq'], info['is_fut']\n        ('M', True)\n        >>> info = market_info('CL1 Comdty')\n        >>> info['freq'], info['is_fut']\n        ('M', True)\n        >>> # Wrong tickers\n        >>> market_info('C XX Equity')\n        {}\n        >>> market_info('XXX Comdty')\n        {}\n        >>> market_info('Bond_ISIN Corp')\n        {}\n        >>> market_info('XYZ Index')\n        {}\n        >>> market_info('XYZ Curncy')\n        {}\n    \"\"\"\n    t_info = ticker.split()\n    assets = param.load_info('assets')\n\n    # ========================== #\n    #           Equity           #\n    # ========================== #\n\n    if (t_info[-1] == 'Equity') and ('=' not in t_info[0]):\n        exch = t_info[-2]\n        for info in assets.get('Equity', [dict()]):\n            if 'exch_codes' not in info: continue\n            if exch in info['exch_codes']: return info\n        return dict()\n\n    # ============================ #\n    #           Currency           #\n    # ============================ #\n\n    if t_info[-1] == 'Curncy':\n        for info in assets.get('Curncy', [dict()]):\n            if 'tickers' not in info: continue\n            if (t_info[0].split('+')[0] in info['tickers']) or \\\n                    (t_info[0][-1].isdigit() and (t_info[0][:-1] in info['tickers'])):\n                return info\n        return dict()\n\n    if t_info[-1] == 'Comdty':\n        for info in assets.get('Comdty', [dict()]):\n            if 'tickers' not in info: continue\n            if t_info[0][:-1] in info['tickers']: return info\n        return dict()\n\n    # =================================== #\n    #           Index / Futures           #\n    # =================================== #\n\n    if (t_info[-1] == 'Index') or (\n        (t_info[-1] == 'Equity') and ('=' in t_info[0])\n    ):\n        if t_info[-1] == 'Equity':\n            tck = t_info[0].split('=')[0]\n        else:\n            tck = ' '.join(t_info[:-1])\n        for info in assets.get('Index', [dict()]):\n            if 'tickers' not in info: continue\n            if (tck[:2] == 'UX') and ('UX' in info['tickers']): return info\n            if tck in info['tickers']:\n                if t_info[-1] == 'Equity': return info\n                if not info.get('is_fut', False): return info\n            if tck[:-1].rstrip() in info['tickers']:\n                if info.get('is_fut', False): return info\n        return dict()\n\n    if t_info[-1] == 'Corp':\n        for info in assets.get('Corp', [dict()]):\n            if 'ticker' not in info: continue\n\n    return dict()",
        "return_type_from_source": "dict"
    },
    {
        "extra_left": [],
        "left": "def ccy_pair(local, base='USD') -> ",
        "right": ":\n    \"\"\"\n    Currency pair info\n\n    Args:\n        local: local currency\n        base: base currency\n\n    Returns:\n        CurrencyPair\n\n    Examples:\n        >>> ccy_pair(local='HKD', base='USD')\n        CurrencyPair(ticker='HKD Curncy', factor=1.0, power=1)\n        >>> ccy_pair(local='GBp')\n        CurrencyPair(ticker='GBP Curncy', factor=100, power=-1)\n        >>> ccy_pair(local='USD', base='GBp')\n        CurrencyPair(ticker='GBP Curncy', factor=0.01, power=1)\n        >>> ccy_pair(local='XYZ', base='USD')\n        CurrencyPair(ticker='', factor=1.0, power=1)\n        >>> ccy_pair(local='GBP', base='GBp')\n        CurrencyPair(ticker='', factor=0.01, power=1)\n        >>> ccy_pair(local='GBp', base='GBP')\n        CurrencyPair(ticker='', factor=100.0, power=1)\n    \"\"\"\n    ccy_param = param.load_info(cat='ccy')\n    if f'{local}{base}' in ccy_param:\n        info = ccy_param[f'{local}{base}']\n\n    elif f'{base}{local}' in ccy_param:\n        info = ccy_param[f'{base}{local}']\n        info['factor'] = 1. / info.get('factor', 1.)\n        info['power'] = -info.get('power', 1)\n\n    elif base.lower() == local.lower():\n        info = dict(ticker='')\n        info['factor'] = 1.\n        if base[-1].lower() == base[-1]:\n            info['factor'] /= 100.\n        if local[-1].lower() == local[-1]:\n            info['factor'] *= 100.\n\n    else:\n        logger = logs.get_logger(ccy_pair)\n        logger.error(f'incorrect currency - local {local} / base {base}')\n        return CurrencyPair(ticker='', factor=1., power=1)\n\n    if 'factor' not in info: info['factor'] = 1.\n    if 'power' not in info: info['power'] = 1\n    return CurrencyPair(**info)",
        "return_type_from_source": "CurrencyPair"
    },
    {
        "extra_left": [],
        "left": "def market_timing(ticker, dt, timing='EOD', tz='local') -> ",
        "right": ":\n    \"\"\"\n    Market close time for ticker\n\n    Args:\n        ticker: ticker name\n        dt: date\n        timing: [EOD (default), BOD]\n        tz: conversion to timezone\n\n    Returns:\n        str: date & time\n\n    Examples:\n        >>> market_timing('7267 JT Equity', dt='2018-09-10')\n        '2018-09-10 14:58'\n        >>> market_timing('7267 JT Equity', dt='2018-09-10', tz=timezone.TimeZone.NY)\n        '2018-09-10 01:58:00-04:00'\n        >>> market_timing('7267 JT Equity', dt='2018-01-10', tz='NY')\n        '2018-01-10 00:58:00-05:00'\n        >>> market_timing('7267 JT Equity', dt='2018-09-10', tz='SPX Index')\n        '2018-09-10 01:58:00-04:00'\n        >>> market_timing('8035 JT Equity', dt='2018-09-10', timing='BOD')\n        '2018-09-10 09:01'\n        >>> market_timing('Z 1 Index', dt='2018-09-10', timing='FINISHED')\n        '2018-09-10 21:00'\n        >>> market_timing('TESTTICKER Corp', dt='2018-09-10')\n        ''\n    \"\"\"\n    logger = logs.get_logger(market_timing)\n    exch = pd.Series(exch_info(ticker=ticker))\n    if any(req not in exch.index for req in ['tz', 'allday', 'day']):\n        logger.error(f'required exchange info cannot be found in {ticker} ...')\n        return ''\n\n    mkt_time = {\n        'BOD': exch.day[0], 'FINISHED': exch.allday[-1]\n    }.get(timing, exch.day[-1])\n\n    cur_dt = pd.Timestamp(str(dt)).strftime('%Y-%m-%d')\n    if tz == 'local':\n        return f'{cur_dt} {mkt_time}'\n\n    return timezone.tz_convert(f'{cur_dt} {mkt_time}', to_tz=tz, from_tz=exch.tz)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def flatten(iterable, maps=None, unique=False) -> ",
        "right": ":\n    \"\"\"\n    Flatten any array of items to list\n\n    Args:\n        iterable: any array or value\n        maps: map items to values\n        unique: drop duplicates\n\n    Returns:\n        list: flattened list\n\n    References:\n        https://stackoverflow.com/a/40857703/1332656\n\n    Examples:\n        >>> flatten('abc')\n        ['abc']\n        >>> flatten(1)\n        [1]\n        >>> flatten(1.)\n        [1.0]\n        >>> flatten(['ab', 'cd', ['xy', 'zz']])\n        ['ab', 'cd', 'xy', 'zz']\n        >>> flatten(['ab', ['xy', 'zz']], maps={'xy': '0x'})\n        ['ab', '0x', 'zz']\n    \"\"\"\n    if iterable is None: return []\n    if maps is None: maps = dict()\n\n    if isinstance(iterable, (str, int, float)):\n        return [maps.get(iterable, iterable)]\n\n    else:\n        x = [maps.get(item, item) for item in _to_gen_(iterable)]\n        return list(set(x)) if unique else x",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def to_str(\n        data, fmt='{key}={value}', sep=', ', public_only=True\n) -> ",
        "right": ":\n    \"\"\"\n    Convert dict to string\n\n    Args:\n        data: dict\n        fmt: how key and value being represented\n        sep: how pairs of key and value are seperated\n        public_only: if display public members only\n\n    Returns:\n        str: string representation of dict\n\n    Examples:\n        >>> test_dict = dict(b=1, a=0, c=2, _d=3)\n        >>> to_str(test_dict)\n        '{b=1, a=0, c=2}'\n        >>> to_str(test_dict, sep='|')\n        '{b=1|a=0|c=2}'\n        >>> to_str(test_dict, public_only=False)\n        '{b=1, a=0, c=2, _d=3}'\n    \"\"\"\n    if public_only: keys = list(filter(lambda vv: vv[0] != '_', data.keys()))\n    else: keys = list(data.keys())\n    return '{' + sep.join([\n        to_str(data=v, fmt=fmt, sep=sep)\n        if isinstance(v, dict) else fstr(fmt=fmt, key=k, value=v)\n        for k, v in data.items() if k in keys\n    ]) + '}'",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def to_hour(num) -> ",
        "right": ":\n    \"\"\"\n    Convert YAML input to hours\n\n    Args:\n        num: number in YMAL file, e.g., 900, 1700, etc.\n\n    Returns:\n        str\n\n    Examples:\n        >>> to_hour(900)\n        '09:00'\n        >>> to_hour(1700)\n        '17:00'\n    \"\"\"\n    to_str = str(int(num))\n    return pd.Timestamp(f'{to_str[:-2]}:{to_str[-2:]}').strftime('%H:%M')",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def all_files(\n        path_name, keyword='', ext='', full_path=True,\n        has_date=False, date_fmt=DATE_FMT\n) -> ",
        "right": ":\n    \"\"\"\n    Search all files with criteria\n    Returned list will be sorted by last modified\n\n    Args:\n        path_name: full path name\n        keyword: keyword to search\n        ext: file extensions, split by ','\n        full_path: whether return full path (default True)\n        has_date: whether has date in file name (default False)\n        date_fmt: date format to check for has_date parameter\n\n    Returns:\n        list: all file names with criteria fulfilled\n    \"\"\"\n    if not os.path.exists(path=path_name): return []\n    path_name = path_name.replace('\\\\', '/')\n\n    if keyword or ext:\n        keyword = f'*{keyword}*' if keyword else '*'\n        if not ext: ext = '*'\n        files = sort_by_modified([\n            f.replace('\\\\', '/') for f in glob.iglob(f'{path_name}/{keyword}.{ext}')\n            if os.path.isfile(f) and (f.replace('\\\\', '/').split('/')[-1][0] != '~')\n        ])\n\n    else:\n        files = sort_by_modified([\n            f'{path_name}/{f}' for f in os.listdir(path=path_name)\n            if os.path.isfile(f'{path_name}/{f}') and (f[0] != '~')\n        ])\n\n    if has_date:\n        files = filter_by_dates(files, date_fmt=date_fmt)\n\n    return files if full_path else [f.split('/')[-1] for f in files]",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def all_folders(\n        path_name, keyword='', has_date=False, date_fmt=DATE_FMT\n) -> ",
        "right": ":\n    \"\"\"\n    Search all folders with criteria\n    Returned list will be sorted by last modified\n\n    Args:\n        path_name: full path name\n        keyword: keyword to search\n        has_date: whether has date in file name (default False)\n        date_fmt: date format to check for has_date parameter\n\n    Returns:\n        list: all folder names fulfilled criteria\n    \"\"\"\n    if not os.path.exists(path=path_name): return []\n    path_name = path_name.replace('\\\\', '/')\n\n    if keyword:\n        folders = sort_by_modified([\n            f.replace('\\\\', '/') for f in glob.iglob(f'{path_name}/*{keyword}*')\n            if os.path.isdir(f) and (f.replace('\\\\', '/').split('/')[-1][0] != '~')\n        ])\n\n    else:\n        folders = sort_by_modified([\n            f'{path_name}/{f}' for f in os.listdir(path=path_name)\n            if os.path.isdir(f'{path_name}/{f}') and (f[0] != '~')\n        ])\n\n    if has_date:\n        folders = filter_by_dates(folders, date_fmt=date_fmt)\n\n    return folders",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def sort_by_modified(files_or_folders) -> ",
        "right": ":\n    \"\"\"\n    Sort files or folders by modified time\n\n    Args:\n        files_or_folders: list of files or folders\n\n    Returns:\n        list\n    \"\"\"\n    return sorted(files_or_folders, key=os.path.getmtime, reverse=True)",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def filter_by_dates(files_or_folders, date_fmt=DATE_FMT) -> ",
        "right": ":\n    \"\"\"\n    Filter files or dates by date patterns\n\n    Args:\n        files_or_folders: list of files or folders\n        date_fmt: date format\n\n    Returns:\n        list\n    \"\"\"\n    r = re.compile(f'.*{date_fmt}.*')\n    return list(filter(\n        lambda vv: r.match(vv.replace('\\\\', '/').split('/')[-1]) is not None,\n        files_or_folders,\n    ))",
        "return_type_from_source": "list"
    },
    {
        "extra_left": [],
        "left": "def file_modified_time(file_name) -> ",
        "right": ":\n    \"\"\"\n    File modified time in python\n\n    Args:\n        file_name: file name\n\n    Returns:\n        pd.Timestamp\n    \"\"\"\n    return pd.to_datetime(time.ctime(os.path.getmtime(filename=file_name)))",
        "return_type_from_source": "pd.Timestamp"
    },
    {
        "extra_left": [],
        "left": "def get_interval(ticker, session) -> ",
        "right": ":\n    \"\"\"\n    Get interval from defined session\n\n    Args:\n        ticker: ticker\n        session: session\n\n    Returns:\n        Session of start_time and end_time\n\n    Examples:\n        >>> get_interval('005490 KS Equity', 'day_open_30')\n        Session(start_time='09:00', end_time='09:30')\n        >>> get_interval('005490 KS Equity', 'day_normal_30_20')\n        Session(start_time='09:31', end_time='15:00')\n        >>> get_interval('005490 KS Equity', 'day_close_20')\n        Session(start_time='15:01', end_time='15:20')\n        >>> get_interval('700 HK Equity', 'am_open_30')\n        Session(start_time='09:30', end_time='10:00')\n        >>> get_interval('700 HK Equity', 'am_normal_30_30')\n        Session(start_time='10:01', end_time='11:30')\n        >>> get_interval('700 HK Equity', 'am_close_30')\n        Session(start_time='11:31', end_time='12:00')\n        >>> get_interval('ES1 Index', 'day_exact_2130_2230')\n        Session(start_time=None, end_time=None)\n        >>> get_interval('ES1 Index', 'allday_exact_2130_2230')\n        Session(start_time='21:30', end_time='22:30')\n        >>> get_interval('ES1 Index', 'allday_exact_2130_0230')\n        Session(start_time='21:30', end_time='02:30')\n        >>> get_interval('AMLP US', 'day_open_30')\n        Session(start_time=None, end_time=None)\n        >>> get_interval('7974 JP Equity', 'day_normal_180_300') is SessNA\n        True\n        >>> get_interval('Z 1 Index', 'allday_normal_30_30')\n        Session(start_time='01:31', end_time='20:30')\n        >>> get_interval('GBP Curncy', 'day')\n        Session(start_time='17:02', end_time='17:00')\n    \"\"\"\n    if '_' not in session:\n        session = f'{session}_normal_0_0'\n    interval = Intervals(ticker=ticker)\n    ss_info = session.split('_')\n    return getattr(interval, f'market_{ss_info.pop(1)}')(*ss_info)",
        "return_type_from_source": "Session"
    },
    {
        "extra_left": [],
        "left": "def shift_time(start_time, mins) -> ",
        "right": ":\n    \"\"\"\n    Shift start time by mins\n\n    Args:\n        start_time: start time in terms of HH:MM string\n        mins: number of minutes (+ / -)\n\n    Returns:\n        end time in terms of HH:MM string\n    \"\"\"\n    s_time = pd.Timestamp(start_time)\n    e_time = s_time + np.sign(mins) * pd.Timedelta(f'00:{abs(mins)}:00')\n    return e_time.strftime('%H:%M')",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def market_open(self, session, mins) -> ",
        "right": ":\n        \"\"\"\n        Time intervals for market open\n\n        Args:\n            session: [allday, day, am, pm, night]\n            mins: mintues after open\n\n        Returns:\n            Session of start_time and end_time\n        \"\"\"\n        if session not in self.exch: return SessNA\n        start_time = self.exch[session][0]\n        return Session(start_time, shift_time(start_time, int(mins)))",
        "return_type_from_source": "Session"
    },
    {
        "extra_left": [],
        "left": "def market_close(self, session, mins) -> ",
        "right": ":\n        \"\"\"\n        Time intervals for market close\n\n        Args:\n            session: [allday, day, am, pm, night]\n            mins: mintues before close\n\n        Returns:\n            Session of start_time and end_time\n        \"\"\"\n        if session not in self.exch: return SessNA\n        end_time = self.exch[session][-1]\n        return Session(shift_time(end_time, -int(mins) + 1), end_time)",
        "return_type_from_source": "Session"
    },
    {
        "extra_left": [],
        "left": "def market_normal(self, session, after_open, before_close) -> ",
        "right": ":\n        \"\"\"\n        Time intervals between market\n\n        Args:\n            session: [allday, day, am, pm, night]\n            after_open: mins after open\n            before_close: mins before close\n\n        Returns:\n            Session of start_time and end_time\n        \"\"\"\n        logger = logs.get_logger(self.market_normal)\n\n        if session not in self.exch: return SessNA\n        ss = self.exch[session]\n\n        s_time = shift_time(ss[0], int(after_open) + 1)\n        e_time = shift_time(ss[-1], -int(before_close))\n\n        request_cross = pd.Timestamp(s_time) >= pd.Timestamp(e_time)\n        session_cross = pd.Timestamp(ss[0]) >= pd.Timestamp(ss[1])\n        if request_cross and (not session_cross):\n            logger.warning(f'end time {e_time} is earlier than {s_time} ...')\n            return SessNA\n\n        return Session(s_time, e_time)",
        "return_type_from_source": "Session"
    },
    {
        "extra_left": [],
        "left": "def market_exact(self, session, start_time, end_time) -> ",
        "right": ":\n        \"\"\"\n        Explicitly specify start time and end time\n\n        Args:\n            session: predefined session\n            start_time: start time in terms of HHMM string\n            end_time: end time in terms of HHMM string\n\n        Returns:\n            Session of start_time and end_time\n        \"\"\"\n        if session not in self.exch: return SessNA\n        ss = self.exch[session]\n\n        same_day = ss[0] < ss[-1]\n\n        if not start_time: s_time = ss[0]\n        else:\n            s_time = param.to_hour(start_time)\n            if same_day: s_time = max(s_time, ss[0])\n\n        if not end_time: e_time = ss[-1]\n        else:\n            e_time = param.to_hour(end_time)\n            if same_day: e_time = min(e_time, ss[-1])\n\n        if same_day and (s_time > e_time): return SessNA\n        return Session(start_time=s_time, end_time=e_time)",
        "return_type_from_source": "Session"
    },
    {
        "extra_left": [],
        "left": "def tz_convert(dt, to_tz, from_tz=None) -> ",
        "right": ":\n    \"\"\"\n    Convert to tz\n\n    Args:\n        dt: date time\n        to_tz: to tz\n        from_tz: from tz - will be ignored if tz from dt is given\n\n    Returns:\n        str: date & time\n\n    Examples:\n        >>> dt_1 = pd.Timestamp('2018-09-10 16:00', tz='Asia/Hong_Kong')\n        >>> tz_convert(dt_1, to_tz='NY')\n        '2018-09-10 04:00:00-04:00'\n        >>> dt_2 = pd.Timestamp('2018-01-10 16:00')\n        >>> tz_convert(dt_2, to_tz='HK', from_tz='NY')\n        '2018-01-11 05:00:00+08:00'\n        >>> dt_3 = '2018-09-10 15:00'\n        >>> tz_convert(dt_3, to_tz='NY', from_tz='JP')\n        '2018-09-10 02:00:00-04:00'\n    \"\"\"\n    logger = logs.get_logger(tz_convert, level='info')\n    f_tz, t_tz = get_tz(from_tz), get_tz(to_tz)\n\n    from_dt = pd.Timestamp(str(dt), tz=f_tz)\n    logger.debug(f'converting {str(from_dt)} from {f_tz} to {t_tz} ...')\n    return str(pd.Timestamp(str(from_dt), tz=t_tz))",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def missing_info(**kwargs) -> ",
        "right": ":\n    \"\"\"\n    Full infomation for missing query\n    \"\"\"\n    func = kwargs.pop('func', 'unknown')\n    if 'ticker' in kwargs: kwargs['ticker'] = kwargs['ticker'].replace('/', '_')\n    info = utils.to_str(kwargs, fmt='{value}', sep='/')[1:-1]\n    return f'{func}/{info}'",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def current_missing(**kwargs) -> ",
        "right": ":\n    \"\"\"\n    Check number of trials for missing values\n\n    Returns:\n        int: number of trials already tried\n    \"\"\"\n    data_path = os.environ.get(BBG_ROOT, '').replace('\\\\', '/')\n    if not data_path: return 0\n    return len(files.all_files(f'{data_path}/Logs/{missing_info(**kwargs)}'))",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def plane_xz(size=(10, 10), resolution=(10, 10)) -> ",
        "right": ":\n    \"\"\"\n    Generates a plane on the xz axis of a specific size and resolution.\n    Normals and texture coordinates are also included.\n\n    Args:\n        size: (x, y) tuple\n        resolution: (x, y) tuple\n\n    Returns:\n        A :py:class:`demosys.opengl.vao.VAO` instance\n    \"\"\"\n    sx, sz = size\n    rx, rz = resolution\n    dx, dz = sx / rx, sz / rz  # step\n    ox, oz = -sx / 2, -sz / 2  # start offset\n\n    def gen_pos():\n        for z in range(rz):\n            for x in range(rx):\n                yield ox + x * dx\n                yield 0\n                yield oz + z * dz\n\n    def gen_uv():\n        for z in range(rz):\n            for x in range(rx):\n                yield x / (rx - 1)\n                yield 1 - z / (rz - 1)\n\n    def gen_normal():\n        for _ in range(rx * rz):\n            yield 0.0\n            yield 1.0\n            yield 0.0\n\n    def gen_index():\n        for z in range(rz - 1):\n            for x in range(rx - 1):\n                # quad poly left\n                yield z * rz + x + 1\n                yield z * rz + x\n                yield z * rz + x + rx\n                # quad poly right\n                yield z * rz + x + 1\n                yield z * rz + x + rx\n                yield z * rz + x + rx + 1\n\n    pos_data = numpy.fromiter(gen_pos(), dtype=numpy.float32)\n    uv_data = numpy.fromiter(gen_uv(), dtype=numpy.float32)\n    normal_data = numpy.fromiter(gen_normal(), dtype=numpy.float32)\n    index_data = numpy.fromiter(gen_index(), dtype=numpy.uint32)\n\n    vao = VAO(\"plane_xz\", mode=moderngl.TRIANGLES)\n\n    vao.buffer(pos_data, '3f', ['in_position'])\n    vao.buffer(uv_data, '2f', ['in_uv'])\n    vao.buffer(normal_data, '3f', ['in_normal'])\n\n    vao.index_buffer(index_data, index_element_size=4)\n\n    return vao",
        "return_type_from_source": "VAO"
    },
    {
        "extra_left": [],
        "left": "def points_random_3d(count, range_x=(-10.0, 10.0), range_y=(-10.0, 10.0), range_z=(-10.0, 10.0), seed=None) -> ",
        "right": ":\n    \"\"\"\n    Generates random positions inside a confied box.\n\n    Args:\n        count (int): Number of points to generate\n\n    Keyword Args:\n        range_x (tuple): min-max range for x axis: Example (-10.0. 10.0)\n        range_y (tuple): min-max range for y axis: Example (-10.0. 10.0)\n        range_z (tuple): min-max range for z axis: Example (-10.0. 10.0)\n        seed (int): The random seed\n\n    Returns:\n        A :py:class:`demosys.opengl.vao.VAO` instance\n    \"\"\"\n    random.seed(seed)\n\n    def gen():\n        for _ in range(count):\n            yield random.uniform(*range_x)\n            yield random.uniform(*range_y)\n            yield random.uniform(*range_z)\n\n    data = numpy.fromiter(gen(), count=count * 3, dtype=numpy.float32)\n\n    vao = VAO(\"geometry:points_random_3d\", mode=moderngl.POINTS)\n    vao.buffer(data, '3f', ['in_position'])\n\n    return vao",
        "return_type_from_source": "VAO"
    },
    {
        "extra_left": [],
        "left": "def get_time(self) -> ",
        "right": ":\n        \"\"\"\n        Get the current position in the music in seconds\n        \"\"\"\n        if self.paused:\n            return self.pause_time\n\n        return mixer.music.get_pos() / 1000.0",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def get_program(self, label) -> ",
        "right": ":\n        \"\"\"\n        Get a program by its label\n\n        Args:\n            label (str): The label for the program\n\n        Returns: py:class:`moderngl.Program` instance\n        \"\"\"\n        return self._project.get_program(label)",
        "return_type_from_source": "moderngl.Program"
    },
    {
        "extra_left": [],
        "left": "def get_effect_class(self, effect_name, package_name = None) -> ",
        "right": ":\n        \"\"\"\n        Get an effect class by the class name\n\n        Args:\n            effect_name (str): Name of the effect class\n\n        Keyword Args:\n            package_name (str): The package the effect belongs to. This is optional and only\n                                needed when effect class names are not unique.\n\n        Returns:\n            :py:class:`Effect` class\n        \"\"\"\n        return self._project.get_effect_class(effect_name, package_name=package_name)",
        "return_type_from_source": "Type['Effect']"
    },
    {
        "extra_left": [],
        "left": "def quad_2d(width, height, xpos=0.0, ypos=0.0) -> ",
        "right": ":\n    \"\"\"\n    Creates a 2D quad VAO using 2 triangles with normals and texture coordinates.\n\n    Args:\n        width (float): Width of the quad\n        height (float): Height of the quad\n\n    Keyword Args:\n        xpos (float): Center position x\n        ypos (float): Center position y\n\n    Returns:\n        A :py:class:`demosys.opengl.vao.VAO` instance.\n    \"\"\"\n    pos = numpy.array([\n        xpos - width / 2.0, ypos + height / 2.0, 0.0,\n        xpos - width / 2.0, ypos - height / 2.0, 0.0,\n        xpos + width / 2.0, ypos - height / 2.0, 0.0,\n        xpos - width / 2.0, ypos + height / 2.0, 0.0,\n        xpos + width / 2.0, ypos - height / 2.0, 0.0,\n        xpos + width / 2.0, ypos + height / 2.0, 0.0,\n    ], dtype=numpy.float32)\n\n    normals = numpy.array([\n        0.0, 0.0, 1.0,\n        0.0, 0.0, 1.0,\n        0.0, 0.0, 1.0,\n        0.0, 0.0, 1.0,\n        0.0, 0.0, 1.0,\n        0.0, 0.0, 1.0,\n    ], dtype=numpy.float32)\n\n    uvs = numpy.array([\n        0.0, 1.0,\n        0.0, 0.0,\n        1.0, 0.0,\n        0.0, 1.0,\n        1.0, 0.0,\n        1.0, 1.0,\n    ], dtype=numpy.float32)\n\n    vao = VAO(\"geometry:quad\", mode=moderngl.TRIANGLES)\n    vao.buffer(pos, '3f', [\"in_position\"])\n    vao.buffer(normals, '3f', [\"in_normal\"])\n    vao.buffer(uvs, '2f', [\"in_uv\"])\n\n    return vao",
        "return_type_from_source": "VAO"
    },
    {
        "extra_left": [],
        "left": "def stop(self) -> ",
        "right": ":\n        \"\"\"\n        Stop the timer\n\n        Returns:\n            The time the timer was stopped\n        \"\"\"\n        self.stop_time = time.time()\n        return self.stop_time - self.start_time - self.offset",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def sphere(radius=0.5, sectors=32, rings=16) -> ",
        "right": ":\n    \"\"\"\n    Creates a sphere.\n\n    Keyword Args:\n        radius (float): Radius or the sphere\n        rings (int): number or horizontal rings\n        sectors (int): number of vertical segments\n\n    Returns:\n        A :py:class:`demosys.opengl.vao.VAO` instance\n    \"\"\"\n    R = 1.0 / (rings - 1)\n    S = 1.0 / (sectors - 1)\n\n    vertices = [0] * (rings * sectors * 3)\n    normals = [0] * (rings * sectors * 3)\n    uvs = [0] * (rings * sectors * 2)\n\n    v, n, t = 0, 0, 0\n    for r in range(rings):\n        for s in range(sectors):\n            y = math.sin(-math.pi / 2 + math.pi * r * R)\n            x = math.cos(2 * math.pi * s * S) * math.sin(math.pi * r * R)\n            z = math.sin(2 * math.pi * s * S) * math.sin(math.pi * r * R)\n\n            uvs[t] = s * S\n            uvs[t + 1] = r * R\n\n            vertices[v] = x * radius\n            vertices[v + 1] = y * radius\n            vertices[v + 2] = z * radius\n\n            normals[n] = x\n            normals[n + 1] = y\n            normals[n + 2] = z\n\n            t += 2\n            v += 3\n            n += 3\n\n    indices = [0] * rings * sectors * 6\n    i = 0\n    for r in range(rings - 1):\n        for s in range(sectors - 1):\n            indices[i] = r * sectors + s\n            indices[i + 1] = (r + 1) * sectors + (s + 1)\n            indices[i + 2] = r * sectors + (s + 1)\n\n            indices[i + 3] = r * sectors + s\n            indices[i + 4] = (r + 1) * sectors + s\n            indices[i + 5] = (r + 1) * sectors + (s + 1)\n            i += 6\n\n    vbo_vertices = numpy.array(vertices, dtype=numpy.float32)\n    vbo_normals = numpy.array(normals, dtype=numpy.float32)\n    vbo_uvs = numpy.array(uvs, dtype=numpy.float32)\n    vbo_elements = numpy.array(indices, dtype=numpy.uint32)\n\n    vao = VAO(\"sphere\", mode=mlg.TRIANGLES)\n    # VBOs\n    vao.buffer(vbo_vertices, '3f', ['in_position'])\n    vao.buffer(vbo_normals, '3f', ['in_normal'])\n    vao.buffer(vbo_uvs, '2f', ['in_uv'])\n    vao.index_buffer(vbo_elements, index_element_size=4)\n\n    return vao",
        "return_type_from_source": "VAO"
    },
    {
        "extra_left": [],
        "left": "def load(self, meta) -> ",
        "right": ":\n        \"\"\"\n        Loads a resource or return existing one\n\n        :param meta: The resource description\n        \"\"\"\n        self._check_meta(meta)\n        self.resolve_loader(meta)\n        return meta.loader_cls(meta).load()",
        "return_type_from_source": "Any"
    },
    {
        "extra_left": [],
        "left": "def get_loader(self, meta, raise_on_error=False) -> ",
        "right": ":\n        \"\"\"\n        Attempts to get a loader\n\n        :param meta: The resource description instance\n        :param raise_on_error: Raise ImproperlyConfigured if the loader cannot be resolved\n        :returns: The requested loader class\n        \"\"\"\n        for loader in self._loaders:\n            if loader.name == meta.loader:\n                return loader\n\n        if raise_on_error:\n            raise ImproperlyConfigured(\n                \"Resource has invalid loader '{}': {}\\nAvailiable loaders: {}\".format(\n                    meta.loader, meta, [loader.name for loader in self._loaders]))",
        "return_type_from_source": "BaseLoader"
    },
    {
        "extra_left": [],
        "left": "def get(self, name) -> ",
        "right": ":\n        \"\"\"\n        Get or create a Track object.\n\n        :param name: Name of the track\n        :return: Track object\n        \"\"\"\n        name = name.lower()\n        track = self.track_map.get(name)\n        if not track:\n            track = Track(name)\n            self.tacks.append(track)\n            self.track_map[name] = track\n        return track",
        "return_type_from_source": "Track"
    },
    {
        "extra_left": [],
        "left": "def find_commands(command_dir) -> ",
        "right": ":\n    \"\"\"\n    Get all command names in the a folder\n\n    :return: List of commands names\n    \"\"\"\n    if not command_dir:\n        return []\n\n    return [name for _, name, is_pkg in pkgutil.iter_modules([command_dir])\n            if not is_pkg and not name.startswith('_')]",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def instance(self, program) -> ",
        "right": ":\n        \"\"\"\n        Obtain the ``moderngl.VertexArray`` instance for the program.\n        The instance is only created once and cached internally.\n\n        Returns: ``moderngl.VertexArray`` instance\n        \"\"\"\n        vao = self.vaos.get(program.glo)\n        if vao:\n            return vao\n\n        program_attributes = [name for name, attr in program._members.items() if isinstance(attr, moderngl.Attribute)]\n\n        # Make sure all attributes are covered\n        for attrib_name in program_attributes:\n            # Ignore built in attributes for now\n            if attrib_name.startswith('gl_'):\n                continue\n\n            # Do we have a buffer mapping to this attribute?\n            if not sum(buffer.has_attribute(attrib_name) for buffer in self.buffers):\n                raise VAOError(\"VAO {} doesn't have attribute {} for program {}\".format(\n                    self.name, attrib_name, program.name))\n\n        vao_content = []\n\n        # Pick out the attributes we can actually map\n        for buffer in self.buffers:\n            content = buffer.content(program_attributes)\n            if content:\n                vao_content.append(content)\n\n        # Any attribute left is not accounted for\n        if program_attributes:\n            for attrib_name in program_attributes:\n                if attrib_name.startswith('gl_'):\n                    continue\n\n                raise VAOError(\"Did not find a buffer mapping for {}\".format([n for n in program_attributes]))\n\n        # Create the vao\n        if self._index_buffer:\n            vao = context.ctx().vertex_array(program, vao_content,\n                                             self._index_buffer, self._index_element_size)\n        else:\n            vao = context.ctx().vertex_array(program, vao_content)\n\n        self.vaos[program.glo] = vao\n        return vao",
        "return_type_from_source": "moderngl.VertexArray"
    },
    {
        "extra_left": [],
        "left": "def get_dirs(self) -> ",
        "right": ":\n        \"\"\"\n        Get all effect directories for registered effects.\n        \"\"\"\n        for package in self.packages:\n            yield os.path.join(package.path, 'resources')",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def get_effect_resources(self) -> ",
        "right": ":\n        \"\"\"\n        Get all resources registed in effect packages.\n        These are typically located in ``resources.py``\n        \"\"\"\n        resources = []\n        for package in self.packages:\n            resources.extend(package.resources)\n\n        return resources",
        "return_type_from_source": "List[Any]"
    },
    {
        "extra_left": [],
        "left": "def get_package(self, name) -> ",
        "right": ":\n        \"\"\"\n        Get a package by python path. Can also contain path to an effect.\n\n        Args:\n            name (str): Path to effect package or effect\n\n        Returns:\n            The requested EffectPackage\n\n        Raises:\n            EffectError when no package is found\n        \"\"\"\n        name, cls_name = parse_package_string(name)\n\n        try:\n            return self.package_map[name]\n        except KeyError:\n            raise EffectError(\"No package '{}' registered\".format(name))",
        "return_type_from_source": "'EffectPackage'"
    },
    {
        "extra_left": [],
        "left": "def find_effect_class(self, path) -> ",
        "right": ":\n        \"\"\"\n        Find an effect class by class name or full python path to class\n\n        Args:\n            path (str): effect class name or full python path to effect class\n\n        Returns:\n            Effect class\n\n        Raises:\n            EffectError if no class is found\n        \"\"\"\n        package_name, class_name = parse_package_string(path)\n\n        if package_name:\n            package = self.get_package(package_name)\n            return package.find_effect_class(class_name, raise_for_error=True)\n\n        for package in self.packages:\n            effect_cls = package.find_effect_class(class_name)\n            if effect_cls:\n                return effect_cls\n\n        raise EffectError(\"No effect class '{}' found in any packages\".format(class_name))",
        "return_type_from_source": "Type[Effect]"
    },
    {
        "extra_left": [],
        "left": "def runnable_effects(self) -> ",
        "right": ":\n        \"\"\"Returns the runnable effect in the package\"\"\"\n        return [cls for cls in self.effect_classes if cls.runnable]",
        "return_type_from_source": "List[Type[Effect]]"
    },
    {
        "extra_left": [],
        "left": "def get_from_cache(url, cache_dir = None) -> ",
        "right": ":\n    \"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    filename = re.sub(r'.+/', '', url)\n    # get cache path to put the file\n    cache_path = cache_dir / filename\n    if cache_path.exists():\n        return cache_path\n\n    # make HEAD request to check ETag\n    response = requests.head(url)\n\n    if response.status_code != 200:\n        if \"www.dropbox.com\" in url:\n            # dropbox return code 301, so we ignore this error\n            pass\n        else:\n            raise IOError(\"HEAD request failed for url {}\".format(url))\n\n    # add ETag to filename if it exists\n    # etag = response.headers.get(\"ETag\")\n\n    if not cache_path.exists():\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        fd, temp_filename = tempfile.mkstemp()\n        logger.info(\"%s not found in cache, downloading to %s\", url, temp_filename)\n\n        # GET file object\n        req = requests.get(url, stream=True)\n        content_length = req.headers.get('Content-Length')\n        total = int(content_length) if content_length is not None else None\n        progress = Tqdm.tqdm(unit=\"B\", total=total)\n        with open(temp_filename, 'wb') as temp_file:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk: # filter out keep-alive new chunks\n                    progress.update(len(chunk))\n                    temp_file.write(chunk)\n\n        progress.close()\n\n        logger.info(\"copying %s to cache at %s\", temp_filename, cache_path)\n        shutil.copyfile(temp_filename, str(cache_path))\n        logger.info(\"removing temp file %s\", temp_filename)\n        os.close(fd)\n        os.remove(temp_filename)\n\n    return cache_path",
        "return_type_from_source": "Path"
    },
    {
        "extra_left": [],
        "left": "def copy_arguments_to_annotations(args, type_comment, *, is_method=False):\n    \"\"\"Copies AST nodes from `type_comment` into the ast3.arguments in `args`.\n\n    Does validaation of argument count (allowing for untyped self/cls)\n    and type (vararg and kwarg).\n    \"\"\"\n    if isinstance(type_comment, ast3.Ellipsis):\n        return\n\n    expected = len(args.args)\n    if args.vararg:\n        expected += 1\n    expected += len(args.kwonlyargs)\n    if args.kwarg:\n        expected += 1\n    actual = len(type_comment) if isinstance(type_comment, list) else 1\n    if expected != actual:\n        if is_method and expected - actual == 1:\n            pass  # fine, we're just skipping `self`, `cls`, etc.\n        else:\n            raise ValueError(\n                f\"number of arguments in type comment doesn't match; \" +\n                f\"expected {expected}, found {actual}\"\n            )\n\n    if isinstance(type_comment, list):\n        next_value = type_comment.pop\n    else:\n        # If there's just one value, only one of the loops and ifs below will\n        # be populated. We ensure this with the expected/actual length check\n        # above.\n        _tc = type_comment\n\n        def next_value(index = 0) -> ",
        "right": ":\n            return _tc\n\n    for arg in args.args[expected - actual:]:\n        ensure_no_annotation(arg.annotation)\n        arg.annotation = next_value(0)\n\n    if args.vararg:\n        ensure_no_annotation(args.vararg.annotation)\n        args.vararg.annotation = next_value(0)\n\n    for arg in args.kwonlyargs:\n        ensure_no_annotation(arg.annotation)\n        arg.annotation = next_value(0)\n\n    if args.kwarg:\n        ensure_no_annotation(args.kwarg.annotation)\n        args.kwarg.annotation = next_value(0)",
        "return_type_from_source": "ast3.expr"
    },
    {
        "extra_left": [],
        "left": "def _get_connection(cls, connection = None) -> ",
        "right": ":\n        \"\"\"Get a default connection string.\n\n        Wraps :func:`bio2bel.utils.get_connection` and passing this class's :data:`module_name` to it.\n        \"\"\"\n        return get_connection(cls.module_name, connection=connection)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def decode_timestamp(data) -> ",
        "right": ":\n    \"\"\"\n    Decode timestamp using bespoke decoder.\n    Cannot use simple strptime since the ness panel contains a bug\n    that P199E zone and state updates emitted on the hour cause a minute\n    value of `60` to be sent, causing strptime to fail. This decoder handles\n    this edge case.\n    \"\"\"\n    year = 2000 + int(data[0:2])\n    month = int(data[2:4])\n    day = int(data[4:6])\n    hour = int(data[6:8])\n    minute = int(data[8:10])\n    second = int(data[10:12])\n    if minute == 60:\n        minute = 0\n        hour += 1\n\n    return datetime.datetime(year=year, month=month, day=day, hour=hour,\n                             minute=minute, second=second)",
        "return_type_from_source": "datetime.datetime"
    },
    {
        "extra_left": [],
        "left": "async def update(self) -> ",
        "right": ":\n        \"\"\"Force update of alarm status and zones\"\"\"\n        _LOGGER.debug(\"Requesting state update from server (S00, S14)\")\n        await asyncio.gather(\n            # List unsealed Zones\n            self.send_command('S00'),\n            # Arming status update\n            self.send_command('S14'),\n        )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "async def _update_loop(self) -> ",
        "right": ":\n        \"\"\"Schedule a state update to keep the connection alive\"\"\"\n        await asyncio.sleep(self._update_interval)\n        while not self._closed:\n            await self.update()\n            await asyncio.sleep(self._update_interval)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _iterate_namespace_models(self, **kwargs) -> ",
        "right": ":\n        \"\"\"Return an iterator over the models to be converted to the namespace.\"\"\"\n        return tqdm(\n            self._get_query(self.namespace_model),\n            total=self._count_model(self.namespace_model),\n            **kwargs\n        )",
        "return_type_from_source": "Iterable"
    },
    {
        "extra_left": [],
        "left": "def _get_default_namespace(self) -> ",
        "right": ":\n        \"\"\"Get the reference BEL namespace if it exists.\"\"\"\n        return self._get_query(Namespace).filter(Namespace.url == self._get_namespace_url()).one_or_none()",
        "return_type_from_source": "Optional[Namespace]"
    },
    {
        "extra_left": [],
        "left": "def _make_namespace(self) -> ",
        "right": ":\n        \"\"\"Make a namespace.\"\"\"\n        namespace = Namespace(\n            name=self._get_namespace_name(),\n            keyword=self._get_namespace_keyword(),\n            url=self._get_namespace_url(),\n            version=str(time.asctime()),\n        )\n        self.session.add(namespace)\n\n        entries = self._get_namespace_entries(namespace)\n        self.session.add_all(entries)\n\n        t = time.time()\n        log.info('committing models')\n        self.session.commit()\n        log.info('committed models in %.2f seconds', time.time() - t)\n\n        return namespace",
        "return_type_from_source": "Namespace"
    },
    {
        "extra_left": [],
        "left": "def _get_old_entry_identifiers(namespace) -> ",
        "right": ":\n        \"\"\"Convert a PyBEL generalized namespace entries to a set.\n\n        Default to using the identifier, but can be overridden to use the name instead.\n\n        >>> {term.identifier for term in namespace.entries}\n        \"\"\"\n        return {term.identifier for term in namespace.entries}",
        "return_type_from_source": "Set[NamespaceEntry]"
    },
    {
        "extra_left": [],
        "left": "def _update_namespace(self, namespace) -> ",
        "right": ":\n        \"\"\"Update an already-created namespace.\n\n        Note: Only call this if namespace won't be none!\n        \"\"\"\n        old_entry_identifiers = self._get_old_entry_identifiers(namespace)\n        new_count = 0\n        skip_count = 0\n\n        for model in self._iterate_namespace_models():\n            if self._get_identifier(model) in old_entry_identifiers:\n                continue\n\n            entry = self._create_namespace_entry_from_model(model, namespace=namespace)\n            if entry is None or entry.name is None:\n                skip_count += 1\n                continue\n\n            new_count += 1\n            self.session.add(entry)\n\n        t = time.time()\n        log.info('got %d new entries. skipped %d entries missing names. committing models', new_count, skip_count)\n        self.session.commit()\n        log.info('committed models in %.2f seconds', time.time() - t)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def add_namespace_to_graph(self, graph) -> ",
        "right": ":\n        \"\"\"Add this manager's namespace to the graph.\"\"\"\n        namespace = self.upload_bel_namespace()\n        graph.namespace_url[namespace.keyword] = namespace.url\n\n        # Add this manager as an annotation, too\n        self._add_annotation_to_graph(graph)\n\n        return namespace",
        "return_type_from_source": "Namespace"
    },
    {
        "extra_left": [],
        "left": "def _add_annotation_to_graph(self, graph) -> ",
        "right": ":\n        \"\"\"Add this manager as an annotation to the graph.\"\"\"\n        if 'bio2bel' not in graph.annotation_list:\n            graph.annotation_list['bio2bel'] = set()\n\n        graph.annotation_list['bio2bel'].add(self.module_name)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def upload_bel_namespace(self, update = False) -> ",
        "right": ":\n        \"\"\"Upload the namespace to the PyBEL database.\n\n        :param update: Should the namespace be updated first?\n        \"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        namespace = self._get_default_namespace()\n\n        if namespace is None:\n            log.info('making namespace for %s', self._get_namespace_name())\n            return self._make_namespace()\n\n        if update:\n            self._update_namespace(namespace)\n\n        return namespace",
        "return_type_from_source": "Namespace"
    },
    {
        "extra_left": [],
        "left": "def drop_bel_namespace(self) -> ",
        "right": ":\n        \"\"\"Remove the default namespace if it exists.\"\"\"\n        namespace = self._get_default_namespace()\n\n        if namespace is not None:\n            for entry in tqdm(namespace.entries, desc=f'deleting entries in {self._get_namespace_name()}'):\n                self.session.delete(entry)\n            self.session.delete(namespace)\n\n            log.info('committing deletions')\n            self.session.commit()\n            return namespace",
        "return_type_from_source": "Optional[Namespace]"
    },
    {
        "extra_left": [],
        "left": "def write_bel_namespace(self, file, use_names = False) -> ",
        "right": ":\n        \"\"\"Write as a BEL namespace file.\"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        if use_names and not self.has_names:\n            raise ValueError\n\n        values = (\n            self._get_namespace_name_to_encoding(desc='writing names')\n            if use_names else\n            self._get_namespace_identifier_to_encoding(desc='writing identifiers')\n        )\n\n        write_namespace(\n            namespace_name=self._get_namespace_name(),\n            namespace_keyword=self._get_namespace_keyword(),\n            namespace_query_url=self.identifiers_url,\n            values=values,\n            file=file,\n        )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def write_bel_annotation(self, file) -> ",
        "right": ":\n        \"\"\"Write as a BEL annotation file.\"\"\"\n        if not self.is_populated():\n            self.populate()\n\n        values = self._get_namespace_name_to_encoding(desc='writing names')\n\n        write_annotation(\n            keyword=self._get_namespace_keyword(),\n            citation_name=self._get_namespace_name(),\n            description='',\n            values=values,\n            file=file,\n        )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def write_bel_namespace_mappings(self, file, **kwargs) -> ",
        "right": ":\n        \"\"\"Write a BEL namespace mapping file.\"\"\"\n        json.dump(self._get_namespace_identifier_to_name(**kwargs), file, indent=2, sort_keys=True)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def write_directory(self, directory) -> ",
        "right": ":\n        \"\"\"Write a BEL namespace for identifiers, names, name hash, and mappings to the given directory.\"\"\"\n        current_md5_hash = self.get_namespace_hash()\n        md5_hash_path = os.path.join(directory, f'{self.module_name}.belns.md5')\n\n        if not os.path.exists(md5_hash_path):\n            old_md5_hash = None\n        else:\n            with open(md5_hash_path) as file:\n                old_md5_hash = file.read().strip()\n\n        if old_md5_hash == current_md5_hash:\n            return False\n\n        with open(os.path.join(directory, f'{self.module_name}.belns'), 'w') as file:\n            self.write_bel_namespace(file, use_names=False)\n\n        with open(md5_hash_path, 'w') as file:\n            print(current_md5_hash, file=file)\n\n        if self.has_names:\n            with open(os.path.join(directory, f'{self.module_name}-names.belns'), 'w') as file:\n                self.write_bel_namespace(file, use_names=True)\n\n            with open(os.path.join(directory, f'{self.module_name}.belns.mapping'), 'w') as file:\n                self.write_bel_namespace_mappings(file, desc='writing mapping')\n\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_namespace_hash(self, hash_fn=hashlib.md5) -> ",
        "right": ":\n        \"\"\"Get the namespace hash.\n\n        Defaults to MD5.\n        \"\"\"\n        m = hash_fn()\n\n        if self.has_names:\n            items = self._get_namespace_name_to_encoding(desc='getting hash').items()\n        else:\n            items = self._get_namespace_identifier_to_encoding(desc='getting hash').items()\n\n        for name, encoding in items:\n            m.update(f'{name}:{encoding}'.encode('utf8'))\n        return m.hexdigest()",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def _store_helper(model, session = None) -> ",
        "right": ":\n    \"\"\"Help store an action.\"\"\"\n    if session is None:\n        session = _make_session()\n\n    session.add(model)\n    session.commit()\n    session.close()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _make_session(connection = None) -> ",
        "right": ":\n    \"\"\"Make a session.\"\"\"\n    if connection is None:\n        connection = get_global_connection()\n\n    engine = create_engine(connection)\n\n    create_all(engine)\n\n    session_cls = sessionmaker(bind=engine)\n    session = session_cls()\n\n    return session",
        "return_type_from_source": "Session"
    },
    {
        "extra_left": [],
        "left": "def store_populate(cls, resource, session = None) -> ",
        "right": ":\n        \"\"\"Store a \"populate\" event.\n\n        :param resource: The normalized name of the resource to store\n\n        Example:\n\n        >>> from bio2bel.models import Action\n        >>> Action.store_populate('hgnc')\n        \"\"\"\n        action = cls.make_populate(resource)\n        _store_helper(action, session=session)\n        return action",
        "return_type_from_source": "'Action'"
    },
    {
        "extra_left": [],
        "left": "def store_populate_failed(cls, resource, session = None) -> ",
        "right": ":\n        \"\"\"Store a \"populate failed\" event.\n\n        :param resource: The normalized name of the resource to store\n\n        Example:\n\n        >>> from bio2bel.models import Action\n        >>> Action.store_populate_failed('hgnc')\n        \"\"\"\n        action = cls.make_populate_failed(resource)\n        _store_helper(action, session=session)\n        return action",
        "return_type_from_source": "'Action'"
    },
    {
        "extra_left": [],
        "left": "def store_drop(cls, resource, session = None) -> ",
        "right": ":\n        \"\"\"Store a \"drop\" event.\n\n        :param resource: The normalized name of the resource to store\n\n        Example:\n\n        >>> from bio2bel.models import Action\n        >>> Action.store_drop('hgnc')\n        \"\"\"\n        action = cls.make_drop(resource)\n        _store_helper(action, session=session)\n        return action",
        "return_type_from_source": "'Action'"
    },
    {
        "extra_left": [],
        "left": "def ls(cls, session = None) -> ",
        "right": ":\n        \"\"\"Get all actions.\"\"\"\n        if session is None:\n            session = _make_session()\n\n        actions = session.query(cls).order_by(cls.created.desc()).all()\n        session.close()\n        return actions",
        "return_type_from_source": "List['Action']"
    },
    {
        "extra_left": [],
        "left": "def count(cls, session = None) -> ",
        "right": ":\n        \"\"\"Count all actions.\"\"\"\n        if session is None:\n            session = _make_session()\n\n        count = session.query(cls).count()\n        session.close()\n        return count",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def get_data_dir(module_name) -> ",
        "right": ":\n    \"\"\"Ensure the appropriate Bio2BEL data directory exists for the given module, then returns the file path.\n\n    :param module_name: The name of the module. Ex: 'chembl'\n    :return: The module's data directory\n    \"\"\"\n    module_name = module_name.lower()\n    data_dir = os.path.join(BIO2BEL_DIR, module_name)\n    os.makedirs(data_dir, exist_ok=True)\n    return data_dir",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_module_config_cls(module_name) -> ",
        "right": ":  # noqa: D202\n    \"\"\"Build a module configuration class.\"\"\"\n\n    class ModuleConfig(_AbstractModuleConfig):\n        NAME = f'bio2bel:{module_name}'\n        FILES = DEFAULT_CONFIG_PATHS + [\n            os.path.join(DEFAULT_CONFIG_DIRECTORY, module_name, 'config.ini')\n        ]\n\n    return ModuleConfig",
        "return_type_from_source": "Type[_AbstractModuleConfig]"
    },
    {
        "extra_left": [],
        "left": "def get_connection(module_name, connection = None) -> ",
        "right": ":\n    \"\"\"Return the SQLAlchemy connection string if it is set.\n\n    Order of operations:\n\n    1. Return the connection if given as a parameter\n    2. Check the environment for BIO2BEL_{module_name}_CONNECTION\n    3. Look in the bio2bel config file for module-specific connection. Create if doesn't exist. Check the\n       module-specific section for ``connection``\n    4. Look in the bio2bel module folder for a config file. Don't create if doesn't exist. Check the default section\n       for ``connection``\n    5. Check the environment for BIO2BEL_CONNECTION\n    6. Check the bio2bel config file for default\n    7. Fall back to standard default cache connection\n\n    :param module_name: The name of the module to get the configuration for\n    :param connection: get the SQLAlchemy connection string\n    :return: The SQLAlchemy connection string based on the configuration\n    \"\"\"\n    # 1. Use given connection\n    if connection is not None:\n        return connection\n\n    module_name = module_name.lower()\n    module_config_cls = get_module_config_cls(module_name)\n    module_config = module_config_cls.load()\n\n    return module_config.connection or config.connection",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def get_modules() -> ",
        "right": ":\n    \"\"\"Get all Bio2BEL modules.\"\"\"\n    modules = {}\n\n    for entry_point in iter_entry_points(group='bio2bel', name=None):\n        entry = entry_point.name\n\n        try:\n            modules[entry] = entry_point.load()\n        except VersionConflict as exc:\n            log.warning('Version conflict in %s: %s', entry, exc)\n            continue\n        except UnknownExtra as exc:\n            log.warning('Unknown extra in %s: %s', entry, exc)\n            continue\n        except ImportError as exc:\n            log.exception('Issue with importing module %s: %s', entry, exc)\n            continue\n\n    return modules",
        "return_type_from_source": "Mapping"
    },
    {
        "extra_left": [],
        "left": "def clear_cache(module_name, keep_database = True) -> ",
        "right": ":\n    \"\"\"Clear all downloaded files.\"\"\"\n    data_dir = get_data_dir(module_name)\n    if not os.path.exists(data_dir):\n        return\n    for name in os.listdir(data_dir):\n        if name in {'config.ini', 'cfg.ini'}:\n            continue\n        if name == 'cache.db' and keep_database:\n            continue\n        path = os.path.join(data_dir, name)\n        if os.path.isdir(path):\n            shutil.rmtree(path)\n        else:\n            os.remove(path)\n\n        os.rmdir(data_dir)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def count_relations(self) -> ",
        "right": ":\n        \"\"\"Count the number of BEL relations generated.\"\"\"\n        if self.edge_model is ...:\n            raise Bio2BELMissingEdgeModelError('edge_edge model is undefined/count_bel_relations is not overridden')\n        elif isinstance(self.edge_model, list):\n            return sum(self._count_model(m) for m in self.edge_model)\n        else:\n            return self._count_model(self.edge_model)",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def ensure_instruction(instruction) -> ",
        "right": ":\n    \"\"\"\n    Wraps an instruction to be Python 3.6+ compatible. This does nothing on Python 3.5 and below.\n\n    This is most useful for operating on bare, single-width instructions such as\n    ``RETURN_FUNCTION`` in a version portable way.\n\n    :param instruction: The instruction integer to use.\n    :return: A safe bytes object, if applicable.\n    \"\"\"\n    if PY36:\n        return instruction.to_bytes(2, byteorder=\"little\")\n    else:\n        return instruction.to_bytes(1, byteorder=\"little\")",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def pack_value(index) -> ",
        "right": ":\n    \"\"\"\n    Small helper value to pack an index value into bytecode.\n\n    This is used for version compat between 3.5- and 3.6+\n\n    :param index: The item to pack.\n    :return: The packed item.\n    \"\"\"\n    if PY36:\n        return index.to_bytes(1, byteorder=\"little\")\n    else:\n        return index.to_bytes(2, byteorder=\"little\")",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def generate_bytecode_from_obb(obb, previous) -> ",
        "right": ":\n    \"\"\"\n    Generates a bytecode from an object.\n\n    :param obb: The object to generate.\n    :param previous: The previous bytecode to use when generating subobjects.\n    :return: The generated bytecode.\n    \"\"\"\n    # Generates bytecode from a specified object, be it a validator or an int or bytes even.\n    if isinstance(obb, pyte.superclasses._PyteOp):\n        return obb.to_bytes(previous)\n    elif isinstance(obb, (pyte.superclasses._PyteAugmentedComparator,\n                          pyte.superclasses._PyteAugmentedValidator._FakeMathematicalOP)):\n        return obb.to_bytes(previous)\n    elif isinstance(obb, pyte.superclasses._PyteAugmentedValidator):\n        obb.validate()\n        return obb.to_load()\n    elif isinstance(obb, int):\n        return obb.to_bytes((obb.bit_length() + 7) // 8, byteorder=\"little\") or b''\n    elif isinstance(obb, bytes):\n        return obb\n    else:\n        raise TypeError(\"`{}` was not a valid bytecode-encodable item\".format(obb))",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def compile_bytecode(code) -> ",
        "right": ":\n    \"\"\"\n    Compiles Pyte objects into a bytecode list.\n\n    :param code: A list of objects to compile.\n    :return: The computed bytecode.\n    \"\"\"\n    bc = b\"\"\n    for i, op in enumerate(code):\n        try:\n            # Get the bytecode.\n            if isinstance(op, _PyteOp) or isinstance(op, _PyteAugmentedComparator):\n                bc_op = op.to_bytes(bc)\n            elif isinstance(op, int):\n                bc_op = op.to_bytes(1, byteorder=\"little\")\n            elif isinstance(op, bytes):\n                bc_op = op\n            else:\n                raise CompileError(\"Could not compile code of type {}\".format(type(op)))\n            bc += bc_op\n        except Exception as e:\n            print(\"Fatal compiliation error on operator {i} ({op}).\".format(i=i, op=op))\n            raise e\n\n    return bc",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def _simulate_stack(code) -> ",
        "right": ":\n    \"\"\"\n    Simulates the actions of the stack, to check safety.\n\n    This returns the maximum needed stack.\n    \"\"\"\n\n    max_stack = 0\n    curr_stack = 0\n\n    def _check_stack(ins):\n        if curr_stack < 0:\n            raise CompileError(\"Stack turned negative on instruction: {}\".format(ins))\n        if curr_stack > max_stack:\n            return curr_stack\n\n    # Iterate over the bytecode.\n    for instruction in code:\n        assert isinstance(instruction, dis.Instruction)\n        if instruction.arg is not None:\n            try:\n                effect = dis.stack_effect(instruction.opcode, instruction.arg)\n            except ValueError as e:\n                raise CompileError(\"Invalid opcode `{}` when compiling\"\n                                   .format(instruction.opcode)) from e\n        else:\n            try:\n                effect = dis.stack_effect(instruction.opcode)\n            except ValueError as e:\n                raise CompileError(\"Invalid opcode `{}` when compiling\"\n                                   .format(instruction.opcode)) from e\n        curr_stack += effect\n        # Re-check the stack.\n        _should_new_stack = _check_stack(instruction)\n        if _should_new_stack:\n            max_stack = _should_new_stack\n\n    return max_stack",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "async def connect(\n    host,\n    port=22223,\n    version=\"1.19\",\n    on_event=None,\n    on_disconnect=None,\n    timeout=5,\n    loop=None,\n) -> ",
        "right": ":\n    \"\"\"Async function to connect to QTM\n\n    :param host: Address of the computer running QTM.\n    :param port: Port number to connect to, should be the port configured for little endian.\n    :param version: What version of the protocol to use, tested for 1.17 and above but could\n        work with lower versions as well.\n    :param on_disconnect: Function to be called when a disconnect from QTM occurs.\n    :param on_event: Function to be called when there's an event from QTM.\n    :param timeout: The default timeout time for calls to QTM.\n    :param loop: Alternative event loop, will use asyncio default if None.\n\n    :rtype: A :class:`.QRTConnection`\n    \"\"\"\n    loop = loop or asyncio.get_event_loop()\n\n    try:\n        _, protocol = await loop.create_connection(\n            lambda: QTMProtocol(\n                loop=loop, on_event=on_event, on_disconnect=on_disconnect\n            ),\n            host,\n            port,\n        )\n    except (ConnectionRefusedError, TimeoutError, OSError) as exception:\n        LOG.error(exception)\n        return None\n\n    try:\n        await protocol.set_version(version)\n    except QRTCommandException as exception:\n        LOG.error(Exception)\n        return None\n    except TypeError as exception:  # TODO: fix test requiring this (test_connect_set_version)\n        LOG.error(exception)\n        return None\n\n    return QRTConnection(protocol, timeout=timeout)",
        "return_type_from_source": "QRTConnection"
    },
    {
        "extra_left": [],
        "left": "async def get_current_frame(self, components=None) -> ",
        "right": ":\n        \"\"\"Get measured values from QTM for a single frame.\n\n        :param components: A list of components to receive, could be 'all' or any combination of\n                '2d', '2dlin', '3d', '3dres', '3dnolabels',\n                '3dnolabelsres', 'force', 'forcesingle', '6d', '6dres',\n                '6deuler', '6deulerres', 'gazevector', 'image', 'timecode',\n                'skeleton', 'skeleton:global'\n\n        :rtype: A :class:`qtm.QRTPacket` containing requested components\n        \"\"\"\n\n        if components is None:\n            components = [\"all\"]\n        else:\n            _validate_components(components)\n\n        cmd = \"getcurrentframe %s\" % \" \".join(components)\n        return await asyncio.wait_for(\n            self._protocol.send_command(cmd), timeout=self._timeout\n        )",
        "return_type_from_source": "QRTPacket"
    },
    {
        "extra_left": [],
        "left": "def count_subgraph_sizes(graph, annotation = 'Subgraph') -> ",
        "right": ":\n    \"\"\"Count the number of nodes in each subgraph induced by an annotation.\n\n    :param annotation: The annotation to group by and compare. Defaults to 'Subgraph'\n    :return: A dictionary from {annotation value: number of nodes}\n    \"\"\"\n    return count_dict_values(group_nodes_by_annotation(graph, annotation))",
        "return_type_from_source": "Counter[int]"
    },
    {
        "extra_left": [],
        "left": "def to_jupyter(graph, chart = None) -> ",
        "right": ":\n    \"\"\"Render the graph as JavaScript in a Jupyter Notebook.\"\"\"\n    with open(os.path.join(HERE, 'render_with_javascript.js'), 'rt') as f:\n        js_template = Template(f.read())\n\n    return Javascript(js_template.render(**_get_context(graph, chart=chart)))",
        "return_type_from_source": "Javascript"
    },
    {
        "extra_left": [],
        "left": "def remove_nodes_by_function_namespace(graph, func, namespace) -> ",
        "right": ":\n    \"\"\"Remove nodes with the given function and namespace.\n\n    This might be useful to exclude information learned about distant species, such as excluding all information\n    from MGI and RGD in diseases where mice and rats don't give much insight to the human disease mechanism.\n    \"\"\"\n    remove_filtered_nodes(graph, function_namespace_inclusion_builder(func, namespace))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def preprocessing_br_projection_excel(path) -> ",
        "right": ":\n    \"\"\"Preprocess the excel file.\n\n    Parameters\n    ----------\n    path : Filepath of the excel sheet\n    \"\"\"\n    if not os.path.exists(path):\n        raise ValueError(\"Error: %s file not found\" % path)\n\n    return pd.read_excel(path, sheetname=0, header=0)",
        "return_type_from_source": "pd.DataFrame"
    },
    {
        "extra_left": [],
        "left": "def get_regulatory_pairs(graph) -> ",
        "right": ":\n    \"\"\"Find pairs of nodes that have mutual causal edges that are regulating each other such that ``A -> B`` and\n    ``B -| A``.\n\n    :return: A set of pairs of nodes with mutual causal edges\n    \"\"\"\n    cg = get_causal_subgraph(graph)\n\n    results = set()\n\n    for u, v, d in cg.edges(data=True):\n        if d[RELATION] not in CAUSAL_INCREASE_RELATIONS:\n            continue\n\n        if cg.has_edge(v, u) and any(dd[RELATION] in CAUSAL_DECREASE_RELATIONS for dd in cg[v][u].values()):\n            results.add((u, v))\n\n    return results",
        "return_type_from_source": "Set[NodePair]"
    },
    {
        "extra_left": [],
        "left": "def get_chaotic_pairs(graph) -> ",
        "right": ":\n    \"\"\"Find pairs of nodes that have mutual causal edges that are increasing each other such that ``A -> B`` and\n    ``B -> A``.\n\n    :return: A set of pairs of nodes with mutual causal edges\n    \"\"\"\n    cg = get_causal_subgraph(graph)\n\n    results = set()\n\n    for u, v, d in cg.edges(data=True):\n        if d[RELATION] not in CAUSAL_INCREASE_RELATIONS:\n            continue\n\n        if cg.has_edge(v, u) and any(dd[RELATION] in CAUSAL_INCREASE_RELATIONS for dd in cg[v][u].values()):\n            results.add(tuple(sorted([u, v], key=str)))\n\n    return results",
        "return_type_from_source": "SetOfNodePairs"
    },
    {
        "extra_left": [],
        "left": "def get_correlation_graph(graph) -> ",
        "right": ":\n    \"\"\"Extract an undirected graph of only correlative relationships.\"\"\"\n    result = Graph()\n\n    for u, v, d in graph.edges(data=True):\n        if d[RELATION] not in CORRELATIVE_RELATIONS:\n            continue\n\n        if not result.has_edge(u, v):\n            result.add_edge(u, v, **{d[RELATION]: True})\n\n        elif d[RELATION] not in result[u][v]:\n            log.log(5, 'broken correlation relation for %s, %s', u, v)\n            result[u][v][d[RELATION]] = True\n            result[v][u][d[RELATION]] = True\n\n    return result",
        "return_type_from_source": "Graph"
    },
    {
        "extra_left": [],
        "left": "def get_correlation_triangles(graph) -> ",
        "right": ":\n    \"\"\"Return a set of all triangles pointed by the given node.\"\"\"\n    return {\n        tuple(sorted([n, u, v], key=str))\n        for n in graph\n        for u, v in itt.combinations(graph[n], 2)\n        if graph.has_edge(u, v)\n    }",
        "return_type_from_source": "SetOfNodeTriples"
    },
    {
        "extra_left": [],
        "left": "def get_triangles(graph) -> ",
        "right": ":\n    \"\"\"Get a set of triples representing the 3-cycles from a directional graph.\n\n    Each 3-cycle is returned once, with nodes in sorted order.\n    \"\"\"\n    return {\n        tuple(sorted([a, b, c], key=str))\n        for a, b in graph.edges()\n        for c in graph.successors(b)\n        if graph.has_edge(c, a)\n    }",
        "return_type_from_source": "SetOfNodeTriples"
    },
    {
        "extra_left": [],
        "left": "def get_separate_unstable_correlation_triples(graph) -> ",
        "right": ":\n    \"\"\"Yield all triples of nodes A, B, C such that ``A pos B``, ``A pos C``, and ``B neg C``.\n\n    :return: An iterator over triples of unstable graphs, where the second two are negative\n    \"\"\"\n    cg = get_correlation_graph(graph)\n\n    for a, b, c in get_correlation_triangles(cg):\n        if POSITIVE_CORRELATION in cg[a][b] and POSITIVE_CORRELATION in cg[b][c] and NEGATIVE_CORRELATION in \\\n                cg[a][c]:\n            yield b, a, c\n        if POSITIVE_CORRELATION in cg[a][b] and NEGATIVE_CORRELATION in cg[b][c] and POSITIVE_CORRELATION in \\\n                cg[a][c]:\n            yield a, b, c\n        if NEGATIVE_CORRELATION in cg[a][b] and POSITIVE_CORRELATION in cg[b][c] and POSITIVE_CORRELATION in \\\n                cg[a][c]:\n            yield c, a, b",
        "return_type_from_source": "Iterable[NodeTriple]"
    },
    {
        "extra_left": [],
        "left": "def flatten_list_abundance(node) -> ",
        "right": ":\n    \"\"\"Flattens the complex or composite abundance.\"\"\"\n    return node.__class__(list(chain.from_iterable(\n        (\n            flatten_list_abundance(member).members\n            if isinstance(member, ListAbundance) else\n            [member]\n        )\n        for member in node.members\n    )))",
        "return_type_from_source": "ListAbundance"
    },
    {
        "extra_left": [],
        "left": "def list_abundance_expansion(graph) -> ",
        "right": ":\n    \"\"\"Flatten list abundances.\"\"\"\n    mapping = {\n        node: flatten_list_abundance(node)\n        for node in graph\n        if isinstance(node, ListAbundance)\n    }\n    relabel_nodes(graph, mapping, copy=False)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def list_abundance_cartesian_expansion(graph) -> ",
        "right": ":\n    \"\"\"Expand all list abundances to simple subject-predicate-object networks.\"\"\"\n    for u, v, k, d in list(graph.edges(keys=True, data=True)):\n        if CITATION not in d:\n            continue\n\n        if isinstance(u, ListAbundance) and isinstance(v, ListAbundance):\n            for u_member, v_member in itt.product(u.members, v.members):\n                graph.add_qualified_edge(\n                    u_member, v_member,\n                    relation=d[RELATION],\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n        elif isinstance(u, ListAbundance):\n            for member in u.members:\n                graph.add_qualified_edge(\n                    member, v,\n                    relation=d[RELATION],\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n        elif isinstance(v, ListAbundance):\n            for member in v.members:\n                graph.add_qualified_edge(\n                    u, member,\n                    relation=d[RELATION],\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n    _remove_list_abundance_nodes(graph)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _reaction_cartesion_expansion_unqualified_helper(\n        graph,\n        u,\n        v,\n        d,\n) -> ",
        "right": ":\n    \"\"\"Helper to deal with cartension expansion in unqualified edges.\"\"\"\n    if isinstance(u, Reaction) and isinstance(v, Reaction):\n        enzymes = _get_catalysts_in_reaction(u) | _get_catalysts_in_reaction(v)\n\n        for reactant, product in chain(itt.product(u.reactants, u.products),\n                                       itt.product(v.reactants, v.products)):\n            if reactant in enzymes or product in enzymes:\n                continue\n\n            graph.add_unqualified_edge(\n                reactant, product, INCREASES\n            )\n        for product, reactant in itt.product(u.products, u.reactants):\n\n            if reactant in enzymes or product in enzymes:\n                continue\n\n            graph.add_unqualified_edge(\n                product, reactant, d[RELATION],\n            )\n\n    elif isinstance(u, Reaction):\n\n        enzymes = _get_catalysts_in_reaction(u)\n\n        for product in u.products:\n\n            # Skip create increases edges between enzymes\n            if product in enzymes:\n                continue\n\n            # Only add edge between v and reaction if the node is not part of the reaction\n            # In practice skips hasReactant, hasProduct edges\n            if v not in u.products and v not in u.reactants:\n                graph.add_unqualified_edge(\n                    product, v, INCREASES\n                )\n            for reactant in u.reactants:\n                graph.add_unqualified_edge(\n                    reactant, product, INCREASES\n                )\n\n    elif isinstance(v, Reaction):\n\n        enzymes = _get_catalysts_in_reaction(v)\n\n        for reactant in v.reactants:\n\n            # Skip create increases edges between enzymes\n            if reactant in enzymes:\n                continue\n\n            # Only add edge between v and reaction if the node is not part of the reaction\n            # In practice skips hasReactant, hasProduct edges\n            if u not in v.products and u not in v.reactants:\n                graph.add_unqualified_edge(\n                    u, reactant, INCREASES\n                )\n            for product in v.products:\n                graph.add_unqualified_edge(\n                    reactant, product, INCREASES\n                )",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _get_catalysts_in_reaction(reaction) -> ",
        "right": ":\n    \"\"\"Return nodes that are both in reactants and reactions in a reaction.\"\"\"\n    return {\n        reactant\n        for reactant in reaction.reactants\n        if reactant in reaction.products\n    }",
        "return_type_from_source": "Set[BaseAbundance]"
    },
    {
        "extra_left": [],
        "left": "def reaction_cartesian_expansion(graph, accept_unqualified_edges = True) -> ",
        "right": ":\n    \"\"\"Expand all reactions to simple subject-predicate-object networks.\"\"\"\n    for u, v, d in list(graph.edges(data=True)):\n        # Deal with unqualified edges\n        if CITATION not in d and accept_unqualified_edges:\n            _reaction_cartesion_expansion_unqualified_helper(graph, u, v, d)\n            continue\n\n        if isinstance(u, Reaction) and isinstance(v, Reaction):\n            catalysts = _get_catalysts_in_reaction(u) | _get_catalysts_in_reaction(v)\n\n            for reactant, product in chain(itt.product(u.reactants, u.products), itt.product(v.reactants, v.products)):\n                if reactant in catalysts or product in catalysts:\n                    continue\n                graph.add_increases(\n                    reactant, product,\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n            for product, reactant in itt.product(u.products, u.reactants):\n                if reactant in catalysts or product in catalysts:\n                    continue\n\n                graph.add_qualified_edge(\n                    product, reactant,\n                    relation=d[RELATION],\n                    citation=d.get(CITATION),\n                    evidence=d.get(EVIDENCE),\n                    annotations=d.get(ANNOTATIONS),\n                )\n\n        elif isinstance(u, Reaction):\n            catalysts = _get_catalysts_in_reaction(u)\n\n            for product in u.products:\n                # Skip create increases edges between enzymes\n                if product in catalysts:\n                    continue\n\n                # Only add edge between v and reaction if the node is not part of the reaction\n                # In practice skips hasReactant, hasProduct edges\n                if v not in u.products and v not in u.reactants:\n                    graph.add_increases(\n                        product, v,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n                for reactant in u.reactants:\n                    graph.add_increases(\n                        reactant, product,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n        elif isinstance(v, Reaction):\n            for reactant in v.reactants:\n                catalysts = _get_catalysts_in_reaction(v)\n\n                # Skip create increases edges between enzymes\n                if reactant in catalysts:\n                    continue\n\n                # Only add edge between v and reaction if the node is not part of the reaction\n                # In practice skips hasReactant, hasProduct edges\n                if u not in v.products and u not in v.reactants:\n                    graph.add_increases(\n                        u, reactant,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n                for product in v.products:\n                    graph.add_increases(\n                        reactant, product,\n                        citation=d.get(CITATION),\n                        evidence=d.get(EVIDENCE),\n                        annotations=d.get(ANNOTATIONS),\n                    )\n\n    _remove_reaction_nodes(graph)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_graphs_by_ids(self, network_ids) -> ",
        "right": ":\n        \"\"\"Get several graphs by their identifiers.\"\"\"\n        return [\n            self.networks[network_id]\n            for network_id in network_ids\n        ]",
        "return_type_from_source": "List[BELGraph]"
    },
    {
        "extra_left": [],
        "left": "def count_citations(graph, **annotations) -> ",
        "right": ":\n    \"\"\"Counts the citations in a graph based on a given filter\n\n    :param graph: A BEL graph\n    :param dict annotations: The annotation filters to use\n    :return: A counter from {(citation type, citation reference): frequency}\n    \"\"\"\n    citations = defaultdict(set)\n\n    annotation_dict_filter = build_edge_data_filter(annotations)\n\n    for u, v, _, d in filter_edges(graph, annotation_dict_filter):\n        if CITATION not in d:\n            continue\n\n        citations[u, v].add((d[CITATION][CITATION_TYPE], d[CITATION][CITATION_REFERENCE].strip()))\n\n    return Counter(itt.chain.from_iterable(citations.values()))",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def count_author_publications(graph) -> ",
        "right": ":\n    \"\"\"Count the number of publications of each author to the given graph.\"\"\"\n    authors = group_as_dict(_iter_author_publiations(graph))\n    return Counter(count_dict_values(count_defaultdict(authors)))",
        "return_type_from_source": "typing.Counter[str]"
    },
    {
        "extra_left": [],
        "left": "def count_citation_years(graph) -> ",
        "right": ":\n    \"\"\"Count the number of citations from each year.\"\"\"\n    result = defaultdict(set)\n\n    for _, _, data in graph.edges(data=True):\n        if CITATION not in data or CITATION_DATE not in data[CITATION]:\n            continue\n\n        try:\n            dt = _ensure_datetime(data[CITATION][CITATION_DATE])\n            result[dt.year].add((data[CITATION][CITATION_TYPE], data[CITATION][CITATION_REFERENCE]))\n        except Exception:\n            continue\n\n    return count_dict_values(result)",
        "return_type_from_source": "typing.Counter[int]"
    },
    {
        "extra_left": [],
        "left": "def count_confidences(graph) -> ",
        "right": ":\n    \"\"\"Count the confidences in the graph.\"\"\"\n    return Counter(\n        (\n            'None'\n            if ANNOTATIONS not in data or 'Confidence' not in data[ANNOTATIONS] else\n            list(data[ANNOTATIONS]['Confidence'])[0]\n        )\n        for _, _, data in graph.edges(data=True)\n        if CITATION in data  # don't bother with unqualified statements\n    )",
        "return_type_from_source": "typing.Counter[str]"
    },
    {
        "extra_left": [],
        "left": "def enrich_pubmed_citations(graph, manager) -> ",
        "right": ":\n    \"\"\"Overwrite all PubMed citations with values from NCBI's eUtils lookup service.\n\n    :return: A set of PMIDs for which the eUtils service crashed\n    \"\"\"\n    pmids = get_pubmed_identifiers(graph)\n    pmid_data, errors = get_citations_by_pmids(manager=manager, pmids=pmids)\n\n    for u, v, k in filter_edges(graph, has_pubmed):\n        pmid = graph[u][v][k][CITATION][CITATION_REFERENCE].strip()\n\n        if pmid not in pmid_data:\n            log.warning('Missing data for PubMed identifier: %s', pmid)\n            errors.add(pmid)\n            continue\n\n        graph[u][v][k][CITATION].update(pmid_data[pmid])\n\n    return errors",
        "return_type_from_source": "Set[str]"
    },
    {
        "extra_left": [],
        "left": "def is_node_highlighted(graph, node) -> ",
        "right": ":\n    \"\"\"Returns if the given node is highlighted.\n\n    :param graph: A BEL graph\n    :param node: A BEL node\n    :type node: tuple\n    :return: Does the node contain highlight information?\n    :rtype: bool\n    \"\"\"\n    return NODE_HIGHLIGHT in graph.node[node]",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def remove_highlight_nodes(graph, nodes=None) -> ",
        "right": ":\n    \"\"\"Removes the highlight from the given nodes, or all nodes if none given.\n\n    :param graph: A BEL graph\n    :param nodes: The list of nodes to un-highlight\n    \"\"\"\n    for node in graph if nodes is None else nodes:\n        if is_node_highlighted(graph, node):\n            del graph.node[node][NODE_HIGHLIGHT]",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def highlight_edges(graph, edges=None, color=None) -> ",
        "right": ":\n    \"\"\"Adds a highlight tag to the given edges.\n\n    :param graph: A BEL graph\n    :param edges: The edges (4-tuples of u, v, k, d) to add a highlight tag on\n    :type edges: iter[tuple]\n    :param str color: The color to highlight (use something that works with CSS)\n    \"\"\"\n    color = color or EDGE_HIGHLIGHT_DEFAULT_COLOR\n    for u, v, k, d in edges if edges is not None else graph.edges(keys=True, data=True):\n        graph[u][v][k][EDGE_HIGHLIGHT] = color",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_causal_source_nodes(graph, func) -> ",
        "right": ":\n    \"\"\"Return a set of all nodes that have an in-degree of 0.\n\n    This likely means that it is an external perturbagen and is not known to have any causal origin from within the\n    biological system. These nodes are useful to identify because they generally don't provide any mechanistic insight.\n    \"\"\"\n    return {\n        node\n        for node in graph\n        if node.function == func and is_causal_source(graph, node)\n    }",
        "return_type_from_source": "Set[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def get_causal_central_nodes(graph, func) -> ",
        "right": ":\n    \"\"\"Return a set of all nodes that have both an in-degree > 0 and out-degree > 0.\n\n    This means that they are an integral part of a pathway, since they are both produced and consumed.\n    \"\"\"\n    return {\n        node\n        for node in graph\n        if node.function == func and is_causal_central(graph, node)\n    }",
        "return_type_from_source": "Set[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def get_causal_sink_nodes(graph, func) -> ",
        "right": ":\n    \"\"\"Returns a set of all ABUNDANCE nodes that have an causal out-degree of 0.\n\n    This likely means that the knowledge assembly is incomplete, or there is a curation error.\n    \"\"\"\n    return {\n        node\n        for node in graph\n        if node.function == func and is_causal_sink(graph, node)\n    }",
        "return_type_from_source": "Set[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def _collapse_variants_by_function(graph, func) -> ",
        "right": ":\n    \"\"\"Collapse all of the given functions' variants' edges to their parents, in-place.\"\"\"\n    for parent_node, variant_node, data in graph.edges(data=True):\n        if data[RELATION] == HAS_VARIANT and parent_node.function == func:\n            collapse_pair(graph, from_node=variant_node, to_node=parent_node)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _collapse_edge_passing_predicates(graph, edge_predicates = None) -> ",
        "right": ":\n    \"\"\"Collapse all edges passing the given edge predicates.\"\"\"\n    for u, v, _ in filter_edges(graph, edge_predicates=edge_predicates):\n        collapse_pair(graph, survivor=u, victim=v)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def _collapse_edge_by_namespace(graph,\n                                victim_namespaces,\n                                survivor_namespaces,\n                                relations) -> ",
        "right": ":\n    \"\"\"Collapse pairs of nodes with the given namespaces that have the given relationship.\n\n    :param graph: A BEL Graph\n    :param victim_namespaces: The namespace(s) of the node to collapse\n    :param survivor_namespaces: The namespace of the node to keep\n    :param relations: The relation(s) to search\n    \"\"\"\n    relation_filter = build_relation_predicate(relations)\n    source_namespace_filter = build_source_namespace_filter(victim_namespaces)\n    target_namespace_filter = build_target_namespace_filter(survivor_namespaces)\n\n    edge_predicates = [\n        relation_filter,\n        source_namespace_filter,\n        target_namespace_filter\n    ]\n\n    _collapse_edge_passing_predicates(graph, edge_predicates=edge_predicates)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def collapse_orthologies_by_namespace(graph, victim_namespace, survivor_namespace) -> ",
        "right": ":\n    \"\"\"Collapse pairs of nodes with the given namespaces that have orthology relationships.\n\n    :param graph: A BEL Graph\n    :param victim_namespace: The namespace(s) of the node to collapse\n    :param survivor_namespace: The namespace of the node to keep\n\n    To collapse all MGI nodes to their HGNC orthologs, use:\n    >>> collapse_orthologies_by_namespace('MGI', 'HGNC')\n\n\n    To collapse collapse both MGI and RGD nodes to their HGNC orthologs, use:\n    >>> collapse_orthologies_by_namespace(['MGI', 'RGD'], 'HGNC')\n    \"\"\"\n    _collapse_edge_by_namespace(graph, victim_namespace, survivor_namespace, ORTHOLOGOUS)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def collapse_nodes_with_same_names(graph) -> ",
        "right": ":\n    \"\"\"Collapse all nodes with the same name, merging namespaces by picking first alphabetical one.\"\"\"\n    survivor_mapping = defaultdict(set) # Collapse mapping dict\n    victims = set() # Things already mapped while iterating\n\n    it = tqdm(itt.combinations(graph, r=2), total=graph.number_of_nodes() * (graph.number_of_nodes() - 1) / 2)\n    for a, b in it:\n        if b in victims:\n            continue\n\n        a_name, b_name = a.get(NAME), b.get(NAME)\n        if not a_name or not b_name or a_name.lower() != b_name.lower():\n            continue\n\n        if a.keys() != b.keys():  # not same version (might have variants)\n            continue\n\n        # Ensure that the values in the keys are also the same\n        for k in set(a.keys()) - {NAME, NAMESPACE}:\n            if a[k] != b[k]:  # something different\n                continue\n\n        survivor_mapping[a].add(b)\n        # Keep track of things that has been already mapped\n        victims.add(b)\n\n    collapse_nodes(graph, survivor_mapping)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def node_is_upstream_leaf(graph, node) -> ",
        "right": ":\n    \"\"\"Return if the node is an upstream leaf.\n\n    An upstream leaf is defined as a node that has no in-edges, and exactly 1 out-edge.\n    \"\"\"\n    return 0 == len(graph.predecessors(node)) and 1 == len(graph.successors(node))",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_unweighted_upstream_leaves(graph, key = None) -> ",
        "right": ":\n    \"\"\"Get nodes with no incoming edges, one outgoing edge, and without the given key in its data dictionary.\n\n    .. seealso :: :func:`data_does_not_contain_key_builder`\n\n    :param graph: A BEL graph\n    :param key: The key in the node data dictionary representing the experimental data. Defaults to\n     :data:`pybel_tools.constants.WEIGHT`.\n    :return: An iterable over leaves (nodes with an in-degree of 0) that don't have the given annotation\n    \"\"\"\n    if key is None:\n        key = WEIGHT\n\n    return filter_nodes(graph, [node_is_upstream_leaf, data_missing_key_builder(key)])",
        "return_type_from_source": "Iterable[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def is_unweighted_source(graph, node, key) -> ",
        "right": ":\n    \"\"\"Check if the node is both a source and also has an annotation.\n\n    :param graph: A BEL graph\n    :param node: A BEL node\n    :param key: The key in the node data dictionary representing the experimental data\n    \"\"\"\n    return graph.in_degree(node) == 0 and key not in graph.nodes[node]",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_unweighted_sources(graph, key = None) -> ",
        "right": ":\n    \"\"\"Get nodes on the periphery of the sub-graph that do not have a annotation for the given key.\n\n    :param graph: A BEL graph\n    :param key: The key in the node data dictionary representing the experimental data\n    :return: An iterator over BEL nodes that are unannotated and on the periphery of this subgraph\n    \"\"\"\n    if key is None:\n        key = WEIGHT\n\n    for node in graph:\n        if is_unweighted_source(graph, node, key):\n            yield node",
        "return_type_from_source": "Iterable[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def remove_unweighted_sources(graph, key = None) -> ",
        "right": ":\n    \"\"\"Prune unannotated nodes on the periphery of the sub-graph.\n\n    :param graph: A BEL graph\n    :param key: The key in the node data dictionary representing the experimental data. Defaults to\n     :data:`pybel_tools.constants.WEIGHT`.\n    \"\"\"\n    nodes = list(get_unweighted_sources(graph, key=key))\n    graph.remove_nodes_from(nodes)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def prune_mechanism_by_data(graph, key = None) -> ",
        "right": ":\n    \"\"\"Remove all leaves and source nodes that don't have weights.\n\n    Is a thin wrapper around  :func:`remove_unweighted_leaves` and :func:`remove_unweighted_sources`\n\n    :param graph: A BEL graph\n    :param key: The key in the node data dictionary representing the experimental data. Defaults to\n     :data:`pybel_tools.constants.WEIGHT`.\n\n    Equivalent to:\n\n    >>> remove_unweighted_leaves(graph)\n    >>> remove_unweighted_sources(graph)\n    \"\"\"\n    remove_unweighted_leaves(graph, key=key)\n    remove_unweighted_sources(graph, key=key)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def generate_mechanism(graph, node, key = None) -> ",
        "right": ":\n    \"\"\"Generate a mechanistic sub-graph upstream of the given node.\n\n    :param graph: A BEL graph\n    :param node: A BEL node\n    :param key: The key in the node data dictionary representing the experimental data.\n    :return: A sub-graph grown around the target BEL node\n    \"\"\"\n    subgraph = get_upstream_causal_subgraph(graph, node)\n    expand_upstream_causal(graph, subgraph)\n    remove_inconsistent_edges(subgraph)\n    collapse_consistent_edges(subgraph)\n\n    if key is not None:  # FIXME when is it not pruned?\n        prune_mechanism_by_data(subgraph, key)\n\n    return subgraph",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def get_neurommsig_score(graph,\n                         genes,\n                         ora_weight = None,\n                         hub_weight = None,\n                         top_percent = None,\n                         topology_weight = None) -> ",
        "right": ":\n    \"\"\"Calculate the composite NeuroMMSig Score for a given list of genes.\n\n    :param graph: A BEL graph\n    :param genes: A list of gene nodes\n    :param ora_weight: The relative weight of the over-enrichment analysis score from\n     :py:func:`neurommsig_gene_ora`. Defaults to 1.0.\n    :param hub_weight: The relative weight of the hub analysis score from :py:func:`neurommsig_hubs`.\n     Defaults to 1.0.\n    :param top_percent: The percentage of top genes to use as hubs. Defaults to 5% (0.05).\n    :param topology_weight: The relative weight of the topolgical analysis core from\n     :py:func:`neurommsig_topology`. Defaults to 1.0.\n    :return: The NeuroMMSig composite score\n    \"\"\"\n    ora_weight = ora_weight or 1.0\n    hub_weight = hub_weight or 1.0\n    topology_weight = topology_weight or 1.0\n    total_weight = ora_weight + hub_weight + topology_weight\n\n    genes = list(genes)\n\n    ora_score = neurommsig_gene_ora(graph, genes)\n    hub_score = neurommsig_hubs(graph, genes, top_percent=top_percent)\n    topology_score = neurommsig_topology(graph, genes)\n\n    weighted_sum = (\n            ora_weight * ora_score +\n            hub_weight * hub_score +\n            topology_weight * topology_score\n    )\n\n    return weighted_sum / total_weight",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def neurommsig_topology(graph, nodes) -> ",
        "right": ":\n    \"\"\"Calculate the node neighbor score for a given list of nodes.\n    \n    -  Doesn't consider self loops\n\n    .. math::\n        \n         \\frac{\\sum_i^n N_G[i]}{n*(n-1)}\n    \"\"\"\n    nodes = list(nodes)\n    number_nodes = len(nodes)\n\n    if number_nodes <= 1:\n        # log.debug('')\n        return 0.0\n\n    unnormalized_sum = sum(\n        u in graph[v]\n        for u, v in itt.product(nodes, repeat=2)\n        if v in graph and u != v\n    )\n\n    return unnormalized_sum / (number_nodes * (number_nodes - 1.0))",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def get_peripheral_successor_edges(graph, subgraph) -> ",
        "right": ":\n    \"\"\"Get the set of possible successor edges peripheral to the sub-graph.\n\n    The source nodes in this iterable are all inside the sub-graph, while the targets are outside.\n    \"\"\"\n    for u in subgraph:\n        for _, v, k in graph.out_edges(u, keys=True):\n            if v not in subgraph:\n                yield u, v, k",
        "return_type_from_source": "EdgeIterator"
    },
    {
        "extra_left": [],
        "left": "def get_peripheral_predecessor_edges(graph, subgraph) -> ",
        "right": ":\n    \"\"\"Get the set of possible predecessor edges peripheral to the sub-graph.\n\n    The target nodes in this iterable are all inside the sub-graph, while the sources are outside.\n    \"\"\"\n    for v in subgraph:\n        for u, _, k in graph.in_edges(v, keys=True):\n            if u not in subgraph:\n                yield u, v, k",
        "return_type_from_source": "EdgeIterator"
    },
    {
        "extra_left": [],
        "left": "def count_sources(edge_iter) -> ",
        "right": ":\n    \"\"\"Count the source nodes in an edge iterator with keys and data.\n\n    :return: A counter of source nodes in the iterable\n    \"\"\"\n    return Counter(u for u, _, _ in edge_iter)",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def count_targets(edge_iter) -> ",
        "right": ":\n    \"\"\"Count the target nodes in an edge iterator with keys and data.\n\n    :return: A counter of target nodes in the iterable\n    \"\"\"\n    return Counter(v for _, v, _ in edge_iter)",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def enrich_complexes(graph) -> ",
        "right": ":\n    \"\"\"Add all of the members of the complex abundances to the graph.\"\"\"\n    nodes = list(get_nodes_by_function(graph, COMPLEX))\n    for u in nodes:\n        for v in u.members:\n            graph.add_has_component(u, v)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def expand_internal(universe, graph, edge_predicates = None) -> ",
        "right": ":\n    \"\"\"Edges between entities in the sub-graph that pass the given filters.\n\n    :param universe: The full graph\n    :param graph: A sub-graph to find the upstream information\n    :param edge_predicates: Optional list of edge filter functions (graph, node, node, key, data) -> bool\n    \"\"\"\n    edge_filter = and_edge_predicates(edge_predicates)\n\n    for u, v in itt.product(graph, repeat=2):\n        if graph.has_edge(u, v) or not universe.has_edge(u, v):\n            continue\n\n        rs = defaultdict(list)\n        for key, data in universe[u][v].items():\n            if not edge_filter(universe, u, v, key):\n                continue\n\n            rs[data[RELATION]].append((key, data))\n\n        if 1 == len(rs):\n            relation = list(rs)[0]\n            for key, data in rs[relation]:\n                graph.add_edge(u, v, key=key, **data)\n        else:\n            log.debug('Multiple relationship types found between %s and %s', u, v)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def expand_internal_causal(universe, graph) -> ",
        "right": ":\n    \"\"\"Add causal edges between entities in the sub-graph.\n\n    Is an extremely thin wrapper around :func:`expand_internal`.\n\n    :param universe: A BEL graph representing the universe of all knowledge\n    :param graph: The target BEL graph to enrich with causal relations between contained nodes\n\n    Equivalent to:\n\n    >>> from pybel_tools.mutation import expand_internal\n    >>> from pybel.struct.filters.edge_predicates import is_causal_relation\n    >>> expand_internal(universe, graph, edge_predicates=is_causal_relation)\n    \"\"\"\n    expand_internal(universe, graph, edge_predicates=is_causal_relation)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_namespaces_with_incorrect_names(graph) -> ",
        "right": ":\n    \"\"\"Return the set of all namespaces with incorrect names in the graph.\"\"\"\n    return {\n        exc.namespace\n        for _, exc, _ in graph.warnings\n        if isinstance(exc, (MissingNamespaceNameWarning, MissingNamespaceRegexWarning))\n    }",
        "return_type_from_source": "Set[str]"
    },
    {
        "extra_left": [],
        "left": "def get_undefined_namespaces(graph) -> ",
        "right": ":\n    \"\"\"Get all namespaces that are used in the BEL graph aren't actually defined.\"\"\"\n    return {\n        exc.namespace\n        for _, exc, _ in graph.warnings\n        if isinstance(exc, UndefinedNamespaceWarning)\n    }",
        "return_type_from_source": "Set[str]"
    },
    {
        "extra_left": [],
        "left": "def get_incorrect_names_by_namespace(graph, namespace) -> ",
        "right": ":\n    \"\"\"Return the set of all incorrect names from the given namespace in the graph.\n\n    :return: The set of all incorrect names from the given namespace in the graph\n    \"\"\"\n    return {\n        exc.name\n        for _, exc, _ in graph.warnings\n        if isinstance(exc, (MissingNamespaceNameWarning, MissingNamespaceRegexWarning)) and exc.namespace == namespace\n    }",
        "return_type_from_source": "Set[str]"
    },
    {
        "extra_left": [],
        "left": "def get_undefined_namespace_names(graph, namespace) -> ",
        "right": ":\n    \"\"\"Get the names from a namespace that wasn't actually defined.\n\n    :return: The set of all names from the undefined namespace\n    \"\"\"\n    return {\n        exc.name\n        for _, exc, _ in graph.warnings\n        if isinstance(exc, UndefinedNamespaceWarning) and exc.namespace == namespace\n    }",
        "return_type_from_source": "Set[str]"
    },
    {
        "extra_left": [],
        "left": "def set_percentage(x, y) -> ",
        "right": ":\n    \"\"\"What percentage of x is contained within y?\n\n    :param set x: A set\n    :param set y: Another set\n    :return: The percentage of x contained within y\n    \"\"\"\n    a, b = set(x), set(y)\n\n    if not a:\n        return 0.0\n\n    return len(a & b) / len(a)",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def tanimoto_set_similarity(x, y) -> ",
        "right": ":\n    \"\"\"Calculate the tanimoto set similarity.\"\"\"\n    a, b = set(x), set(y)\n    union = a | b\n\n    if not union:\n        return 0.0\n\n    return len(a & b) / len(union)",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def min_tanimoto_set_similarity(x, y) -> ",
        "right": ":\n    \"\"\"Calculate the tanimoto set similarity using the minimum size.\n\n    :param set x: A set\n    :param set y: Another set\n    :return: The similarity between\n        \"\"\"\n    a, b = set(x), set(y)\n\n    if not a or not b:\n        return 0.0\n\n    return len(a & b) / min(len(a), len(b))",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def prepare_c3(data,\n               y_axis_label = 'y',\n               x_axis_label = 'x',\n               ) -> ",
        "right": ":\n    \"\"\"Prepares C3 JSON for making a bar chart from a Counter\n\n    :param data: A dictionary of {str: int} to display as bar chart\n    :param y_axis_label: The Y axis label\n    :param x_axis_label: X axis internal label. Should be left as default 'x')\n    :return: A JSON dictionary for making a C3 bar chart\n    \"\"\"\n    if not isinstance(data, list):\n        data = sorted(data.items(), key=itemgetter(1), reverse=True)\n\n    try:\n        labels, values = zip(*data)\n    except ValueError:\n        log.info(f'no values found for {x_axis_label}, {y_axis_label}')\n        labels, values = [], []\n\n    return json.dumps([\n        [x_axis_label] + list(labels),\n        [y_axis_label] + list(values),\n    ])",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def prepare_c3_time_series(data, y_axis_label = 'y', x_axis_label = 'x') -> ",
        "right": ":\n    \"\"\"Prepare C3 JSON string dump for a time series.\n\n    :param data: A list of tuples [(year, count)]\n    :param y_axis_label: The Y axis label\n    :param x_axis_label: X axis internal label. Should be left as default 'x')\n    \"\"\"\n    years, counter = zip(*data)\n\n    years = [\n        datetime.date(year, 1, 1).isoformat()\n        for year in years\n    ]\n\n    return json.dumps([\n        [x_axis_label] + list(years),\n        [y_axis_label] + list(counter)\n    ])",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def calculate_betweenness_centality(graph, number_samples = CENTRALITY_SAMPLES) -> ",
        "right": ":\n    \"\"\"Calculate the betweenness centrality over nodes in the graph.\n\n    Tries to do it with a certain number of samples, but then tries a complete approach if it fails.\n    \"\"\"\n    try:\n        res = nx.betweenness_centrality(graph, k=number_samples)\n    except Exception:\n        res = nx.betweenness_centrality(graph)\n    return Counter(res)",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def canonical_circulation(elements, key = None) -> ",
        "right": ":\n    \"\"\"Get get a canonical representation of the ordered collection by finding its minimum circulation with the\n    given sort key\n    \"\"\"\n    return min(get_circulations(elements), key=key)",
        "return_type_from_source": "T"
    },
    {
        "extra_left": [],
        "left": "def pair_has_contradiction(graph, u, v) -> ",
        "right": ":\n    \"\"\"Check if a pair of nodes has any contradictions in their causal relationships.\n\n    Assumes both nodes are in the graph.\n    \"\"\"\n    relations = {data[RELATION] for data in graph[u][v].values()}\n    return relation_set_has_contradictions(relations)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def relation_set_has_contradictions(relations) -> ",
        "right": ":\n    \"\"\"Return if the set of BEL relations contains a contradiction.\"\"\"\n    has_increases = any(relation in CAUSAL_INCREASE_RELATIONS for relation in relations)\n    has_decreases = any(relation in CAUSAL_DECREASE_RELATIONS for relation in relations)\n    has_cnc = any(relation == CAUSES_NO_CHANGE for relation in relations)\n    return 1 < sum([has_cnc, has_decreases, has_increases])",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def update_spia_matrices(spia_matrices,\n                         u,\n                         v,\n                         edge_data,\n                         ) -> ",
        "right": ":\n    \"\"\"Populate the adjacency matrix.\"\"\"\n    if u.namespace.upper() != 'HGNC' or v.namespace.upper() != 'HGNC':\n        return\n\n    u_name = u.name\n    v_name = v.name\n    relation = edge_data[RELATION]\n\n    if relation in CAUSAL_INCREASE_RELATIONS:\n        # If it has pmod check which one and add it to the corresponding matrix\n        if v.variants and any(isinstance(variant, ProteinModification) for variant in v.variants):\n            for variant in v.variants:\n                if not isinstance(variant, ProteinModification):\n                    continue\n                if variant[IDENTIFIER][NAME] == \"Ub\":\n                    spia_matrices[\"activation_ubiquination\"][u_name][v_name] = 1\n                elif variant[IDENTIFIER][NAME] == \"Ph\":\n                    spia_matrices[\"activation_phosphorylation\"][u_name][v_name] = 1\n        elif isinstance(v, (Gene, Rna)):  # Normal increase, add activation\n            spia_matrices['expression'][u_name][v_name] = 1\n        else:\n            spia_matrices['activation'][u_name][v_name] = 1\n\n    elif relation in CAUSAL_DECREASE_RELATIONS:\n        # If it has pmod check which one and add it to the corresponding matrix\n        if v.variants and any(isinstance(variant, ProteinModification) for variant in v.variants):\n            for variant in v.variants:\n                if not isinstance(variant, ProteinModification):\n                    continue\n                if variant[IDENTIFIER][NAME] == \"Ub\":\n                    spia_matrices['inhibition_ubiquination'][u_name][v_name] = 1\n                elif variant[IDENTIFIER][NAME] == \"Ph\":\n                    spia_matrices[\"inhibition_phosphorylation\"][u_name][v_name] = 1\n        elif isinstance(v, (Gene, Rna)):  # Normal decrease, check which matrix\n            spia_matrices[\"repression\"][u_name][v_name] = 1\n        else:\n            spia_matrices[\"inhibition\"][u_name][v_name] = 1\n\n    elif relation == ASSOCIATION:\n        spia_matrices[\"binding_association\"][u_name][v_name] = 1",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def spia_matrices_to_excel(spia_matrices, path) -> ",
        "right": ":\n    \"\"\"Export a SPIA data dictionary into an Excel sheet at the given path.\n\n    .. note::\n\n        # The R import should add the values:\n        # [\"nodes\"] from the columns\n        # [\"title\"] from the name of the file\n        # [\"NumberOfReactions\"] set to \"0\"\n    \"\"\"\n    writer = pd.ExcelWriter(path, engine='xlsxwriter')\n\n    for relation, df in spia_matrices.items():\n        df.to_excel(writer, sheet_name=relation, index=False)\n\n    # Save excel\n    writer.save()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def spia_matrices_to_tsvs(spia_matrices, directory) -> ",
        "right": ":\n    \"\"\"Export a SPIA data dictionary into a directory as several TSV documents.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    for relation, df in spia_matrices.items():\n        df.to_csv(os.path.join(directory, f'{relation}.tsv'), index=True)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def overlay_data(graph,\n                 data,\n                 label = None,\n                 overwrite = False,\n                 ) -> ",
        "right": ":\n    \"\"\"Overlays tabular data on the network\n\n    :param graph: A BEL Graph\n    :param data: A dictionary of {tuple node: data for that node}\n    :param label: The annotation label to put in the node dictionary\n    :param overwrite: Should old annotations be overwritten?\n    \"\"\"\n    if label is None:\n        label = WEIGHT\n\n    for node, value in data.items():\n        if node not in graph:\n            log.debug('%s not in graph', node)\n            continue\n\n        if label in graph.nodes[node] and not overwrite:\n            log.debug('%s already on %s', label, node)\n            continue\n\n        graph.nodes[node][label] = value",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def overlay_type_data(graph,\n                      data,\n                      func,\n                      namespace,\n                      label = None,\n                      overwrite = False,\n                      impute = None,\n                      ) -> ",
        "right": ":\n    \"\"\"Overlay tabular data on the network for data that comes from an data set with identifiers that lack\n    namespaces.\n\n    For example, if you want to overlay differential gene expression data from a table, that table\n    probably has HGNC identifiers, but no specific annotations that they are in the HGNC namespace or\n    that the entities to which they refer are RNA.\n\n    :param graph: A BEL Graph\n    :param dict data: A dictionary of {name: data}\n    :param func: The function of the keys in the data dictionary\n    :param namespace: The namespace of the keys in the data dictionary\n    :param label: The annotation label to put in the node dictionary\n    :param overwrite: Should old annotations be overwritten?\n    :param impute: The value to use for missing data\n    \"\"\"\n    new_data = {\n        node: data.get(node[NAME], impute)\n        for node in filter_nodes(graph, function_namespace_inclusion_builder(func, namespace))\n    }\n\n    overlay_data(graph, new_data, label=label, overwrite=overwrite)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def make_pubmed_gene_group(entrez_ids) -> ",
        "right": ":\n    \"\"\"Builds a skeleton for gene summaries\n\n    :param entrez_ids: A list of Entrez Gene identifiers to query the PubMed service\n    :return: An iterator over statement lines for NCBI Entrez Gene summaries\n    \"\"\"\n    url = PUBMED_GENE_QUERY_URL.format(','.join(str(x).strip() for x in entrez_ids))\n    response = requests.get(url)\n    tree = ElementTree.fromstring(response.content)\n\n    for x in tree.findall('./DocumentSummarySet/DocumentSummary'):\n        yield '\\n# {}'.format(x.find('Description').text)\n        yield 'SET Citation = {{\"Other\", \"PubMed Gene\", \"{}\"}}'.format(x.attrib['uid'])\n        yield 'SET Evidence = \"{}\"'.format(x.find('Summary').text.strip().replace('\\n', ''))\n        yield '\\nUNSET Evidence\\nUNSET Citation'",
        "return_type_from_source": "Iterable[str]"
    },
    {
        "extra_left": [],
        "left": "def write_boilerplate(name,\n                      version = None,\n                      description = None,\n                      authors = None,\n                      contact = None,\n                      copyright = None,\n                      licenses = None,\n                      disclaimer = None,\n                      namespace_url = None,\n                      namespace_patterns = None,\n                      annotation_url = None,\n                      annotation_patterns = None,\n                      annotation_list = None,\n                      pmids = None,\n                      entrez_ids = None,\n                      file = None,\n                      ) -> ",
        "right": ":\n    \"\"\"Write a boilerplate BEL document, with standard document metadata, definitions.\n\n    :param name: The unique name for this BEL document\n    :param contact: The email address of the maintainer\n    :param description: A description of the contents of this document\n    :param authors: The authors of this document\n    :param version: The version. Defaults to current date in format ``YYYYMMDD``.\n    :param copyright: Copyright information about this document\n    :param licenses: The license applied to this document\n    :param disclaimer: The disclaimer for this document\n    :param namespace_url: an optional dictionary of {str name: str URL} of namespaces\n    :param namespace_patterns: An optional dictionary of {str name: str regex} namespaces\n    :param annotation_url: An optional dictionary of {str name: str URL} of annotations\n    :param annotation_patterns: An optional dictionary of {str name: str regex} of regex annotations\n    :param annotation_list: An optional dictionary of {str name: set of names} of list annotations\n    :param pmids: A list of PubMed identifiers to auto-populate with citation and abstract\n    :param entrez_ids: A list of Entrez identifiers to autopopulate the gene summary as evidence\n    :param file: A writable file or file-like. If None, defaults to :data:`sys.stdout`\n    \"\"\"\n    lines = make_knowledge_header(\n        name=name,\n        version=version or '1.0.0',\n        description=description,\n        authors=authors,\n        contact=contact,\n        copyright=copyright,\n        licenses=licenses,\n        disclaimer=disclaimer,\n        namespace_url=namespace_url,\n        namespace_patterns=namespace_patterns,\n        annotation_url=annotation_url,\n        annotation_patterns=annotation_patterns,\n        annotation_list=annotation_list,\n    )\n\n    for line in lines:\n        print(line, file=file)\n\n    if pmids is not None:\n        for line in make_pubmed_abstract_group(pmids):\n            print(line, file=file)\n\n    if entrez_ids is not None:\n        for line in make_pubmed_gene_group(entrez_ids):\n            print(line, file=file)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def get_subgraph_by_node_search(graph, query) -> ",
        "right": ":\n    \"\"\"Get a sub-graph induced over all nodes matching the query string.\n\n    :param graph: A BEL Graph\n    :param query: A query string or iterable of query strings for node names\n\n    Thinly wraps :func:`search_node_names` and :func:`get_subgraph_by_induction`.\n    \"\"\"\n    nodes = search_node_names(graph, query)\n    return get_subgraph_by_induction(graph, nodes)",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def get_largest_component(graph) -> ",
        "right": ":\n    \"\"\"Get the giant component of a graph.\"\"\"\n    biggest_component_nodes = max(nx.weakly_connected_components(graph), key=len)\n    return subgraph(graph, biggest_component_nodes)",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def random_by_nodes(graph, percentage = None) -> ",
        "right": ":\n    \"\"\"Get a random graph by inducing over a percentage of the original nodes.\n\n    :param graph: A BEL graph\n    :param percentage: The percentage of edges to keep\n    \"\"\"\n    percentage = percentage or 0.9\n\n    assert 0 < percentage <= 1\n\n    nodes = graph.nodes()\n    n = int(len(nodes) * percentage)\n\n    subnodes = random.sample(nodes, n)\n\n    result = graph.subgraph(subnodes)\n\n    update_node_helper(graph, result)\n\n    return result",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def random_by_edges(graph, percentage = None) -> ",
        "right": ":\n    \"\"\"Get a random graph by keeping a certain percentage of original edges.\n\n    :param graph: A BEL graph\n    :param percentage: What percentage of eges to take\n    \"\"\"\n    percentage = percentage or 0.9\n    assert 0 < percentage <= 1\n\n    edges = graph.edges(keys=True)\n    n = int(graph.number_of_edges() * percentage)\n\n    subedges = random.sample(edges, n)\n\n    rv = graph.fresh_copy()\n\n    for u, v, k in subedges:\n        safe_add_edge(rv, u, v, k, graph[u][v][k])\n\n    update_node_helper(graph, rv)\n\n    return rv",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def shuffle_node_data(graph, key, percentage = None) -> ",
        "right": ":\n    \"\"\"Shuffle the node's data.\n\n    Useful for permutation testing.\n\n    :param graph: A BEL graph\n    :param key: The node data dictionary key\n    :param percentage: What percentage of possible swaps to make\n    \"\"\"\n    percentage = percentage or 0.3\n    assert 0 < percentage <= 1\n\n    n = graph.number_of_nodes()\n    swaps = int(percentage * n * (n - 1) / 2)\n\n    result = graph.copy()\n\n    for _ in range(swaps):\n        s, t = random.sample(result.node, 2)\n        result.nodes[s][key], result.nodes[t][key] = result.nodes[t][key], result.nodes[s][key]\n\n    return result",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def shuffle_relations(graph, percentage = None) -> ",
        "right": ":\n    \"\"\"Shuffle the relations.\n\n    Useful for permutation testing.\n\n    :param graph: A BEL graph\n    :param percentage: What percentage of possible swaps to make\n    \"\"\"\n    percentage = percentage or 0.3\n    assert 0 < percentage <= 1\n\n    n = graph.number_of_edges()\n    swaps = int(percentage * n * (n - 1) / 2)\n\n    result = graph.copy()\n\n    edges = result.edges(keys=True)\n\n    for _ in range(swaps):\n        (s1, t1, k1), (s2, t2, k2) = random.sample(edges, 2)\n        result[s1][t1][k1], result[s2][t2][k2] = result[s2][t2][k2], result[s1][t1][k1]\n\n    return result",
        "return_type_from_source": "BELGraph"
    },
    {
        "extra_left": [],
        "left": "def self_edge_filter(_, source, target, __) -> ",
        "right": ":\n    \"\"\"Check if the source and target nodes are the same.\"\"\"\n    return source == target",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def has_protein_modification_increases_activity(graph,\n                                                source,\n                                                target,\n                                                key,\n                                                ) -> ",
        "right": ":\n    \"\"\"Check if pmod of source causes activity of target.\"\"\"\n    edge_data = graph[source][target][key]\n    return has_protein_modification(graph, source) and part_has_modifier(edge_data, OBJECT, ACTIVITY)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def has_degradation_increases_activity(data) -> ",
        "right": ":\n    \"\"\"Check if the degradation of source causes activity of target.\"\"\"\n    return part_has_modifier(data, SUBJECT, DEGRADATION) and part_has_modifier(data, OBJECT, ACTIVITY)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def has_translocation_increases_activity(data) -> ",
        "right": ":\n    \"\"\"Check if the translocation of source causes activity of target.\"\"\"\n    return part_has_modifier(data, SUBJECT, TRANSLOCATION) and part_has_modifier(data, OBJECT, ACTIVITY)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def complex_has_member(graph, complex_node, member_node) -> ",
        "right": ":\n    \"\"\"Does the given complex contain the member?\"\"\"\n    return any(  # TODO can't you look in the members of the complex object (if it's enumerated)\n        v == member_node\n        for _, v, data in graph.out_edges(complex_node, data=True)\n        if data[RELATION] == HAS_COMPONENT\n    )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def complex_increases_activity(graph, u, v, key) -> ",
        "right": ":\n    \"\"\"Return if the formation of a complex with u increases the activity of v.\"\"\"\n    return (\n        isinstance(u, (ComplexAbundance, NamedComplexAbundance)) and\n        complex_has_member(graph, u, v) and\n        part_has_modifier(graph[u][v][key], OBJECT, ACTIVITY)\n    )",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def summarize_edge_filter(graph, edge_predicates) -> ",
        "right": ":\n    \"\"\"Print a summary of the number of edges passing a given set of filters.\"\"\"\n    passed = count_passed_edge_filter(graph, edge_predicates)\n    print('{}/{} edges passed {}'.format(\n        passed, graph.number_of_edges(),\n        (\n            ', '.join(edge_filter.__name__ for edge_filter in edge_predicates)\n            if isinstance(edge_predicates, Iterable) else\n            edge_predicates.__name__\n        )\n    ))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def node_has_namespace(node, namespace) -> ",
        "right": ":\n    \"\"\"Pass for nodes that have the given namespace.\"\"\"\n    ns = node.get(NAMESPACE)\n    return ns is not None and ns == namespace",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def node_has_namespaces(node, namespaces) -> ",
        "right": ":\n    \"\"\"Pass for nodes that have one of the given namespaces.\"\"\"\n    ns = node.get(NAMESPACE)\n    return ns is not None and ns in namespaces",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_cutoff(value, cutoff = None) -> ",
        "right": ":\n    \"\"\"Assign if a value is greater than or less than a cutoff.\"\"\"\n    cutoff = cutoff if cutoff is not None else 0\n\n    if value > cutoff:\n        return 1\n\n    if value < (-1 * cutoff):\n        return - 1\n\n    return 0",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def calculate_concordance(graph, key, cutoff = None,\n                          use_ambiguous = False) -> ",
        "right": ":\n    \"\"\"Calculates network-wide concordance.\n\n    Assumes data already annotated with given key\n\n    :param graph: A BEL graph\n    :param key: The node data dictionary key storing the logFC\n    :param cutoff: The optional logFC cutoff for significance\n    :param use_ambiguous: Compare to ambiguous edges as well\n    \"\"\"\n    correct, incorrect, ambiguous, _ = calculate_concordance_helper(graph, key, cutoff=cutoff)\n\n    try:\n        return correct / (correct + incorrect + (ambiguous if use_ambiguous else 0))\n    except ZeroDivisionError:\n        return -1.0",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def one_sided(value, distribution) -> ",
        "right": ":\n    \"\"\"Calculate the one-sided probability of getting a value more extreme than the distribution.\"\"\"\n    assert distribution\n    return sum(value < element for element in distribution) / len(distribution)",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def multi_run_epicom(graphs, path) -> ",
        "right": ":\n    \"\"\"Run EpiCom analysis on many graphs.\"\"\"\n    if isinstance(path, str):\n        with open(path, 'w') as file:\n            _multi_run_helper_file_wrapper(graphs, file)\n\n    else:\n        _multi_run_helper_file_wrapper(graphs, path)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def remove_inconsistent_edges(graph) -> ",
        "right": ":\n    \"\"\"Remove all edges between node pairs with inconsistent edges.\n\n    This is the all-or-nothing approach. It would be better to do more careful investigation of the evidences during\n    curation.\n    \"\"\"\n    for u, v in get_inconsistent_edges(graph):\n        edges = [(u, v, k) for k in graph[u][v]]\n        graph.remove_edges_from(edges)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def build_database(manager, annotation_url = None) -> ",
        "right": ":\n    \"\"\"Build a database of scores for NeuroMMSig annotated graphs.\n\n    1. Get all networks that use the Subgraph annotation\n    2. run on each\n    \"\"\"\n    annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL\n\n    annotation = manager.get_namespace_by_url(annotation_url)\n\n    if annotation is None:\n        raise RuntimeError('no graphs in database with given annotation')\n\n    networks = get_networks_using_annotation(manager, annotation)\n\n    dtis = ...\n\n    for network in networks:\n        graph = network.as_bel()\n\n        scores = epicom_on_graph(graph, dtis)\n\n        for (drug_name, subgraph_name), score in scores.items():\n            drug_model = get_drug_model(manager, drug_name)\n            subgraph_model = manager.get_annotation_entry(annotation_url, subgraph_name)\n\n            score_model = Score(\n                network=network,\n                annotation=subgraph_model,\n                drug=drug_model,\n                score=score\n            )\n\n            manager.session.add(score_model)\n\n    t = time.time()\n    logger.info('committing scores')\n    manager.session.commit()\n    logger.info('committed scores in %.2f seconds', time.time() - t)",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def workflow(\n        graph,\n        node,\n        key = None,\n        tag = None,\n        default_score = None,\n        runs = None,\n        minimum_nodes = 1,\n) -> ",
        "right": ":\n    \"\"\"Generate candidate mechanisms and run the heat diffusion workflow.\n\n    :param graph: A BEL graph\n    :param node: The BEL node that is the focus of this analysis\n    :param key: The key in the node data dictionary representing the experimental data. Defaults to\n     :data:`pybel_tools.constants.WEIGHT`.\n    :param tag: The key for the nodes' data dictionaries where the scores will be put. Defaults to 'score'\n    :param default_score: The initial score for all nodes. This number can go up or down.\n    :param runs: The number of times to run the heat diffusion workflow. Defaults to 100.\n    :param minimum_nodes: The minimum number of nodes a sub-graph needs to try running heat diffusion\n    :return: A list of runners\n    \"\"\"\n    subgraph = generate_mechanism(graph, node, key=key)\n\n    if subgraph.number_of_nodes() <= minimum_nodes:\n        return []\n\n    runners = multirun(subgraph, node, key=key, tag=tag, default_score=default_score, runs=runs)\n    return list(runners)",
        "return_type_from_source": "List['Runner']"
    },
    {
        "extra_left": [],
        "left": "def workflow_aggregate(graph,\n                       node,\n                       key = None,\n                       tag = None,\n                       default_score = None,\n                       runs = None,\n                       aggregator = None,\n                       ) -> ",
        "right": ":\n    \"\"\"Get the average score over multiple runs.\n\n    This function is very simple, and can be copied to do more interesting statistics over the :class:`Runner`\n    instances. To iterate over the runners themselves, see :func:`workflow`\n\n    :param graph: A BEL graph\n    :param node: The BEL node that is the focus of this analysis\n    :param key: The key in the node data dictionary representing the experimental data. Defaults to\n     :data:`pybel_tools.constants.WEIGHT`.\n    :param tag: The key for the nodes' data dictionaries where the scores will be put. Defaults to 'score'\n    :param default_score: The initial score for all nodes. This number can go up or down.\n    :param runs: The number of times to run the heat diffusion workflow. Defaults to 100.\n    :param aggregator: A function that aggregates a list of scores. Defaults to :func:`numpy.average`.\n                       Could also use: :func:`numpy.mean`, :func:`numpy.median`, :func:`numpy.min`, :func:`numpy.max`\n    :return: The average score for the target node\n    \"\"\"\n    runners = workflow(graph, node, key=key, tag=tag, default_score=default_score, runs=runs)\n    scores = [runner.get_final_score() for runner in runners]\n\n    if not scores:\n        log.warning('Unable to run the heat diffusion workflow for %s', node)\n        return\n\n    if aggregator is None:\n        return np.average(scores)\n\n    return aggregator(scores)",
        "return_type_from_source": "Optional[float]"
    },
    {
        "extra_left": [],
        "left": "def iter_leaves(self) -> ",
        "right": ":\n        \"\"\"Return an iterable over all nodes that are leaves.\n\n        A node is a leaf if either:\n\n         - it doesn't have any predecessors, OR\n         - all of its predecessors have a score in their data dictionaries\n        \"\"\"\n        for node in self.graph:\n            if self.tag in self.graph.nodes[node]:\n                continue\n\n            if not any(self.tag not in self.graph.nodes[p] for p in self.graph.predecessors(node)):\n                yield node",
        "return_type_from_source": "Iterable[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def unscored_nodes_iter(self) -> ",
        "right": ":\n        \"\"\"Iterate over all nodes without a score.\"\"\"\n        for node, data in self.graph.nodes(data=True):\n            if self.tag not in data:\n                yield node",
        "return_type_from_source": "BaseEntity"
    },
    {
        "extra_left": [],
        "left": "def remove_random_edge_until_has_leaves(self) -> ",
        "right": ":\n        \"\"\"Remove random edges until there is at least one leaf node.\"\"\"\n        while True:\n            leaves = set(self.iter_leaves())\n            if leaves:\n                return\n            self.remove_random_edge()",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def score_leaves(self) -> ",
        "right": ":\n        \"\"\"Calculate the score for all leaves.\n\n        :return: The set of leaf nodes that were scored\n        \"\"\"\n        leaves = set(self.iter_leaves())\n\n        if not leaves:\n            log.warning('no leaves.')\n            return set()\n\n        for leaf in leaves:\n            self.graph.nodes[leaf][self.tag] = self.calculate_score(leaf)\n            log.log(5, 'chomping %s', leaf)\n\n        return leaves",
        "return_type_from_source": "Set[BaseEntity]"
    },
    {
        "extra_left": [],
        "left": "def run_with_graph_transformation(self) -> ",
        "right": ":\n        \"\"\"Calculate scores for all leaves until there are none, removes edges until there are, and repeats until\n        all nodes have been scored. Also, yields the current graph at every step so you can make a cool animation\n        of how the graph changes throughout the course of the algorithm\n\n        :return: An iterable of BEL graphs\n        \"\"\"\n        yield self.get_remaining_graph()\n        while not self.done_chomping():\n            while not list(self.iter_leaves()):\n                self.remove_random_edge()\n                yield self.get_remaining_graph()\n            self.score_leaves()\n            yield self.get_remaining_graph()",
        "return_type_from_source": "Iterable[BELGraph]"
    },
    {
        "extra_left": [],
        "left": "def done_chomping(self) -> ",
        "right": ":\n        \"\"\"Determines if the algorithm is complete by checking if the target node of this analysis has been scored\n        yet. Because the algorithm removes edges when it gets stuck until it is un-stuck, it is always guaranteed to\n        finish.\n\n        :return: Is the algorithm done running?\n        \"\"\"\n        return self.tag in self.graph.nodes[self.target_node]",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_final_score(self) -> ",
        "right": ":\n        \"\"\"Return the final score for the target node.\n\n        :return: The final score for the target node\n        \"\"\"\n        if not self.done_chomping():\n            raise ValueError('algorithm has not yet completed')\n\n        return self.graph.nodes[self.target_node][self.tag]",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def calculate_score(self, node) -> ",
        "right": ":\n        \"\"\"Calculate the new score of the given node.\"\"\"\n        score = (\n            self.graph.nodes[node][self.tag]\n            if self.tag in self.graph.nodes[node] else\n            self.default_score\n        )\n\n        for predecessor, _, d in self.graph.in_edges(node, data=True):\n            if d[RELATION] in CAUSAL_INCREASE_RELATIONS:\n                score += self.graph.nodes[predecessor][self.tag]\n            elif d[RELATION] in CAUSAL_DECREASE_RELATIONS:\n                score -= self.graph.nodes[predecessor][self.tag]\n\n        return score",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def summarize_node_filter(graph, node_filters) -> ",
        "right": ":\n    \"\"\"Print a summary of the number of nodes passing a given set of filters.\n\n    :param graph: A BEL graph\n    :param node_filters: A node filter or list/tuple of node filters\n    \"\"\"\n    passed = count_passed_node_filter(graph, node_filters)\n    print('{}/{} nodes passed'.format(passed, graph.number_of_nodes()))",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def variants_of(\n        graph,\n        node,\n        modifications = None,\n) -> ",
        "right": ":\n    \"\"\"Returns all variants of the given node.\"\"\"\n    if modifications:\n        return _get_filtered_variants_of(graph, node, modifications)\n\n    return {\n        v\n        for u, v, key, data in graph.edges(keys=True, data=True)\n        if (\n            u == node\n            and data[RELATION] == HAS_VARIANT\n            and pybel.struct.has_protein_modification(v)\n        )\n    }",
        "return_type_from_source": "Set[Protein]"
    },
    {
        "extra_left": [],
        "left": "def count_unique_relations(graph) -> ",
        "right": ":\n    \"\"\"Return a histogram of the different types of relations present in a graph.\n\n    Note: this operation only counts each type of edge once for each pair of nodes\n    \"\"\"\n    return Counter(itt.chain.from_iterable(get_edge_relations(graph).values()))",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def count_annotation_values(graph, annotation) -> ",
        "right": ":\n    \"\"\"Count in how many edges each annotation appears in a graph\n\n    :param graph: A BEL graph\n    :param annotation: The annotation to count\n    :return: A Counter from {annotation value: frequency}\n    \"\"\"\n    return Counter(iter_annotation_values(graph, annotation))",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def count_annotation_values_filtered(graph,\n                                     annotation,\n                                     source_predicate = None,\n                                     target_predicate = None,\n                                     ) -> ",
        "right": ":\n    \"\"\"Count in how many edges each annotation appears in a graph, but filter out source nodes and target nodes.\n\n    See :func:`pybel_tools.utils.keep_node` for a basic filter.\n\n    :param graph: A BEL graph\n    :param annotation: The annotation to count\n    :param source_predicate: A predicate (graph, node) -> bool for keeping source nodes\n    :param target_predicate: A predicate (graph, node) -> bool for keeping target nodes\n    :return: A Counter from {annotation value: frequency}\n    \"\"\"\n    if source_predicate and target_predicate:\n        return Counter(\n            data[ANNOTATIONS][annotation]\n            for u, v, data in graph.edges(data=True)\n            if edge_has_annotation(data, annotation) and source_predicate(graph, u) and target_predicate(graph, v)\n        )\n    elif source_predicate:\n        return Counter(\n            data[ANNOTATIONS][annotation]\n            for u, v, data in graph.edges(data=True)\n            if edge_has_annotation(data, annotation) and source_predicate(graph, u)\n        )\n    elif target_predicate:\n        return Counter(\n            data[ANNOTATIONS][annotation]\n            for u, v, data in graph.edges(data=True)\n            if edge_has_annotation(data, annotation) and target_predicate(graph, u)\n        )\n    else:\n        return Counter(\n            data[ANNOTATIONS][annotation]\n            for u, v, data in graph.edges(data=True)\n            if edge_has_annotation(data, annotation)\n        )",
        "return_type_from_source": "Counter"
    },
    {
        "extra_left": [],
        "left": "def pair_is_consistent(graph, u, v) -> ",
        "right": ":\n    \"\"\"Return if the edges between the given nodes are consistent, meaning they all have the same relation.\n\n    :return: If the edges aren't consistent, return false, otherwise return the relation type\n    \"\"\"\n    relations = {data[RELATION] for data in graph[u][v].values()}\n\n    if 1 != len(relations):\n        return\n\n    return list(relations)[0]",
        "return_type_from_source": "Optional[str]"
    },
    {
        "extra_left": [],
        "left": "def Re(L, v, nu) -> ",
        "right": ":\n    \"\"\"\n    Calculate the Reynolds number.\n\n    :param L: [m] surface characteristic length.\n    :param v: [m/s] fluid velocity relative to the object.\n    :param nu: [m2/s] fluid kinematic viscosity.\n\n    :returns: float\n    \"\"\"\n\n    return v * L / nu",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def Ra(L, Ts, Tf, alpha, beta, nu\n       ) -> ",
        "right": ":\n    \"\"\"\n    Calculate the Ralleigh number.\n\n    :param L: [m] heat transfer surface characteristic length.\n    :param Ts: [K] heat transfer surface temperature.\n    :param Tf: [K] bulk fluid temperature.\n    :param alpha: [m2/s] fluid thermal diffusivity.\n    :param beta: [1/K] fluid coefficient of thermal expansion.\n    :param nu: [m2/s] fluid kinematic viscosity.\n\n    :returns: float\n\n    Ra = Gr*Pr\n\n    Characteristic dimensions:\n        * vertical plate: vertical length\n        * pipe: diameter\n        * bluff body: diameter\n    \"\"\"\n\n    return g * beta * (Ts - Tinf) * L**3.0 / (nu * alpha)",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def Nu(L, h, k) -> ",
        "right": ":\n    \"\"\"\n    Calculate the Nusselt number.\n\n    :param L: [m] heat transfer surface characteristic length.\n    :param h: [W/K/m2] convective heat transfer coefficient.\n    :param k: [W/K/m] fluid thermal conductivity.\n\n    :returns: float\n    \"\"\"\n\n    return h * L / k",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def Sh(L, h, D) -> ",
        "right": ":\n    \"\"\"\n    Calculate the Sherwood number.\n\n    :param L: [m] mass transfer surface characteristic length.\n    :param h: [m/s] mass transfer coefficient.\n    :param D: [m2/s] fluid mass diffusivity.\n\n    :returns: float\n    \"\"\"\n\n    return h * L / D",
        "return_type_from_source": "float"
    },
    {
        "extra_left": [],
        "left": "def parse(\n        self,\n        value,\n        type_ = str,\n        subtype = str,\n    ) -> ",
        "right": ":\n        \"\"\"\n        Parse value from string.\n\n        Convert :code:`value` to\n\n        .. code-block:: python\n\n           >>> parser = Config()\n           >>> parser.parse('12345', type_=int)\n           <<< 12345\n           >>>\n           >>> parser.parse('1,2,3,4', type_=list, subtype=int)\n           <<< [1, 2, 3, 4]\n\n        :param value: string\n        :param type\\\\_: the type to return\n        :param subtype: subtype for iterator types\n        :return: the parsed config value\n\n        \"\"\"\n        if type_ is bool:\n            return type_(value.lower() in self.TRUE_STRINGS)\n\n        try:\n            if isinstance(type_, type) and issubclass(\n                type_, (list, tuple, set, frozenset)\n            ):\n                return type_(\n                    self.parse(v.strip(\" \"), subtype)\n                    for v in value.split(\",\")\n                    if value.strip(\" \")\n                )\n\n            return type_(value)\n        except ValueError as e:\n            raise ConfigError(*e.args)",
        "return_type_from_source": "typing.Any"
    },
    {
        "extra_left": [],
        "left": "def get(\n        self,\n        key,\n        default = UNSET,\n        type_ = str,\n        subtype = str,\n        mapper = None,\n    ) -> ",
        "right": ":\n        \"\"\"\n        Parse a value from an environment variable.\n\n        .. code-block:: python\n\n           >>> os.environ['FOO']\n           <<< '12345'\n           >>>\n           >>> os.environ['BAR']\n           <<< '1,2,3,4'\n           >>>\n           >>> 'BAZ' in os.environ\n           <<< False\n           >>>\n           >>> parser = Config()\n           >>> parser.get('FOO', type_=int)\n           <<< 12345\n           >>>\n           >>> parser.get('BAR', type_=list, subtype=int)\n           <<< [1, 2, 3, 4]\n           >>>\n           >>> parser.get('BAZ', default='abc123')\n           <<< 'abc123'\n           >>>\n           >>> parser.get('FOO', type_=int, mapper=lambda x: x*10)\n           <<< 123450\n\n        :param key: the key to look up the value under\n        :param default: default value to return when when no value is present\n        :param type\\\\_: the type to return\n        :param subtype: subtype for iterator types\n        :param mapper: a function to post-process the value with\n        :return: the parsed config value\n\n        \"\"\"\n        value = self.environ.get(key, UNSET)\n\n        if value is UNSET and default is UNSET:\n            raise ConfigError(\"Unknown environment variable: {0}\".format(key))\n\n        if value is UNSET:\n            value = default\n        else:\n            value = self.parse(typing.cast(str, value), type_, subtype)\n\n        if mapper:\n            value = mapper(value)\n\n        return value",
        "return_type_from_source": "typing.Any"
    },
    {
        "extra_left": [],
        "left": "def create_client() -> ",
        "right": ":\n    \"\"\"\n    Clients a Docker client.\n\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\n    :return: the Docker client\n    \"\"\"\n    global _client\n    client = _client()\n    if client is None:\n        # First try looking at the environment variables for specification of the daemon's location\n        docker_environment = kwargs_from_env(assert_hostname=False)\n        if \"base_url\" in docker_environment:\n            client = _create_client(docker_environment.get(\"base_url\"), docker_environment.get(\"tls\"))\n            if client is None:\n                raise ConnectionError(\n                    \"Could not connect to the Docker daemon specified by the `DOCKER_X` environment variables: %s\"\n                    % docker_environment)\n            else:\n                logging.info(\"Connected to Docker daemon specified by the environment variables\")\n        else:\n            # Let's see if the Docker daemon is accessible via the UNIX socket\n            client = _create_client(\"unix://var/run/docker.sock\")\n            if client is not None:\n                logging.info(\"Connected to Docker daemon running on UNIX socket\")\n            else:\n                raise ConnectionError(\n                    \"Cannot connect to Docker - is the Docker daemon running? `$DOCKER_HOST` should be set or the \"\n                    \"daemon should be accessible via the standard UNIX socket.\")\n        _client = weakref.ref(client)\n    assert isinstance(client, APIClient)\n    return client",
        "return_type_from_source": "APIClient"
    },
    {
        "extra_left": [],
        "left": "def _hash(secret, data, alg) -> ",
        "right": ":\n    \"\"\"\n    Create a new HMAC hash.\n\n    :param secret: The secret used when hashing data.\n    :type secret: bytes\n    :param data: The data to hash.\n    :type data: bytes\n    :param alg: The algorithm to use when hashing `data`.\n    :type alg: str\n    :return: New HMAC hash.\n    :rtype: bytes\n    \"\"\"\n    algorithm = get_algorithm(alg)\n    return hmac \\\n        .new(secret, msg=data, digestmod=algorithm) \\\n        .digest()",
        "return_type_from_source": "bytes"
    },
    {
        "extra_left": [],
        "left": "def compare_signature(expected,\n                      actual) -> ",
        "right": ":\n    \"\"\"\n    Compares the given signatures.\n\n    :param expected: The expected signature.\n    :type expected: Union[str, bytes]\n    :param actual: The actual signature.\n    :type actual: Union[str, bytes]\n    :return: Do the signatures match?\n    :rtype: bool\n    \"\"\"\n    expected = util.to_bytes(expected)\n    actual = util.to_bytes(actual)\n    return hmac.compare_digest(expected, actual)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def compare_token(expected,\n                  actual) -> ",
        "right": ":\n    \"\"\"\n    Compares the given tokens.\n\n    :param expected: The expected token.\n    :type expected: Union[str, bytes]\n    :param actual: The actual token.\n    :type actual: Union[str, bytes]\n    :return: Do the tokens match?\n    :rtype: bool\n    \"\"\"\n    expected = util.to_bytes(expected)\n    actual = util.to_bytes(actual)\n    _, expected_sig_seg = expected.rsplit(b'.', 1)\n    _, actual_sig_seg = actual.rsplit(b'.', 1)\n    expected_sig = util.b64_decode(expected_sig_seg)\n    actual_sig = util.b64_decode(actual_sig_seg)\n    return compare_signature(expected_sig, actual_sig)",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def valid(self, time = None) -> ",
        "right": ":\n        \"\"\"\n        Is the token valid? This method only checks the timestamps within the\n        token and compares them against the current time if none is provided.\n\n        :param time: The timestamp to validate against\n        :type time: Union[int, None]\n        :return: The validity of the token.\n        :rtype: bool\n        \"\"\"\n        if time is None:\n            epoch = datetime(1970, 1, 1, 0, 0, 0)\n            now = datetime.utcnow()\n            time = int((now - epoch).total_seconds())\n        if isinstance(self.valid_from, int) and time < self.valid_from:\n            return False\n        if isinstance(self.valid_to, int) and time > self.valid_to:\n            return False\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def encode(self) -> ",
        "right": ":\n        \"\"\"\n        Create a token based on the data held in the class.\n\n        :return: A new token\n        :rtype: str\n        \"\"\"\n        payload = {}\n        payload.update(self.registered_claims)\n        payload.update(self.payload)\n        return encode(self.secret, payload, self.alg, self.header)",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def decode(secret, token,\n               alg = default_alg) -> ",
        "right": ":\n        \"\"\"\n        Decodes the given token into an instance of `Jwt`.\n\n        :param secret: The secret used to decode the token. Must match the\n            secret used when creating the token.\n        :type secret: Union[str, bytes]\n        :param token: The token to decode.\n        :type token: Union[str, bytes]\n        :param alg: The algorithm used to decode the token. Must match the\n            algorithm used when creating the token.\n        :type alg: str\n        :return: The decoded token.\n        :rtype: `Jwt`\n        \"\"\"\n        header, payload = decode(secret, token, alg)\n        return Jwt(secret, payload, alg, header)",
        "return_type_from_source": "'Jwt'"
    },
    {
        "extra_left": [],
        "left": "def compare(self, jwt, compare_dates = False) -> ",
        "right": ":\n        \"\"\"\n        Compare against another `Jwt`.\n\n        :param jwt: The token to compare against.\n        :type jwt: Jwt\n        :param compare_dates: Should the comparision take dates into account?\n        :type compare_dates: bool\n        :return: Are the two Jwt's the same?\n        :rtype: bool\n        \"\"\"\n        if self.secret != jwt.secret:\n            return False\n        if self.payload != jwt.payload:\n            return False\n        if self.alg != jwt.alg:\n            return False\n        if self.header != jwt.header:\n            return False\n        expected_claims = self.registered_claims\n        actual_claims = jwt.registered_claims\n        if not compare_dates:\n            strip = ['exp', 'nbf', 'iat']\n            expected_claims = {k: {v if k not in strip else None} for k, v in\n                               expected_claims.items()}\n            actual_claims = {k: {v if k not in strip else None} for k, v in\n                             actual_claims.items()}\n        if expected_claims != actual_claims:\n            return False\n        return True",
        "return_type_from_source": "bool"
    },
    {
        "extra_left": [],
        "left": "def get_sheet_list(xl_path) -> ",
        "right": ":\n    \"\"\"Return a list with the name of the sheets in\n    the Excel file in `xl_path`.\n    \"\"\"\n    wb = read_xl(xl_path)\n\n    if hasattr(wb, 'sheetnames'):\n        return wb.sheetnames\n    else:\n        return wb.sheet_names()",
        "return_type_from_source": "List"
    },
    {
        "extra_left": [],
        "left": "def log_error(self, text) -> ",
        "right": ":\n        '''\n        Given some error text it will log the text if self.log_errors is True\n\n        :param text: Error text to log\n        '''\n        if self.log_errors:\n            with self._log_fp.open('a+') as log_file:\n                log_file.write(f'{text}\\n')",
        "return_type_from_source": "None"
    },
    {
        "extra_left": [],
        "left": "def parse_conll(self, texts, retry_count = 0) -> ",
        "right": ":\n        '''\n        Processes the texts using TweeboParse and returns them in CoNLL format.\n\n        :param texts: The List of Strings to be processed by TweeboParse.\n        :param retry_count: The number of times it has retried for. Default\n                            0 does not require setting, main purpose is for\n                            recursion.\n        :return: A list of CoNLL formated strings.\n        :raises ServerError: Caused when the server is not running.\n        :raises :py:class:`requests.exceptions.HTTPError`: Caused when the\n                input texts is not formated correctly e.g. When you give it a\n                String not a list of Strings.\n        :raises :py:class:`json.JSONDecodeError`: Caused if after self.retries\n                attempts to parse the data it cannot decode the data.\n\n        :Example:\n\n        '''\n        post_data = {'texts': texts, 'output_type': 'conll'}\n        try:\n            response = requests.post(f'http://{self.hostname}:{self.port}',\n                                     json=post_data,\n                                     headers={'Connection': 'close'})\n            response.raise_for_status()\n        except (requests.exceptions.ConnectionError,\n                requests.exceptions.Timeout) as server_error:\n            raise ServerError(server_error, self.hostname, self.port)\n        except requests.exceptions.HTTPError as http_error:\n            raise http_error\n        else:\n            try:\n                return response.json()\n            except json.JSONDecodeError as json_exception:\n                if retry_count == self.retries:\n                    self.log_error(response.text)\n                    raise Exception('Json Decoding error cannot parse this '\n                                    f':\\n{response.text}')\n                return self.parse_conll(texts, retry_count + 1)",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def update(cls, cur, table, values, where_keys) -> ",
        "right": ":\n        \"\"\"\n        Creates an update query with only chosen fields\n        Supports only a single field where clause\n\n        Args:\n            table: a string indicating the name of the table\n            values: a dict of fields and values to be inserted\n            where_keys: list of dictionary\n            example of where keys: [{'name':('>', 'cip'),'url':('=', 'cip.com'},{'type':{'<=', 'manufacturer'}}]\n            where_clause will look like ((name>%s and url=%s) or (type <= %s))\n            items within each dictionary get 'AND'-ed and dictionaries themselves get 'OR'-ed\n\n        Returns:\n            an integer indicating count of rows deleted\n\n        \"\"\"\n        keys = cls._COMMA.join(values.keys())\n        value_place_holder = cls._PLACEHOLDER * len(values)\n        where_clause, where_values = cls._get_where_clause_with_values(where_keys)\n        query = cls._update_string.format(table, keys, value_place_holder[:-1], where_clause)\n        yield from cur.execute(query, (tuple(values.values()) + where_values))\n        return (yield from cur.fetchall())",
        "return_type_from_source": "tuple"
    },
    {
        "extra_left": [],
        "left": "def _rindex(mylist, x) -> ",
        "right": ":\n    \"\"\"Index of the last occurrence of x in the sequence.\"\"\"\n    return len(mylist) - mylist[::-1].index(x) - 1",
        "return_type_from_source": "int"
    },
    {
        "extra_left": [],
        "left": "def error(code, *args, **kwargs) -> ",
        "right": ":\n    \"\"\"\n    Creates an error from the given code, and args and kwargs.\n\n    :param code: The acknowledgement code\n    :param args: Exception args\n    :param kwargs: Exception kwargs\n    :return: the error for the given acknowledgement code\n    \"\"\"\n    # TODO add proper error code\n    if code == FAILED_COMMAND and len(args) >= 1 and args[0] == \"Emergency Shutdown activated\":\n        return EmergencyShutdown(*args, **kwargs)\n    return _errors[code](*args, **kwargs)",
        "return_type_from_source": "HedgehogCommandError"
    },
    {
        "extra_left": [],
        "left": "def parse(self, data) -> ",
        "right": ":\n        \"\"\"\\\n        Parses a binary protobuf message into a Message object.\n        \"\"\"\n        try:\n            return self.receiver.parse(data)\n        except KeyError as err:\n            raise UnknownCommandError from err\n        except DecodeError as err:\n            raise UnknownCommandError(f\"{err}\") from err",
        "return_type_from_source": "Message"
    },
    {
        "extra_left": [],
        "left": "def get_fuel_prices(self) -> ",
        "right": ":\n        \"\"\"Fetches fuel prices for all stations.\"\"\"\n        response = requests.get(\n            '{}/prices'.format(API_URL_BASE),\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        return GetFuelPricesResponse.deserialize(response.json())",
        "return_type_from_source": "GetFuelPricesResponse"
    },
    {
        "extra_left": [],
        "left": "def get_fuel_prices_for_station(\n            self,\n            station\n    ) -> ",
        "right": ":\n        \"\"\"Gets the fuel prices for a specific fuel station.\"\"\"\n        response = requests.get(\n            '{}/prices/station/{}'.format(API_URL_BASE, station),\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        data = response.json()\n        return [Price.deserialize(data) for data in data['prices']]",
        "return_type_from_source": "List[Price]"
    },
    {
        "extra_left": [],
        "left": "def get_fuel_prices_within_radius(\n            self, latitude, longitude, radius,\n            fuel_type, brands = None\n    ) -> ",
        "right": ":\n        \"\"\"Gets all the fuel prices within the specified radius.\"\"\"\n\n        if brands is None:\n            brands = []\n        response = requests.post(\n            '{}/prices/nearby'.format(API_URL_BASE),\n            json={\n                'fueltype': fuel_type,\n                'latitude': latitude,\n                'longitude': longitude,\n                'radius': radius,\n                'brand': brands,\n            },\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        data = response.json()\n        stations = {\n            station['code']: Station.deserialize(station)\n            for station in data['stations']\n        }\n        station_prices = []  # type: List[StationPrice]\n        for serialized_price in data['prices']:\n            price = Price.deserialize(serialized_price)\n            station_prices.append(StationPrice(\n                price=price,\n                station=stations[price.station_code]\n            ))\n\n        return station_prices",
        "return_type_from_source": "List[StationPrice]"
    },
    {
        "extra_left": [],
        "left": "def get_fuel_price_trends(self, latitude, longitude,\n                              fuel_types) -> ",
        "right": ":\n        \"\"\"Gets the fuel price trends for the given location and fuel types.\"\"\"\n        response = requests.post(\n            '{}/prices/trends/'.format(API_URL_BASE),\n            json={\n                'location': {\n                    'latitude': latitude,\n                    'longitude': longitude,\n                },\n                'fueltypes': [{'code': type} for type in fuel_types],\n            },\n            headers=self._get_headers(),\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        data = response.json()\n        return PriceTrends(\n            variances=[\n                Variance.deserialize(variance)\n                for variance in data['Variances']\n            ],\n            average_prices=[\n                AveragePrice.deserialize(avg_price)\n                for avg_price in data['AveragePrices']\n            ]\n        )",
        "return_type_from_source": "PriceTrends"
    },
    {
        "extra_left": [],
        "left": "def get_reference_data(\n            self,\n            modified_since = None\n    ) -> ",
        "right": ":\n        \"\"\"\n        Fetches API reference data.\n\n        :param modified_since: The response will be empty if no\n        changes have been made to the reference data since this\n        timestamp, otherwise all reference data will be returned.\n        \"\"\"\n\n        if modified_since is None:\n            modified_since = datetime.datetime(year=2010, month=1, day=1)\n\n        response = requests.get(\n            '{}/lovs'.format(API_URL_BASE),\n            headers={\n                'if-modified-since': self._format_dt(modified_since),\n                **self._get_headers(),\n            },\n            timeout=self._timeout,\n        )\n\n        if not response.ok:\n            raise FuelCheckError.create(response)\n\n        # return response.text\n        return GetReferenceDataResponse.deserialize(response.json())",
        "return_type_from_source": "GetReferenceDataResponse"
    },
    {
        "extra_left": [],
        "left": "def get_indices_list(s) -> ",
        "right": ":\n    \"\"\" Retrieve a list of characters and escape codes where each escape\n        code uses only one index. The indexes will not match up with the\n        indexes in the original string.\n    \"\"\"\n    indices = get_indices(s)\n    return [\n        indices[i] for i in sorted(indices, key=int)\n    ]",
        "return_type_from_source": "List[str]"
    },
    {
        "extra_left": [],
        "left": "def strip_codes(s) -> ",
        "right": ":\n    \"\"\" Strip all color codes from a string.\n        Returns empty string for \"falsey\" inputs.\n    \"\"\"\n    return codepat.sub('', str(s) if (s or (s == 0)) else '')",
        "return_type_from_source": "str"
    },
    {
        "extra_left": [],
        "left": "def merge_ordered(ordereds) -> ",
        "right": ":\n    \"\"\"Merge multiple ordered so that within-ordered order is preserved\n    \"\"\"\n    seen_set = set()\n    add_seen = seen_set.add\n    return reversed(tuple(map(\n        lambda obj: add_seen(obj) or obj,\n        filterfalse(\n            seen_set.__contains__,\n            chain.from_iterable(map(reversed, reversed(ordereds))),\n        ),\n    )))",
        "return_type_from_source": "typing.Iterable[typing.Any]"
    },
    {
        "extra_left": [],
        "left": "def process_macros(self, content) -> ",
        "right": ":\n        '''Replace macros with content defined in the config.\n\n        :param content: Markdown content\n\n        :returns: Markdown content without macros\n        '''\n\n        def _sub(macro):\n            name = macro.group('body')\n            params = self.get_options(macro.group('options'))\n\n            return self.options['macros'].get(name, '').format_map(params)\n\n        return self.pattern.sub(_sub, content)",
        "return_type_from_source": "str"
    }
]